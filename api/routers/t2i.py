# api/routers/t2i.py
"""
Text-to-Image router (DI-based).
- No import-time side effects
- Engines injected via api/dependencies.py
"""
from __future__ import annotations
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any
import torch

from api.schemas import T2IResponse  # keep your shared schema if you use it elsewhere
from ..dependencies import get_t2i

router = APIRouter(prefix="/t2i", tags=["text-to-image"])


class GenerationRequest(BaseModel):
    prompt: str = Field(..., description="Generation prompt")
    negative_prompt: str = Field("", description="Negative prompt")
    width: int = Field(768, ge=256, le=1024, description="Image width")
    height: int = Field(768, ge=256, le=1024, description="Image height")
    num_inference_steps: int = Field(
        25, ge=10, le=50, description="Number of inference steps"
    )
    guidance_scale: float = Field(7.5, ge=1.0, le=20.0, description="Guidance scale")
    seed: Optional[int] = Field(None, description="Random seed for reproducibility")
    style_id: Optional[str] = Field(None, description="Style preset ID")
    scene_id: str = Field("scene_001", description="Scene identifier")
    image_type: str = Field("bg", description="Image type (bg/character/portrait)")


class GenerationResponse(BaseModel):
    success: bool
    image_path: Optional[str] = None
    metadata_path: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class ModelInfo(BaseModel):
    current_model: Optional[str]
    available_styles: Dict[str, str]
    gpu_available: bool
    memory_info: Optional[Dict[str, Any]] = None


@router.get("/models", response_model=ModelInfo)
async def get_model_info(t2i=Depends(get_t2i)):
    """Return current model and available styles."""
    try:
        memory_info = None
        if torch.cuda.is_available():
            memory_info = {
                "allocated_mb": torch.cuda.memory_allocated(0) / 1024**2,
                "reserved_mb": torch.cuda.memory_reserved(0) / 1024**2,
            }

        # Be tolerant: engines may expose different style APIs
        get_styles = getattr(t2i, "get_available_styles", None) or getattr(
            t2i, "styles", None
        )
        styles = get_styles() if callable(get_styles) else {}

        current_model = getattr(t2i, "current_model", None)
        return ModelInfo(
            current_model=current_model,
            available_styles=styles or {},
            gpu_available=torch.cuda.is_available(),
            memory_info=memory_info,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get model info: {e}")


@router.post(
    "/generate",
    response_model=GenerationResponse,
    summary="Generate an image from text",
)
async def generate_image(request: GenerationRequest, t2i=Depends(get_t2i)):
    """Delegate generation to the shared T2I engine."""
    try:
        # Optional style validation if engine exposes styles
        get_styles = getattr(t2i, "get_available_styles", None) or getattr(
            t2i, "styles", None
        )
        if request.style_id and callable(get_styles):
            styles = get_styles() or {}
            if request.style_id not in styles:
                raise HTTPException(
                    status_code=400, detail=f"Unknown style '{request.style_id}'"
                )

        result = t2i.txt2img(
            prompt=request.prompt,
            negative=request.negative_prompt,
            steps=request.num_inference_steps,
            width=request.width,
            height=request.height,
            guidance_scale=request.guidance_scale,
            seed=request.seed,
            style_id=request.style_id,
            scene_id=request.scene_id,
            image_type=request.image_type,
        )

        # result should contain image_path / metadata_path; keep response shape
        ok = bool(result.get("image_path"))
        return GenerationResponse(
            success=ok,
            image_path=result.get("image_path"),
            metadata_path=result.get("metadata_path"),
            metadata=result.get("metadata"),
            error=None if ok else result.get("error", "Unknown error"),
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {e}")


@router.post("/load_model")
async def load_model(
    model_id: str = Query(..., description="Model ID to load"), t2i=Depends(get_t2i)
):
    """Load a specific base model (if engine supports it)."""
    try:
        if not hasattr(t2i, "load_model"):
            raise HTTPException(
                status_code=501,
                detail="Model loading is not supported by current engine",
            )
        success = t2i.load_model(model_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to load model")
        return {
            "success": True,
            "model": model_id,
            "message": "Model loaded successfully",
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model loading error: {e}")


You said:
æˆ‘ç­‰ä¸€ä¸‹æœƒçµ¦ä½ æˆ‘å°ˆæ¡ˆæª”æ¡ˆè³‡æ–™çµæ§‹çš„åœ–ï¼Œè«‹ä½ å¹«æˆ‘è¨˜æ†¶ä¸¦æ•´ç†ï¼Œæœ€å¾Œæˆ‘æƒ³è¦ä½ å·²è³‡æ–™å¤¾æª”æ¡ˆçš„å±¤æ¬¡çµæ§‹å‘ˆç¾ã€‚å¤ªå¤šçš„è©± æˆ‘æœƒåˆ†æ‰¹çµ¦ä½ ç…§ç‰‡
ChatGPT said:

å¥½çš„ï¼ä½ å¯ä»¥åˆ†æ‰¹æŠŠå°ˆæ¡ˆæª”æ¡ˆè³‡æ–™çµæ§‹çš„åœ–å‚³çµ¦æˆ‘ï¼Œæˆ‘æœƒä¸€å¼µä¸€å¼µå¹«ä½ æ•´ç†å‡ºæ–‡å­—åŒ–çš„ç›®éŒ„æ¨¹ç‹€çµæ§‹ï¼ˆä»¥è³‡æ–™å¤¾ã€æª”æ¡ˆéšå±¤æ–¹å¼å‘ˆç¾ï¼‰ã€‚æœ€å¾Œæˆ‘æœƒæŠŠæ‰€æœ‰æ‰¹æ¬¡æ•´åˆèµ·ä¾†ï¼Œè¼¸å‡ºä¸€ä»½å®Œæ•´çš„å°ˆæ¡ˆå±¤ç´šçµæ§‹ã€‚

ä½ åªéœ€è¦ä¾åºæŠŠåœ–ç‰‡å‚³éä¾†å³å¯ï¼Œæˆ‘æœƒè¨˜éŒ„ä¸¦ç´¯ç©æ•´ç†ï¼Œä¸ç”¨æ“”å¿ƒæœƒéºæ¼ã€‚
è¦é–‹å§‹çš„æ™‚å€™ï¼Œç›´æ¥å‚³ç¬¬ä¸€æ‰¹åœ–ç‰‡å°±è¡Œäº†ã€‚

You said:
å¥½ä»¥ä¸Šæ˜¯æ‰€æœ‰è³‡æ–™å¤¾å’Œæª”æ¡ˆ è«‹æ•´ç†ä¸¦å‘ˆç¾å®Œæ•´çµæ§‹çµ¦æˆ‘
ChatGPT said:

æˆ‘å·²ç¶“æŠŠä½ çµ¦çš„æ‰€æœ‰å°ˆæ¡ˆçµæ§‹åœ–ç‰‡æ•´åˆå®Œæˆï¼Œä»¥ä¸‹æ˜¯ å®Œæ•´å±¤ç´šçµæ§‹ï¼ˆä»¥è³‡æ–™å¤¾èˆ‡æª”æ¡ˆæ¨¹ç‹€è¡¨ç¤ºï¼‰ï¼š

ANIME-ADVENTURE-LAB
â”œâ”€â”€ .vscode/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ admin.py
â”‚   â”‚   â”œâ”€â”€ batch.py
â”‚   â”‚   â”œâ”€â”€ export.py
â”‚   â”‚   â”œâ”€â”€ finetune.py
â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”œâ”€â”€ monitoring.py
â”‚   â”‚   â”œâ”€â”€ rag.py
â”‚   â”‚   â”œâ”€â”€ safety.py
â”‚   â”‚   â”œâ”€â”€ story.py
â”‚   â”‚   â”œâ”€â”€ t2i.py
â”‚   â”‚   â”œâ”€â”€ vlm.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ middleware.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ presets/
â”‚   â”‚   â”œâ”€â”€ anime_style.yaml
â”‚   â”‚   â”œâ”€â”€ realistic_style.yaml
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ dreambooth.yaml
â”‚   â”‚   â”œâ”€â”€ lora-sd15-anime.yaml
â”‚   â”‚   â”œâ”€â”€ lora-sdxl.yaml
â”‚   â”‚   â”œâ”€â”€ app.yaml
â”‚   â”‚   â”œâ”€â”€ models.yaml
â”‚   â”‚   â”œâ”€â”€ performance.yaml
â”‚   â”‚   â”œâ”€â”€ rag.yaml
â”‚   â”œâ”€â”€ core/
â”‚       â”œâ”€â”€ __pycache__/
â”‚       â”œâ”€â”€ export/
â”‚       â”‚   â””â”€â”€ story_exporter.py
â”‚       â”œâ”€â”€ llm/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ adapter.py
â”‚       â”‚   â”œâ”€â”€ ollama_llm.py
â”‚       â”‚   â”œâ”€â”€ prompt_templates.py
â”‚       â”‚   â””â”€â”€ transformers_llm.py
â”‚       â”œâ”€â”€ performance/
â”‚       â”‚   â”œâ”€â”€ cache_manager.py
â”‚       â”‚   â””â”€â”€ memory_manager.py
â”‚       â”œâ”€â”€ rag/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ chunkers.py
â”‚       â”‚   â”œâ”€â”€ embedders.py
â”‚       â”‚   â”œâ”€â”€ engine.py
â”‚       â”‚   â”œâ”€â”€ memory.py
â”‚       â”‚   â”œâ”€â”€ parsers.py
â”‚       â”‚   â”œâ”€â”€ rerankers.py
â”‚       â”‚   â””â”€â”€ retrievers.py
â”‚       â”œâ”€â”€ safety/
â”‚       â”‚   â”œâ”€â”€ detector.py
â”‚       â”‚   â”œâ”€â”€ license.py
â”‚       â”‚   â””â”€â”€ watermark.py
â”‚       â””â”€â”€ story/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ choice_resolver.py
â”‚           â”œâ”€â”€ data_structures.py
â”‚           â”œâ”€â”€ engine.py
â”‚           â”œâ”€â”€ game_state.py
â”‚           â””â”€â”€ persona.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ story/ (é‡è¤‡åç¨±ï¼Œç¨ç«‹æ–¼ configs/core ä¸‹)
â”‚   â”œâ”€â”€ t2i/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ controlnet.py
â”‚   â”‚   â”œâ”€â”€ lora_manager.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ safety.py
â”‚   â”‚   â””â”€â”€ watermark.py
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ evaluators.py
â”‚   â”‚   â”œâ”€â”€ lora_trainer.py
â”‚   â”‚   â””â”€â”€ registry.py
â”‚   â”œâ”€â”€ vlm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ captioner.py
â”‚       â”œâ”€â”€ consistency.py
â”‚       â”œâ”€â”€ tagger.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ performance.py
â”‚   â””â”€â”€ shared_cache.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ 01_setup.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_llm_basics.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_rag_pipeline.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_t2i_generation.ipynb
â”‚   â”‚   â””â”€â”€ 05_lora_training.ipynb
â”‚   â”œâ”€â”€ api.md
â”‚   â”œâ”€â”€ deployment.md
â”‚   â”œâ”€â”€ development.md
â”‚   â””â”€â”€ worldpack_format.md
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ desktop/
â”‚   â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â””â”€â”€ widgets/
â”‚   â”‚       â””â”€â”€ main.py
â”‚   â”œâ”€â”€ gradio/
â”‚       â”œâ”€â”€ components/
â”‚       â””â”€â”€ static/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ app.py
â”‚           â”œâ”€â”€ performance_monitor.py
â”‚           â”œâ”€â”€ safety_admin.py
â”‚           â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ demo/
â”‚   â”‚   â”œâ”€â”€ demo_stage1.py
â”‚   â”‚   â”œâ”€â”€ demo_stage2.py
â”‚   â”‚   â”œâ”€â”€ batch_generate.py
â”‚   â”‚   â”œâ”€â”€ demo_complete_workflow.py
â”‚   â”‚   â”œâ”€â”€ demo_stage5.sh
â”‚   â”‚   â”œâ”€â”€ demo_stage6_vlm.py
â”‚   â”‚   â”œâ”€â”€ demo_stage7.py
â”‚   â”‚   â”œâ”€â”€ demo_stage8.py
â”‚   â”‚   â”œâ”€â”€ eval_lora.py
â”‚   â”‚   â”œâ”€â”€ eval_rag.py
â”‚   â”‚   â”œâ”€â”€ ingest_worldpack.py
â”‚   â”‚   â”œâ”€â”€ release_v1.py
â”‚   â”‚   â”œâ”€â”€ setup_env.py
â”‚   â”‚   â”œâ”€â”€ setup_low_vram.py
â”‚   â”‚   â”œâ”€â”€ setup_stage5.py
â”‚   â”‚   â”œâ”€â”€ smoke_test_stage8.sh
â”‚   â”‚   â”œâ”€â”€ smoke_test_t2i.py
â”‚   â”‚   â””â”€â”€ test_stage9_performance.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ fixtures/
â”‚   â”‚   â”œâ”€â”€ sample_docs/
â”‚   â”‚   â””â”€â”€ sample_images/
â”‚   â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_llm.py
â”‚   â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”‚   â”œâ”€â”€ test_t2i.py
â”‚   â”‚   â”œâ”€â”€ test_train.py
â”‚   â”‚   â”œâ”€â”€ conftest.py
â”‚   â”‚   â”œâ”€â”€ test_e2e_complete.py
â”‚   â”‚   â”œâ”€â”€ test_safety_integration.py
â”‚   â”‚   â”œâ”€â”€ test_stage1_smoke.py
â”‚   â”‚   â”œâ”€â”€ test_stage2.py
â”‚   â”‚   â””â”€â”€ test_stage4_smoke.py
â”‚
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch.py
â”‚   â”‚   â”œâ”€â”€ generation.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ job_tracker.py
â”‚   â”‚   â”œâ”€â”€ queue_monitor.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ celery_app.py
â”‚
â”œâ”€â”€ worldpacks/
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.prod.yml
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


é€™æ¨£ä½ å°±æœ‰ä¸€ä»½å®Œæ•´çš„ å°ˆæ¡ˆç›®éŒ„çµæ§‹æ¨¹ äº† âœ…
è¦ä¸è¦æˆ‘å†å¹«ä½ åšä¸€ä»½ã€Œåˆ†å€æ•´ç†ç‰ˆã€ï¼Œä¾‹å¦‚ å¾Œç«¯ (api/core/configs)ã€å‰ç«¯ (frontend)ã€å·¥å…·èˆ‡æ¸¬è©¦ (scripts/tests/workers)ã€æ–‡ä»¶ (docs/notebooks)ï¼Œæ–¹ä¾¿å¿«é€Ÿç†è§£æ¨¡çµ„åˆ†å·¥ï¼Ÿ

You said:
æˆ‘ç¾åœ¨ç…§è‘—claude aiçš„èªªæ˜åŠæŒ‡å°ä¸‹ ä»¥åŠå®Œæˆäº† å®ƒèªç‚ºæ‰€æœ‰çš„å°ˆæ¡ˆå¯¦ä½œ ä½†å…¶å¯¦é‚„æœ‰å¾ˆå¤šå•é¡Œ
å…ˆä¸è€ƒé‡å‰ç«¯ç›¸é—œä»¥åŠ å…¶ä»–éšæ®µsmoke æˆ–æ¸¬è©¦test åŠå…¶ä»–ç›¸é—œçš„scriptsæª”æ¡ˆç­‰ï¼Œ ç›®å‰é€™æ¨£çš„å°ˆæ¡ˆçµæ§‹ä¸‹ claude AI é‚„æœ‰ä»¥ä¸‹çš„æª”æ¡ˆç¨‹å¼ç¢¼æ²’æœ‰å¯¦ä½œã€‚å¯ä»¥å‘Šè¨´æˆ‘èƒ½å¦‚ä½•å‘Šè¨´claude aié€™äº›è³‡è¨Šï¼Œè®“å®ƒçŸ¥é“ç›®å‰å°ˆæ¡ˆçš„æƒ…æ³ å»æ›´æ–°æ”¹å–„? å°æ‡‰é©åˆçš„prompt?
ä»¥ä¸‹æ˜¯æ‰€æœ‰æœªå¯¦ä½œçš„éƒ¨åˆ†ï¼Œè«‹ä½ å…ˆè¨˜å¾—é€™äº›è³‡è¨Šå³å¯ æˆ‘é‚„æœƒå†çµ¦ä½ æˆ‘çµ¦claude aiçš„ æ‰€æœ‰project fileså…§å®¹ è®“ä½ æ›´äº†è§£é€™å°ˆæ¡ˆçš„è³‡è¨Šã€‚åˆ°é‚£æ™‚å€™ä½ å†å›æ‡‰å³å¯ã€‚

api/routers/__init.py, admin.py, batch.py,  æ­¤å¤–å…¶ä»–çš„APIRouter æ²’è¨­å®šå®Œæ•´

api/__init.py


configs/train/dreambooth.yaml, lora-sdxl.yaml

configs/app.yaml, models.yaml


core/llm/__init.py, ollama_llm.py

core/rag/embedders.py, memory.py parsers.py, rerankers.py, retrievers.py

core/story/__init__.py

core/t2i/__init__.py, controlnet.py, lora_manager.py, safety.py, watermark.py

core/train/__init__.py, dataset.py, evaluators.py, registry.py

core/vlm/__init__.py,

core/__init__.py

workers çš†æ²’å¯¦ä½œ

ChatGPT said:
Updated saved memory
Thought for 40s

æ”¶åˆ°ï¼æˆ‘å·²æŠŠã€Œå°šæœªå¯¦ä½œï¼æœªå®Œæˆã€æ¸…å–®å®Œæ•´è¨˜ä¸‹ä¾† âœ…
ç­‰ä½ æŠŠæä¾›çµ¦ Claude çš„æ‰€æœ‰ project files è²¼ä¸Šå¾Œï¼Œæˆ‘å†ä¸€æ¬¡æ€§ç”¢å‡ºï¼š

ç²¾æº–çš„å·®ç•°æ¸…å–®ï¼ˆå“ªäº›æª”æ¡ˆç¼ºã€å“ªäº›è·¯ç”±/æ¨¡çµ„æœªå°é½Šï¼‰ï¼Œ

ä¸€ä»½å¯ç›´æ¥è²¼çµ¦ Claude çš„å®Œå–„ Promptï¼ˆå«çµæ§‹ã€å„ªå…ˆé †åºã€é©—æ”¶æ¢ä»¶ï¼‰ã€‚

ä½ éš¨æ™‚æŠŠå°ˆæ¡ˆæª”è²¼ä¸Šä¾†å°±è¡Œã€‚

You said:
CharaForgae-T2I-Lab installation-guide.md
File
CharaForge-T2I-Lab api-design.py
Python
CharaForge-T2I-Lab fine-tuningè¦åŠƒ.md
File
CharaForge-T2I-Lab å°ˆæ¡ˆæ¶æ§‹.md
File
CharaForge-T2I-Lab å°ˆæ¡ˆè¦å‰‡.md
File
CharaForge-T2I-Lab å°ˆæ¡ˆè¦åŠƒ.md
File
CharaForge-T2I-Lab å°ˆæ¡ˆæœ€æ–°æ¶æ§‹.md
File
CharaForge-T2I-Lab å­¸ç¿’è¨ˆç•«è¡¨.md
File
é€™äº›æ˜¯æ­¤å°ˆæ¡ˆæˆ‘ç›®å‰çµ¦claude aiæ‰€æœ‰çš„project files è«‹å¹«æˆ‘ç”¢å‡º :

1. ç²¾æº–çš„å·®ç•°æ¸…å–®ï¼ˆå“ªäº›æª”æ¡ˆç¼ºã€å“ªäº›è·¯ç”±/æ¨¡çµ„æœªå°é½Šï¼‰ï¼Œ

2. ä¸€ä»½å¯ç›´æ¥è²¼çµ¦ Claude çš„å®Œå–„ Promptï¼ˆå«çµæ§‹ã€å„ªå…ˆé †åºã€é©—æ”¶æ¢ä»¶ï¼‰ã€‚
ChatGPT said:
Thought for 3m 11s

å¤ªå¥½äº†ï¼Œä½ æŠŠã€Œçµ¦ Claude çš„å°ˆæ¡ˆæ–‡ä»¶ã€éƒ½æ•´ç†ä¸Šä¾†äº†ã€‚ä¸‹é¢åˆ†å…©éƒ¨åˆ†äº¤ä»˜ï¼š

1) ç²¾æº–å·®ç•°æ¸…å–®ï¼ˆç¼ºä»€éº¼ã€å“ªè£¡æ²’å°é½Šï¼‰

åŸºæº–ï¼ˆClaude ä»¥ç‚ºçš„è¨­è¨ˆï¼‰

backend/main.py æœƒè¨»å†Šä¸‹åˆ—è·¯ç”±ï¼šcaption, vqa, chat, rag, agent, game, t2i, controlnet, lora, batchï¼Œä¸” API å‰ç¶´ /api/v1ï¼ˆæ–‡ä»¶èˆ‡ç¯„ä¾‹ç¨‹å¼å¤šè™•ä¸€è‡´ï¼‰ã€‚


æ ¸å¿ƒ API æ‡‰è‡³å°‘æœ‰ï¼š/txt2imgã€/img2imgã€/controlnet/*ã€/lora/*ã€/batch/*ã€/eval/*ã€/healthã€‚


æ‰€æœ‰å…¥å£ç¨‹å¼é ˆå…ˆåšã€ŒShared Cache Bootstrapã€ä¸¦ä¾ AI_CACHE_ROOT å»ºç«‹å€‰å„²çµæ§‹ã€‚


è¨“ç·´é…ç½®ç¯„ä¾‹å¦‚ train/configs/charA_sd15_r16.yamlã€charA_sdxl_r16.yamlã€‚

ä½ ç›®å‰çš„ repoï¼ˆANIME-ADVENTURE-LABï¼Œåƒ…å¾Œç«¯/æ ¸å¿ƒï¼Œå¿½ç•¥å‰ç«¯ã€smokeã€testsã€scriptsï¼‰

è·¯ç”±ç›®éŒ„æ˜¯ api/routers/ï¼Œæª”æ¡ˆæœ‰ï¼šadmin.py, batch.py, export.py, finetune.py, health.py, llm.py, monitoring.py, rag.py, safety.py, story.py, t2i.py, vlm.pyï¼ˆä½ çš„æˆªåœ–ï¼‰ã€‚

ä½ ä¹Ÿå¦å¤–åˆ—å‡ºæœªå¯¦ä½œï¼š

api/routers/__init__.py, admin.py, batch.pyï¼ˆä»¥åŠå…¶ä»– APIRouter è¨­å®šæœªå®Œæˆï¼‰

api/__init__.py

configs/train/dreambooth.yaml, lora-sdxl.yamlï¼›configs/app.yaml, models.yaml

core/llm/__init__.py, ollama_llm.py

core/rag/{embedders.py, memory.py, parsers.py, rerankers.py, retrievers.py}

core/story/__init__.py

core/t2i/{__init__.py, controlnet.py, lora_manager.py, safety.py, watermark.py}

core/train/{__init__.py, dataset.py, evaluators.py, registry.py}

core/vlm/__init__.py, core/__init__.py

workers/ å…¨éƒ¨æœªå¯¦ä½œ

ç¶œåˆä»¥ä¸Šï¼Œå·®ç•°èˆ‡ä¿®æ­£å»ºè­°å¦‚ä¸‹ï¼ˆåªåˆ—èˆ‡ä½ è¦æ±‚ç›¸é—œçš„å¾Œç«¯/æ ¸å¿ƒï¼‰ï¼š

A) è·¯ç”±èˆ‡å‘½åä¸ä¸€è‡´
é¡åˆ¥	æ–‡ä»¶/Claude é æœŸ	ä½  repo ç›®å‰	å·®ç•° & ä¿®æ­£å»ºè­°
è·¯ç”±å…¥å£	backend/main.py åŒ¯å…¥ caption, vqa, chat, rag, agent, game, t2i, controlnet, lora, batchï¼Œprefix=/api/v1ã€‚
	api/main.pyï¼ˆå­˜åœ¨ï¼‰+ api/routers/*	1) å¯«å¥½ api/routers/__init__.py åŒ¯å‡ºå¯è¨»å†Šçš„ routers åˆ—è¡¨ï¼›2) åœ¨ api/main.py çµ±ä¸€ include_routerï¼Œç¢ºä¿ prefix=/api/v1ã€tags å®Œæ•´ã€‚
æ–‡ç”Ÿåœ–	t2i.pyï¼ˆ/txt2imgï¼‰
	t2i.py å·²æœ‰	å°é½Š Pydantic schemaï¼ˆå¯¬é«˜ã€æ­¥æ•¸ã€seedã€lora_idsâ€¦ï¼‰ï¼›å›å‚³éœ€å«è¼¸å‡ºè·¯å¾‘èˆ‡è€—æ™‚ã€‚
ControlNet	controlnet.pyï¼ˆ/controlnet/{pose,depth,edge,line}ï¼‰
	ç¼º	å¯¦ä½œè·¯ç”±èˆ‡å°æ‡‰ core/t2i/controlnet.pyã€‚
LoRA ç®¡ç†	lora.pyï¼ˆ`/lora/list	load	unload`ï¼‰

æ‰¹æ¬¡	batch.pyï¼ˆ/batch/submit,status,cancelï¼‰
	batch.py ä½†æœªå®Œ	å°é½Šåƒæ•¸/å›å‚³ï¼›èˆ‡ workersï¼ˆCelery æˆ–ç°¡åŒ–ç‰ˆï¼‰ä¸²æ¥ã€‚
RAG	rag.py	rag.py å·²æœ‰	å°é½Š schemaï¼›æœ€å°å¯ç”¨ï¼šä¸Šå‚³/ç´¢å¼•/å•ç­”å„ä¸€ã€‚
Chat	chat.pyï¼ˆ/chatï¼‰
	ä½ çš„ llm.py	å…©è€…å°æ‡‰ï¼šä¿ç•™ llm.py åŠŸèƒ½ï¼Œæ–°å¢ chat.py è–„å°è£è·¯ç”±æˆ–æŠŠç¾æœ‰æª”æ”¹åã€‚
Caption	caption.pyï¼ˆ/captionï¼‰
	ç„¡	æ–°å¢è·¯ç”±ï¼›å‘¼å« core/vlm/captioner.pyï¼ˆä½ æœ‰ core/vlmï¼‰ã€‚
VQA	vqa.pyï¼ˆ/vqaï¼‰
	ç„¡	æ–°å¢è·¯ç”±ï¼›å‘¼å« core/vlm çš„ VQA pipelineã€‚
Game	game.pyï¼ˆ/game/new, /game/stepï¼‰
	ä½ çš„ story.py	å…©è€…å°æ‡‰ï¼šæ–°å¢ game.py å°è£æˆ–èª¿æ•´ story.py çš„è·¯å¾‘/å‘½åã€‚
å…¶ä»–ç¾æœ‰ä½†æ–‡ä»¶æœªåˆ—	admin, export, finetune, monitoring, safety, health, vlm	ä½ å·²æœ‰	æ¨™è¨˜ç‚ºå…§éƒ¨ç®¡ç†ï¼›ä¿ç•™ä½†ä¸å½±éŸ¿å°å¤– APIã€‚
B) Schemasï¼ˆPydanticï¼‰èˆ‡åŒ¯å…¥è·¯å¾‘

æ–‡ä»¶ç¨‹å¼ç¢¼å¼•ç”¨ backend.schemas.*ï¼ˆä¾‹å¦‚ CaptionRequest/Responseï¼‰ã€‚

ä½ ç›®å‰ repo æ²’çœ‹åˆ° schemas/ ç›®éŒ„â€”â€”éœ€è¦æ–°å¢ api/schemasï¼ˆæˆ– backend/schemasï¼‰ä¸¦æŠŠå„è·¯ç”±ç”¨åˆ°çš„è«‹æ±‚/å›æ‡‰æ¨¡å‹è£œé½Šï¼›æˆ–æ”¹è·¯å¾‘é¿å… ImportErrorã€‚

C) Core å±¤ç¼ºä»¶ï¼ˆä¾ä½ åˆ—èˆ‰ï¼‰

core/t2i/ï¼šcontrolnet.py, lora_manager.py, safety.py, watermark.py, __init__.py æœªå¯¦ä½œ â†’ é€™äº›ç›´æ¥å½±éŸ¿ /controlnet/*ã€/lora/*ã€å®‰å…¨èˆ‡æµ®æ°´å°ã€‚

core/rag/ï¼šembedders.py, memory.py, parsers.py, rerankers.py, retrievers.py æœªå¯¦ä½œ â†’ å…ˆåšæœ€å°å¯ç”¨ç‰ˆæœ¬ï¼ˆFaiss + sentence-transformersï¼‰ã€‚

core/llm/ollama_llm.pyã€core/story/__init__.pyã€core/vlm/__init__.pyã€core/train/*ã€core/__init__.py æœªå¯¦ä½œ â†’ å…ˆè£œç©ºæ®¼ + æœ€å°åŠŸèƒ½ï¼ˆé¿å…åŒ¯å…¥å¤±æ•—ï¼‰ã€‚

D) è¨­å®šæª”ï¼ˆYAMLï¼‰ç¼ºå…§å®¹

configs/train/dreambooth.yaml, lora-sdxl.yaml â†’ åƒè€ƒæ–‡ä»¶ä¸­çš„ç¯„ä¾‹éµå€¼ï¼ˆmodel_id,resolution,rank,lr,train_batch_size,steps,...ï¼‰è£œé½Šã€‚

configs/app.yaml, models.yaml â†’ è‡³å°‘æ”¾ AI_CACHE_ROOT ä½¿ç”¨çš„æ¨¡å‹åˆ¥åã€è¼‰å…¥ç­–ç•¥ï¼ˆæ–‡ä»¶ä¹Ÿç¤ºç¯„é performance/config æƒ³å®šï¼‰ã€‚

E) Workersï¼ˆæ‰¹æ¬¡ï¼‰

æ–‡ä»¶è¦åŠƒä»¥ Celery/Redis è·‘æ‰¹æ¬¡èˆ‡è¨“ç·´ä»»å‹™ï¼›ä½  repo çš„ workers/ æœªå¯¦ä½œã€‚å…ˆåšæœ€å°å¯ç”¨ï¼šä¸€å€‹ celery_app.py + tasks/batch.pyï¼ˆæäº¤/æŸ¥è©¢ç‹€æ…‹ï¼‰ã€‚æ–‡ä»¶ä¹ŸæŠŠé€™å¡Šæ”¾åœ¨ In-Scopeï¼ˆæœå‹™å±¤ï¼‰ã€‚

F) å…±åŒå€‰å„²ï¼ˆAI_CACHE_ROOTï¼‰å•Ÿå‹•ç¢¼

éœ€ç¢ºä¿æ¯å€‹å…¥å£ï¼ˆä¾‹å¦‚ api/main.pyã€notebookã€workerï¼‰åœ¨ import ä¹‹å‰è·‘ bootstrapï¼›æ–‡ä»¶æä¾›äº†æ¨™æº–ç‰‡æ®µã€‚

2) å¯ç›´æ¥è²¼çµ¦ Claude çš„å®Œå–„ Promptï¼ˆçµæ§‹ï¼å„ªå…ˆé †åºï¼é©—æ”¶ï¼‰

è²¼ä¸Šå°±èƒ½ç”¨ã€‚å…§å®¹æ˜¯ã€Œä»»å‹™èªªæ˜ + ç¾æ³ + è¦æ±‚ + é©—æ”¶ã€ã€‚
ï¼ˆæé†’ï¼šæ‰€æœ‰ç¨‹å¼ç¢¼èˆ‡æäº¤è¨Šæ¯è«‹ç”¨è‹±æ–‡ï¼›å¯åœ¨ PR èªªæ˜å¯«ä¸­æ–‡ã€‚ï¼‰

Roleï¼šYou are a senior FastAPI + Diffusers backend engineer.
Goalï¼šBring the current repo in sync with the intended API surface and core modules; ignore frontend, tests, and scripts for now.

Context (what you must assume true)

The running repo uses this layout (backend only): api/routers/{admin,batch,export,finetune,health,llm,monitoring,rag,safety,story,t2i,vlm}, plus core/ and configs/. Many modules are stubs or missing as listed below.

The product docs expect these public routes (prefix /api/v1): caption, vqa, chat, rag, agent, game, t2i, controlnet, lora, batch; also /health. See design examples and minimal backend in the docs (you can mirror their shapes).

Every entry point must run the Shared Cache Bootstrap with AI_CACHE_ROOT to create model/data/output folders before imports.

Do NOT touch

Frontend folders, smoke tests, tests/, or any extra scripts/ for now.

A. Whatâ€™s missing â€“ implement in this order (highest first)

Router wiring & API surface (must)

Create/finish api/routers/__init__.py exporting a list/tuple of routers.

In api/main.py, include all routers with prefix /api/v1 and meaningful tags; ensure CORS and a /health endpoint.

Add these new public routers (thin layers if we already have inner logic):

chat.py (can wrap current llm.py)

game.py (can wrap current story.py)

caption.py, vqa.py (call into core/vlm)

controlnet.py (pose/depth/edge/line)

lora.py (list|load|unload)

Keep existing admin/export/finetune/monitoring/safety as internal (optional prefix /admin/*).

Core t2i & utilities (must)

Implement minimal but working:

core/t2i/controlnet.py (apply control image/JSON),

core/t2i/lora_manager.py (load/unload/compose),

core/t2i/safety.py (basic NSFW flag) & core/t2i/watermark.py (optional),

core/t2i/__init__.py exporting loaders.

Provide a simple save_image_to_cache() that writes image + sidecar JSON under $AI_CACHE_ROOT/outputs/....

Schemas (must)

Add backend or api/schemas/ with Pydantic models for: T2IRequest/Response, ChatRequest/Response, CaptionResponse, VQARequest/Response, Game*, Batch*.

Keep all field names aligned with the docs (prompt, negative, steps, width/height, lora_ids, seed, etc.).

Configs (must)

Fill configs/train/dreambooth.yaml and configs/train/lora-sdxl.yaml using the keys from the examples (model_id,resolution,rank,learning_rate,train_batch_size,gradient_accumulation_steps,max_train_steps,dataset_meta,output_dir,text_encoder_train,...).

Fill configs/models.yaml (model aliases/paths) & configs/app.yaml (API settings, device).

Ensure all code reads AI_CACHE_ROOT and never writes large assets to the repo.

RAG minimal (should)

Implement stubbed core/rag/{embedders,parsers,retrievers,rerankers,memory}.py with a minimal FAISS+sentence-transformers pipeline and plain text parsingâ€”just enough to answer /rag/ask.

Workers (should)

Minimal Celery app under workers/ï¼ˆor keep your folder nameï¼‰ with a single batch.generate task; /batch/submit enqueues a job and returns job_id, /batch/status/{id} polls Redis/backend.

Leftovers

Add missing __init__.py files inside core/* packages to avoid import errors.

B. Implementation constraints

Code must run Shared Cache Bootstrap before heavy imports (HF caches, model dirs).

Respect typing, Pydantic validation, and return explicit error messages.

No large downloads or training in this PR; routes may return simplified results but must be functional.

Code & commit messages in English.

C. Deliverables (what to output)

A single PR-sized patch containing:

New/updated files for routers, schemas, core, workers, configs.

Exact uvicorn run command and example curl snippets.

README snippet describing how to start API and a note on AI_CACHE_ROOT.

Short CHANGELOG section.

D. Acceptance criteria (copy/paste to verify)

Health: curl -f http://localhost:8000/api/v1/health â†’ JSON with gpu_available and cache_root.

T2I:

curl -X POST http://localhost:8000/api/v1/txt2img \
  -H "Content-Type: application/json" \
  -d '{"prompt":"anime girl, blue hair","steps":8,"width":512,"height":512,"seed":42}'


â†’ 200, JSON åŒ…å« image_pathã€metadata_pathã€elapsed_msã€‚

LoRA:

GET /api/v1/lora/list â†’ 200, å›å‚³ç©ºé™£åˆ—æˆ–å¯ç”¨æ¸…å–®ã€‚

POST /api/v1/lora/load with a fake id â†’ åˆç†éŒ¯èª¤è¨Šæ¯ï¼ˆ400/404ï¼‰ã€‚

ControlNetï¼ˆposeï¼‰: POST /api/v1/controlnet/pose with mock pose JSON â†’ 200ï¼Œè‡³å°‘å›å‚³ã€Œå·²å¥—ç”¨æ¢ä»¶ã€è¨Šæ¯ã€‚

Chat: POST /api/v1/chat â†’ 200ï¼Œå›å‚³ messageï¼ˆå¯ç‚º dummy æˆ–æ¨¡å‹è¼¸å‡ºï¼‰ã€‚ï¼ˆæ–‡ä»¶ä¹Ÿç¤ºä¾‹é curl å½¢å¼ï¼‰

RAG: POST /api/v1/rag/askï¼ˆçµ¦ä¸€æ®µçŸ­æ–‡å­—ï¼‰â†’ 200ï¼Œå›å‚³ answer èˆ‡ sourcesï¼ˆå¯ç‚º mock ç´¢å¼•ï¼‰ã€‚

Batch: POST /api/v1/batch/submit â†’ å›å‚³ job_idï¼›GET /api/v1/batch/status/{job_id} â†’ queued|running|done|errorã€‚

å…¥å£ç¨‹å¼çš†å°å‡º [cache] ... | GPU: <bool> æˆ–åŒç¾©ï¼ˆShared Cache Bootstrapï¼‰ã€‚

E. File-by-file TODO (please produce full code blocks)

api/routers/__init__.py â€“ export a routers list and __all__.

api/routers/{caption.py,vqa.py,chat.py,game.py,controlnet.py,lora.py,batch.py} â€“ complete APIRouter with tags and schemas.

api/main.py â€“ include all routers with /api/v1.

api/schemas/* â€“ Pydantic models for each route.

core/t2i/{__init__.py,controlnet.py,lora_manager.py,safety.py,watermark.py} â€“ minimal working versions; reusable save_image_to_cache().

core/rag/{embedders.py,parsers.py,retrievers.py,rerankers.py,memory.py} â€“ minimal FAISS pipeline.

workers/{celery_app.py,tasks/batch.py} â€“ queue + status.

configs/train/{dreambooth.yaml,lora-sdxl.yaml} â€“ fill with keys from the sample YAMLsã€‚

configs/{app.yaml,models.yaml} â€“ basic runtime + model aliases.

F. Commit style

Use Conventional Commits, e.g.:
feat(api): add /controlnet pose route and schemasã€chore(env): add shared cache bootstrapã€feat(worker): minimal batch submit/status.ï¼ˆæ–‡ä»¶æœ‰è¦ç¯„èˆ‡ç¯„ä¾‹ï¼‰

å¦‚æœä½ è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥æŠŠä¸Šè¿°ã€ŒFile-by-file TODOã€æ‹†æˆå¤šå€‹å° PR çš„æŒ‡ä»¤ç¨¿ï¼ˆåˆ†æ”¯å‘½åã€æ¯æ­¥é©Ÿ commit messageï¼‰ï¼Œæˆ–ç›´æ¥å¹«ä½ æŠŠ configs/* çš„ YAML æœ€å°ç¯„ä¾‹ç”Ÿå‡ºä¾†å°é½Šæ–‡ä»¶éµåã€‚

You said:
å¥½ å…¶å¯¦promptçš„éƒ¨åˆ†æˆ‘èªç‚ºç°¡ç•¥ä¸€é» ç”¨ä¸­æ–‡å°±å¥½ ç„¶å¾Œä¸éœ€è¦å¼·èª¿roleå®šä½ é‚£ä¸€äº›åœ°èªªæ˜ ç›´æ¥åˆ‡å…¥é‡é» é—œæ–¼éœ€è¦æ”¹å–„çš„éƒ¨åˆ† å› ç‚ºclaude æœ‰è¨Šæ¯çš„é™åˆ¶ æˆ‘åœ¨project Instructionsä¸­å·²ç¶“åœ°ä½å¥½roleçš„è§’è‰²äº† å› æ­¤é€™äº›éƒ¨åˆ†å¯ä»¥çœç•¥ æ¸›å°‘ä¸å¿…è¦çš„è¨Šæ¯é•·åº¦
ChatGPT said:
Thought for 54s

å¤ªå¥½äº†ï¼é€™æ˜¯ä¸€ä»½ç²¾ç°¡ä¸­æ–‡ç‰ˆã€å¯ä»¥ç›´æ¥è²¼çµ¦ Claude çš„æŒ‡ä»¤ï¼ˆå·²å»é™¤è§’è‰²é‹ªé™³ï¼Œç›´å…¥é‡é»ï¼‰ğŸ‘‡

æƒ…å¢ƒ
- å°ˆæ¡ˆå¾Œç«¯å·²å¯å•Ÿå‹•ï¼Œä½†å¤šè™•æœªå¯¦ä½œæˆ–æœªå°é½Šã€‚å…ˆå¿½ç•¥ frontendã€testsã€scriptsã€‚
- ç›®æ¨™ï¼šåœ¨ä¸æ“´å……éœ€æ±‚çš„å‰æä¸‹ï¼Œè£œé½Šã€Œæœ€å°å¯ç”¨ MVPã€ï¼Œä»¥å–®ä¸€ PR æäº¤ã€‚

æœªå®Œæˆæ¸…å–®ï¼ˆè«‹å…ˆä»¥æ­¤ç‚ºæº–ï¼‰
- api/routersï¼š__init__.pyã€admin.pyã€batch.pyï¼ˆå…¶é¤˜ APIRouter è¨»å†Š/å‰ç¶´æœªçµ±ä¸€ï¼‰
- api/__init__.py
- configsï¼šapp.yamlã€models.yaml
- configs/trainï¼šdreambooth.yamlã€lora-sdxl.yaml
- core/llmï¼š__init__.pyã€ollama_llm.py
- core/ragï¼šembedders.pyã€memory.pyã€parsers.pyã€rerankers.pyã€retrievers.py
- core/storyï¼š__init__.py
- core/t2iï¼š__init__.pyã€controlnet.pyã€lora_manager.pyã€safety.pyã€watermark.py
- core/trainï¼š__init__.pyã€dataset.pyã€evaluators.pyã€registry.py
- core/vlmï¼š__init__.py
- core/__init__.py
- workersï¼šå…¨éƒ¨ï¼ˆcelery_appã€tasksã€utilsï¼‰

å„ªå…ˆé †åºèˆ‡è¦æ±‚ï¼ˆå‹™å¿…ç…§é †åºå®Œæˆï¼‰
1) è·¯ç”±èˆ‡è¨»å†Š
   - å¯«å¥½ api/routers/__init__.py åŒ¯å‡º routers æ¸…å–®ï¼›api/main.py çµ±ä¸€ include_routerï¼Œprefix çµ±ä¸€ç‚º /api/v1ã€‚
   - å°å¤–å…¬é–‹è·¯ç”±éœ€å¯ç”¨ï¼šhealth, chat, game(story), rag, t2i, controlnet, lora, batch, caption, vqaã€‚
   - å…è¨±ä»¥ã€Œè–„å°è£ã€å°æ¥ç¾æœ‰æ¨¡çµ„ï¼ˆä¾‹å¦‚ chat åŒ…è£ llmï¼›game åŒ…è£ storyï¼‰ã€‚

2) t2i æ ¸å¿ƒæœ€å°å¯¦ä½œ
   - å®Œæˆ core/t2i/{controlnet,lora_manager,safety,watermark,__init__} æœ€å°å¯ç”¨ç‰ˆæœ¬ã€‚
   - æä¾› save_image_to_cache()ï¼šè¼¸å‡ºåœ–èˆ‡ metadata åˆ° $AI_CACHE_ROOT/outputs/...ã€‚

3) Schemas
   - æ–°å¢ api/schemasï¼ˆæˆ–ç­‰åƒ¹è·¯å¾‘ï¼‰ï¼Œå®šç¾© T2I/Chat/Caption/VQA/Game/Batch çš„è«‹æ±‚èˆ‡å›æ‡‰ Pydantic æ¨¡å‹ã€‚
   - æ¬„ä½çµ±ä¸€ï¼špromptã€negativeã€stepsã€widthã€heightã€seedã€lora_idsã€controlnet_typeâ€¦ç­‰ã€‚

4) Configs
   - è£œ configs/{app.yaml,models.yaml} èˆ‡ configs/train/{dreambooth.yaml,lora-sdxl.yaml}ï¼ˆéµå€¼å« model_idã€resolutionã€rankã€lrã€batch_sizeã€max_stepsã€output_dir ç­‰ï¼‰ã€‚

5) RAG æœ€å°ç‰ˆ
   - core/rag/* å¯¦ä½œæœ€å°å¯ç”¨ç®¡ç·šï¼ˆç´”æ–‡å­— parser + sentence-transformers + FAISS æª¢ç´¢ + ç°¡å–® rerankï¼‰ã€‚

6) Workers æœ€å°ç‰ˆ
   - å»ºç«‹ workers/celery_app.py èˆ‡ tasks/batch.pyï¼šæä¾› batch.submit / batch.status æµç¨‹ï¼ˆè¨˜æ†¶é«”æˆ– Redis ä»»ä¸€ç¨®å³å¯ï¼‰ã€‚

7) å…¶é¤˜
   - è£œé½Šå„å±¤ __init__.pyï¼Œé¿å… ImportErrorã€‚
   - æ‰€æœ‰å…¥å£éœ€å…ˆè·‘ Shared Cache Bootstrapï¼ˆAI_CACHE_ROOTï¼‰å†è¼‰å…¥é‡æ¨¡çµ„ï¼›ä¸å¾—æŠŠå¤§å‹æª”æ¡ˆå¯«å…¥ repoã€‚

é©—æ”¶æ¢ä»¶ï¼ˆè‡ªæˆ‘æª¢æŸ¥ï¼‰
- GET /api/v1/health â†’ 200ï¼ŒJSON è‡³å°‘å« { "gpu_available": bool, "cache_root": str }ã€‚
- POST /api/v1/txt2img â†’ 200ï¼ŒJSON å« { "image_path", "metadata_path", "elapsed_ms" }ã€‚
- GET /api/v1/lora/list â†’ 200ï¼ˆå¯ç‚ºç©ºé™£åˆ—ï¼‰ï¼›POST /api/v1/lora/load â†’ å°ä¸å­˜åœ¨ id å›å‚³ 4xx ä¸”è¨Šæ¯æ¸…æ¥šã€‚
- POST /api/v1/controlnet/poseï¼ˆå¯æ¥å—ç°¡åŒ–è¼¸å…¥ï¼‰â†’ 200ï¼Œå›å‚³å·²å¥—ç”¨çš„æ¢ä»¶æ‘˜è¦ã€‚
- POST /api/v1/chat â†’ 200ï¼Œå›å‚³ {"message": "..."}ï¼ˆå¯ç‚ºç°¡åŒ–è¼¸å‡ºï¼‰ã€‚
- POST /api/v1/rag/ask â†’ 200ï¼Œå›å‚³ {"answer": "...", "sources": [...] }ï¼ˆsources å¯ç‚ºæœ€å°å¯ç”¨ï¼‰ã€‚
- POST /api/v1/batch/submit â†’ å›å‚³ job_idï¼›GET /api/v1/batch/status/{id} â†’ queued|running|done|errorã€‚

æäº¤ç‰©
- å–®ä¸€ PRï¼šå«ä¸Šè¿°æª”æ¡ˆèˆ‡ç¨‹å¼ç¢¼ï¼ŒREADME ç‰‡æ®µï¼ˆå•Ÿå‹•æ–¹å¼èˆ‡ AI_CACHE_ROOT èªªæ˜ï¼‰ï¼Œä»¥åŠç°¡çŸ­ CHANGELOGã€‚
- ç¨‹å¼èˆ‡æäº¤è¨Šæ¯è«‹ä»¥è‹±æ–‡æ’°å¯«ï¼›éŒ¯èª¤è¨Šæ¯éœ€æ¸…æ¥šå¯è®€ã€‚


å¦‚æœä½ éœ€è¦ã€Œæ›´çŸ­åˆ°æ¥µé™ã€çš„ç‰ˆæœ¬ï¼ˆåªä¿ç•™æœªå®Œæˆæ¸…å–®ï¼‹å„ªå…ˆé †åºï¼‹é©—æ”¶ä¸‰è¡Œï¼‰ï¼Œæˆ‘ä¹Ÿå¯ä»¥å†å£“ç¸®ä¸€ç‰ˆçµ¦ä½ ã€‚

You said:
é¦–å…ˆæˆ‘è¦å…ˆé‡å°coreçš„éƒ¨åˆ† ç›®å‰çœ‹ä¾†åŸºæœ¬çš„æª”æ¡ˆ å¦‚ __int__ config performance shared_cache é€™äº›.py éƒ½éœ€è¦æ›´æ­£ å®ƒå€‘èˆ‡coreå…¶ä»–foler filesæœ‰é‡è¦çš„é—œä¿‚ ä¹Ÿéœ€è¦ä¿®æ­£å®Œæ‰èƒ½èª¿æ•´ api å’Œå…¶routersçš„éƒ¨åˆ†ã€‚ç›®å‰èƒ½è«‹ä½ å°±ä»¥ä¸Šæ‰€æœ‰è³‡è¨ŠåŒ…å«project filesç­‰ å‘Šè¨´æˆ‘é€™æ¨£çš„æƒ³æ³•æ˜¯æ­£ç¢ºçš„å— ä¸¦å»ºè­°æˆ‘æ¥ä¸‹ä¾†çš„åšæ³•
ChatGPT said:
Thought for 19s

çµè«–ï¼šä½ çš„æƒ³æ³•æ˜¯å°çš„ã€‚
å…ˆæŠŠ core ç©©å®šï¼ˆ__init__.py / config.py / performance.py / shared_cache.pyï¼‰â†’ å†ä¿® api/dependencies.py èˆ‡å„ routers/*ã€‚åŸå› æ˜¯ï¼šæ‰€æœ‰è·¯ç”±éƒ½è¦é  core çš„è¨­å®šã€å¿«å–è·¯å¾‘ã€è£ç½®é¸æ“‡èˆ‡å–®ä¾‹ï¼ˆt2i/rag/llm/vlm/storyï¼‰ä¾†å·¥ä½œï¼›è‹¥ core çš„ä»‹é¢èˆ‡è¼‰å…¥æµç¨‹ä¸ç©©ï¼ŒAPI åªæœƒä¸€ç›´ç¢è£‚ã€‚

ä¸‹é¢çµ¦ä½ ä¸€å¥—ã€Œæœ€å°è€Œæ­£ç¢ºã€çš„åŸ·è¡Œé †åºèˆ‡éª¨æ¶ï¼Œç…§åšå°±èƒ½æŠŠå¾Œç«¯ç«™ç©©ï¼Œä¹‹å¾Œå†è£œé€²ç´°åŠŸèƒ½ã€‚

å»ºè­°åŸ·è¡Œé †åºï¼ˆå…ˆ coreã€å¾Œ APIï¼‰
Phase 1 â€” ç©©å®š core åŸºåº§ï¼ˆä¸€å®šå…ˆåšï¼‰

ç›®æ¨™ï¼šçµ±ä¸€è¨­å®šã€å¿«å–ã€è£ç½®èˆ‡å–®ä¾‹å…¥å£ï¼›è®“ä»»ä½•è·¯ç”±éƒ½èƒ½ç©©å®šå–å¾—å¼•æ“ã€‚

core/shared_cache.pyï¼ˆå•Ÿå‹•æ™‚å…ˆè·‘ï¼‰

å»ºç«‹ä¸¦å›å‚³æ¨™æº–è³‡æ–™å¤¾ï¼š{cache_root}/{models,data,outputs,logs,tmp}

è² è²¬å°å‡ºå•Ÿå‹•è¨Šæ¯ï¼ˆæ–¹ä¾¿ Health æª¢æŸ¥ï¼‰ã€‚

# core/shared_cache.py
from pathlib import Path
import os, logging
def bootstrap() -> dict:
    root = Path(os.getenv("AI_CACHE_ROOT", "/mnt/ai/cache")).resolve()
    dirs = {k: root / k for k in ["models","data","outputs","logs","tmp"]}
    for d in dirs.values(): d.mkdir(parents=True, exist_ok=True)
    logging.getLogger(__name__).info(f"[cache] root={root}")
    return {"root": str(root), **{k: str(v) for k,v in dirs.items()}}


core/config.pyï¼ˆå–®ä¸€çœŸç›¸ä¾†æºï¼‰

ç”¨ Pydantic Settings è®€ .env/YAMLï¼›æä¾› settings() å–®ä¾‹ã€‚

# core/config.py
from pydantic import BaseModel, Field
from functools import lru_cache
import os

class AppSettings(BaseModel):
    api_prefix: str = "/api/v1"
    device: str = Field(default="cuda" if os.getenv("USE_CUDA","1")=="1" else "cpu")
    cache_root: str = Field(default_factory=lambda: os.getenv("AI_CACHE_ROOT","/mnt/ai/cache"))

@lru_cache
def settings() -> AppSettings:
    return AppSettings()


core/performance.pyï¼ˆå…±ç”¨å·¥å…·ï¼‰

ç°¡å–®ç§’è¡¨ / VRAM å¯ç”¨æª¢æŸ¥ / æ‰¹æ¬¡å¤§å°å»ºè­°ã€‚

# core/performance.py
import time, torch
class Timer:
    def __enter__(self): self.t=time.perf_counter(); return self
    def __exit__(self,*_): self.ms=int((time.perf_counter()-self.t)*1000)
def gpu_available() -> bool:
    try: return torch.cuda.is_available()
    except Exception: return False


core/__init__.pyï¼ˆæœ€é—œéµï¼šä»‹é¢ + æ‡¶è¼‰å…¥å–®ä¾‹ï¼‰

å®šç¾©æœ€å°ä»‹é¢ï¼ˆProtocols / ABCï¼‰ï¼Œæä¾› lazy loader èˆ‡ registryã€‚

# core/__init__.py
from typing import Protocol, Any, Dict
from functools import lru_cache
from .config import settings
from .shared_cache import bootstrap

# 1) Interfaces
class T2IEngine(Protocol):
    def txt2img(self, prompt:str, **kw) -> Dict[str,Any]: ...
class LoRAManager(Protocol):
    def list(self)->list: ...
    def load(self, lora_id:str)->bool: ...
class ControlNetProcessor(Protocol):
    def apply(self, cond:Dict[str,Any], **kw)->Dict[str,Any]: ...
class RAGEngine(Protocol):
    def ask(self, query:str)->Dict[str,Any]: ...
class LLMEngine(Protocol):
    def chat(self, messages:list)->str: ...
class VLMEngine(Protocol):
    def caption(self, image_path:str)->str: ...
    def vqa(self, image_path:str, question:str)->str: ...
class GameEngine(Protocol):
    def new(self, seed:int|None=None)->Dict[str,Any]: ...
    def step(self, state:Dict[str,Any], choice:int)->Dict[str,Any]: ...

# 2) Lazy singletons (replace with real impls later)
@lru_cache
def registry() -> Dict[str, Any]:
    # side-effects: ensure cache folders ready
    cache_info = bootstrap()
    # NOTE: å…ˆä»¥ã€Œå¯å›å‚³å‡è³‡æ–™ã€çš„æœ€å°å¯¦ä½œï¼Œç­‰è·¯ç”±é€šäº†å†æ›¿æ›
    from .t2i.pipeline import MinimalT2I
    from .vlm.captioner import MinimalCaptioner
    from .rag.engine import MinimalRAG
    from .llm.adapter import MinimalLLM
    from .story.engine import MinimalStory
    return {
        "cache": cache_info,
        "t2i": MinimalT2I(),
        "lora": MinimalT2I().lora,          # æš«æ™‚å€Ÿç”¨
        "controlnet": MinimalT2I().control, # æš«æ™‚å€Ÿç”¨
        "rag": MinimalRAG(),
        "llm": MinimalLLM(),
        "vlm": MinimalCaptioner(),
        "game": MinimalStory(),
    }


è¨»ï¼šMinimalT2I / MinimalCaptioner / MinimalRAG / MinimalLLM / MinimalStory å¯ä»¥å…ˆå›å‚³ å‡è³‡æ–™ï¼ˆæˆ–æœ€å°ç”Ÿæˆï¼‰ç¢ºä¿ API æœ‰å›æ‡‰ï¼Œä¹‹å¾Œå†é€æ­¥æ›æˆçœŸæ­£çš„å¯¦ä½œã€‚

Phase 2 â€” è£œæœ€å°å¯¦ä½œï¼ˆå¯å…ˆå‡è³‡æ–™ï¼‰

ç›®æ¨™ï¼šè®“è·¯ç”±ä¸å† 500ï¼Œå…¨éƒ¨ 200ã€‚

core/t2i/pipeline.pyï¼šç”¢ç”Ÿä¸€å¼µç©ºç™½åœ– + å¯« metadataï¼›æ›å‡º lora èˆ‡ control å­ç‰©ä»¶çš„å‡å¯¦ä½œã€‚

core/rag/engine.pyï¼šå›å‚³ echo ç­”æ¡ˆèˆ‡ sources=[]ã€‚

core/llm/adapter.pyï¼šæŠŠ messages æ‹¼èµ·ä¾†å›è¦†ä¸€æ®µå›ºå®šå¥å­ï¼ˆæˆ–èµ° transformers å°æ¨¡å‹ï¼‰ã€‚

core/vlm/captioner.pyï¼šå›è¦†å›ºå®š Captionï¼›vqa() ä¹Ÿå…ˆå›å›ºå®šç­”æ¡ˆã€‚

core/story/engine.pyï¼šnew() å›åˆå§‹ stateã€step() ä¾ choice ç”¢å‡ºä¸‹ä¸€æ®µã€‚

é€™äº›æª”æ¡ˆåç¨±å·²åœ¨ä½ çš„ç›®éŒ„ä¸­ï¼Œå…ˆç¢ºä¿å®ƒå€‘å­˜åœ¨ä¸”åŒ¯å…¥æˆåŠŸï¼Œå°±èƒ½æ‰“é€šè·¯ç”±ã€‚

Phase 3 â€” å°é½Š api/dependencies.py èˆ‡å„ routers/*

api/dependencies.py ç›´æ¥å¾ core.registry() å–å‡ºå–®ä¾‹ï¼š

# api/dependencies.py
from fastapi import Depends
from core import registry
def get_registry():
    return registry()
def get_t2i(reg=Depends(get_registry)): return reg["t2i"]
# åŒç† get_lora/get_control/get_rag/get_llm/get_vlm/get_game/get_cache


ä¿® api/main.pyï¼šinclude_router(..., prefix=settings().api_prefix)ï¼Œä¸¦æŠŠ health æ¥åˆ° performance.gpu_available() èˆ‡ registry()["cache"]ã€‚

åœ¨ routers å…§ï¼šç”¨ Depends(get_xxx) å–å¼•æ“ï¼Œä¸è¦è‡ªè¡Œ newï¼Œé¿å…å¤šä»½ç‹€æ…‹èˆ‡å¿«å–è·¯å¾‘ä¸ä¸€è‡´ã€‚

Phase 4 â€” Config / YAML å°é½Š

å…ˆæ”¾æœ€å°å¯ç”¨çš„ configs/app.yamlã€configs/models.yamlã€configs/train/{dreambooth.yaml,lora-sdxl.yaml}ï¼ˆå°±ç®—æ˜¯ placeholderï¼Œä¹Ÿè®“ config.py èƒ½è®€åˆ°ï¼‰ã€‚

ä¹‹å¾Œå†é€æ­¥å¡«å…¥çœŸå€¼ã€‚

Phase 5 â€” Health & åŸºæœ¬ E2E é©—æ”¶

GET /api/v1/healthï¼šå› {"gpu_available": bool, "cache_root": "..."}

POST /api/v1/txt2imgï¼šå›ä¸€å€‹æª”æ¡ˆè·¯å¾‘ï¼ˆå°±ç®—æ˜¯å‡åœ–ï¼‰ï¼›elapsed_ms ä¾†è‡ª Timerã€‚

POST /api/v1/chatã€/rag/askã€/captionã€/vqaã€/game/new|stepï¼šçš†å› 200ã€‚

ç‚ºä»€éº¼è¦å…ˆåšé€™å››æ”¯æª”æ¡ˆï¼Ÿ

core/__init__.pyï¼šæ±ºå®šå¤–éƒ¨å¯è¦‹çš„ä»‹é¢èˆ‡è¼‰å…¥æ–¹å¼ï¼Œç­‰æ–¼æ ¸å¿ƒã€Œåˆç´„ã€ã€‚

core/config.pyï¼šæ¶ˆé™¤åƒæ•¸é£„ç§»ï¼ˆè·¯ç”±ã€coreã€workers å°åŒä¸€ä»½è¨­å®šï¼‰ã€‚

core/shared_cache.pyï¼šé¿å…åˆ°è™•å„è‡ªå¯«æª”èˆ‡å¿«å–ï¼Œç¢ºä¿ç›®éŒ„æ¬Šé™èˆ‡å¯æ”œæ€§ã€‚

core/performance.pyï¼šæ‰€æœ‰è·¯ç”±éƒ½éœ€è¦ç§’è¡¨èˆ‡ GPU åµæ¸¬ï¼Œå¥åº·æª¢æŸ¥ä¹Ÿä¾è³´å®ƒã€‚

å…ˆæŠŠé€™å››å€‹å›ºå®šä½ï¼Œå¾Œé¢æ”¹ä»»ä½•å¼•æ“åªéœ€éµå®ˆä»‹é¢å³å¯ï¼Œä¸æœƒå†é€£é–å´©ã€‚

ä½ å¯ä»¥ç›´æ¥åŸ·è¡Œçš„ã€Œä¸‹ä¸€æ­¥ã€

å»ºåˆ†æ”¯ï¼šfix/core-bootstrap-and-registry

æ–°å¢/è¦†è“‹ä¸Šè¿°å››æª”ï¼ˆcore/__init__.py, config.py, shared_cache.py, performance.pyï¼‰ï¼‹ æ¯å€‹ minimal å¼•æ“æª”çš„ã€Œå‡å¯¦ä½œéª¨æ¶ã€ã€‚

èª¿æ•´ api/dependencies.py â†’ èµ° core.registry()ã€‚

ä¿® api/main.py â†’ prefix ä¾†è‡ª settings().api_prefixï¼Œæ–°å¢ health å›å ± gpu_available + cache_rootã€‚

è·‘æœ€å°é©—æ”¶ï¼ˆcurl æˆ– Swaggerï¼‰ã€‚

æäº¤ï¼š

feat(core): add unified settings, cache bootstrap and lazy registry

refactor(api): wire routers to core registry via dependencies

feat(core): add minimal placeholder engines (t2i/rag/llm/vlm/story)

feat(api): add health endpoint with gpu and cache info

ç­‰ core ç©©å®šå¾Œï¼Œå†ä¾†è£œ t2i/controlnet/lora_manager çœŸå¯¦é‚è¼¯ã€rag pipelineã€workersã€ä»¥åŠå„è·¯ç”±çš„ Pydantic schema ç´°ç¯€ã€‚
å¦‚æœä½ è¦ï¼Œæˆ‘å¯ä»¥æŠŠ Minimal å¼•æ“æª”ï¼ˆæ¯æ”¯ ~20â€“40 è¡Œçš„å¯å›æ‡‰å‡è³‡æ–™ç‰ˆæœ¬ï¼‰ä¹Ÿç›´æ¥ç”Ÿçµ¦ä½ ï¼Œè®“ä½ ä»Šå¤©å°±èƒ½æŠŠ API æ­£å¸¸å› 200ã€‚

You said:
coreåŸºæœ¬ç›¸é—œç¨‹å¼æª”.txt
Document
å¥½çš„ ä»¥ä¸‹æ˜¯æˆ‘ç›®å‰ core åŸºæœ¬çš„é€™äº›ç›¸é—œ.pyæª”æ¡ˆ è«‹å‘Šè¨´æˆ‘ç›®å‰æœ‰å“ªä¸€äº›å•é¡Œéœ€è¦ä¿®æ­£ æˆ‘ç›®å‰æœ‰ç™¼ç¾åˆ°çš„æ˜¯ è·Ÿå…¶ä»–core folder filesä¹‹é–“çš„å”èª¿æ€§  åƒæ˜¯config.py ä»¥åŠshared_cache.py å°±æ˜¯æ•´é«”å°ˆæ¡ˆç›¸é—œçš„åŸºæœ¬è¨­å®š åŒ…å«å„ç¨®modelç­‰è·¯å¾‘è¨­å®šç­‰ï¼Œå› æ­¤æˆ‘æƒ³æ‡‰è©²å…¶ä»–fileæœ‰éœ€è¦é‹ç”¨é€™äº›åƒæ•¸çš„éƒ¨åˆ† æ‡‰è©²éƒ½æ˜¯è¦å¾é€™é‚Šå®šç¾©å¥½çš„å»å¥—ç”¨ï¼Œè€Œä¸æ˜¯ æ¯å€‹æª”æ¡ˆ åˆå…±åŒçš„é–‹é ­ å¾æ–°è¨­å®š ä¸€æ¨¡ä¸€æ¨£çš„ AI_CACHE_ROOT æˆ–å…¶ä»–APP_DIRç­‰?
ChatGPT said:
Thought for 27s

ä½ çš„åˆ¤æ–·å®Œå…¨æ­£ç¢ºï¼šcore æ˜¯æ•´å€‹å¾Œç«¯çš„åœ°åŸºã€‚å…ˆæŠŠ core çš„ã€Œå–®ä¸€çœŸç›¸ä¾†æºï¼ˆconfig + shared_cacheï¼‰ï¼‹çµ±ä¸€å…¥å£ï¼ˆcore/init.pyï¼‰ï¼‹æ•ˆèƒ½/è³‡æºç®¡ç†ï¼ˆperformanceï¼‰ã€ç©©å®šï¼ŒAPI/routers æ‰æœ‰è¾¦æ³•å°é½Šï¼Œä¸ç„¶æ¯å€‹æ¨¡çµ„å„è‡ªå®£å‘Š AI_CACHE_ROOTã€å„è‡ªæ±ºå®š device èˆ‡è·¯å¾‘ï¼Œä¸€å®šæœƒäº’ç›¸æ‰“æ¶ã€‚ä½ ç›®å‰çš„ core ç¨‹å¼æª”æˆ‘å·²çœ‹éï¼Œä¸‹é¢æ˜¯ç«‹å³è¦ä¿®èˆ‡å»ºè­°åšæ³•ï¼ˆéƒ½ä»¥ä½ è²¼çš„æª”æ¡ˆç‚ºæº–ï¼‰ã€‚

å…ˆä¿®é‡é»ï¼ˆç›´æ¥èƒ½è¸©åˆ°é›·çš„åœ°æ–¹ï¼‰

ã€Œç’°å¢ƒè®Šæ•¸ç›´è®€ã€æ•£è½å„æª”
ä½ åœ¨ core/train/registry.pyã€core/train/evaluators.pyã€core/t2i/pipeline.py ç­‰ä»ä»¥ os.getenv("AI_CACHE_ROOT", "...") æˆ– f-string æ‰‹å‹•æ‹¼è·¯å¾‘ï¼ˆä¾‹å¦‚ .../models/clipï¼‰ã€‚é€™æœƒé€ æˆï¼š

è·¯å¾‘ä¸ä¸€è‡´ï¼ˆä¸åŒæª”é è¨­å€¼ä¸åŒï¼‰

ç„¡æ³•ä¸€æ¬¡èª¿æ•´ï¼ˆæ”¹ cache root éœ€å…¨ repo æœå°‹æ›¿æ›ï¼‰

â–¶ æ”¹æ³•ï¼šå…¨éƒ¨æ”¹è®€ get_config().cache.root æˆ– get_shared_cache() å–å¾—æ¨™æº–è·¯å¾‘éµå€¼ï¼Œå†ç”± SharedCache.get_path(key) å–ç”¨ã€‚ä¾‹å¦‚æ”¹æˆï¼š

from core.shared_cache import get_shared_cache
cache = get_shared_cache()
clip_dir = cache.get_path("MODELS_EMBEDDING")  # æˆ–åŠ ä¸€å€‹ "MODELS_CLIP"


æˆ–ï¼š

from core.config import get_config
cache_root = get_config().cache.root


ï¼ˆå…©è€…æ“‡ä¸€ï¼Œä¸è¦åŒæª”å…©ç¨®éƒ½ç”¨ï¼‰

performance.py å–è¨­å®šè·¯å¾‘éŒ¯èª¤
PerformanceOptimizer.__init__ ç”¨ self.settings = get_config() å¾Œæ¥è‘— .redis_urlï¼Œä½† redis_url æ˜¯ CacheConfig è£¡çš„å±¬æ€§ï¼Œæ‡‰è©²æ˜¯ get_config().cache.redis_urlã€‚
â–¶ ä¿®æˆï¼š

self.settings = get_config()
self.redis_client = redis.from_url(self.settings.cache.redis_url)


ï¼ˆåŒç†ï¼Œè‹¥è¦çœ‹ cache rootã€æ¨¡å‹è£ç½®ç­‰ï¼Œéƒ½è¦å¾å°æ‡‰å­è¨­å®šæ‹¿ï¼‰

åœ¨åŒ¯å…¥æœŸåšã€Œå‰¯ä½œç”¨ã€å‹•ä½œ
t2i/pipeline.py ç›®å‰ from core.shared_cache import bootstrap_cache ä¸¦åœ¨æ¨¡çµ„é ‚éƒ¨å°±å‘¼å«ï¼Œæœƒè®“ä»»ä½• import é€™æ”¯æª”æ¡ˆçš„åœ°æ–¹éƒ½è§¸ç™¼ I/O èˆ‡ GPU æª¢æŸ¥ï¼ˆåœ¨ worker/æ¸¬è©¦/å·¥å…·è…³æœ¬å°¤å…¶å®¹æ˜“ç‚¸ï¼‰ã€‚
â–¶ æ”¹æ³•ï¼š

æŠŠ bootstrap_cache() æ”¾åˆ°æ‡‰ç”¨å•Ÿå‹•é»ï¼ˆå¦‚ api/main.py çš„ startup eventï¼‰æˆ– core.registry() çš„ç¬¬ä¸€æ¬¡å‘¼å«æ™‚æ‰åšï¼ˆlazyï¼‰ã€‚

åœ¨æ¨¡çµ„é ‚éƒ¨ä¸è¦å‘¼å« bootstrapï¼›éœ€è¦ cache ç‰©ä»¶æ™‚æ‰ get_shared_cache()ã€‚

core/__init__.py çš„å…¬é–‹ API è¦ã€Œç©©å®š + ç°¡æ½”ã€
ä½ ç›®å‰ re-export get_t2i_pipeline, save_image_to_cache, LoRAManager, DocumentMemoryã€‚è«‹ç¢ºå®šï¼š

åç¨±èˆ‡å¯¦ä½œçœŸçš„å­˜åœ¨ã€ä¸”ç›¸ä¾æ–¹å‘å–®ç´”ï¼ˆé¿å…äº’ç›¸ importï¼‰ã€‚

å»ºè­°æŠŠã€Œå°å¤–åˆç´„ã€é›†ä¸­ç‚ºå°‘é‡å…¥å£ï¼ˆä¾‹å¦‚ registry() å–å„ engineï¼›æˆ– get_t2i_pipeline() é€™ç¨®å›ºå®šåç¨±ï¼‰ã€‚

performance.py åŒ¯å…¥é‡é‡ç´šå¥—ä»¶çš„æ™‚æ©Ÿ
ç¾åœ¨åœ¨æª”æ¡ˆé ‚éƒ¨ import redis, sentence_transformersï¼Œä¸¦å»ºç«‹å…¨åŸŸ optimizer = PerformanceOptimizer()ï¼Œå®¹æ˜“åœ¨é–‹æ©Ÿæˆ–æ¸¬è©¦æ™‚å°±åƒè³‡æºï¼é€£ç·šå¤±æ•—ã€‚
â–¶ æ”¹æ³•ï¼ˆå¯åˆ†å…©æ­¥ï¼‰ï¼š

ä¿ç•™å…¨åŸŸå–®ä¾‹ï¼Œä½†æŠŠ expensive import ç§»åˆ°æ–¹æ³•å…§æˆ– @lru_cache åŒ…è£ï¼ˆä½ å·²æœ‰ @lru_cache çš„ get_modelï¼Œå¾ˆå¥½ï¼‰ã€‚

é€£åˆ° Redis å¤±æ•—æ™‚ï¼Œå‹™å¿… fallback æˆã€Œè¨˜æ†¶é«”å¿«å–ã€ä¸”ä¸æ‹‹è‡´å‘½ä¾‹å¤–ï¼ˆä½ å·²æœ‰ try/exceptï¼Œå»¶çºŒé€™é¢¨æ ¼ï¼‰ã€‚

sys.path.append(ROOT_DIR) é€™é¡ hack
t2i/pipeline.py é€é ROOT_DIR+sys.path.append ä¾†è§£ç›¸å°åŒ¯å…¥ï¼Œé€™åœ¨å¥—ä»¶åŒ–å¾Œæœƒå‡ºå•é¡Œã€‚
â–¶ æ”¹æ³•ï¼šæ”¹ç”¨ç›¸å°åŒ¯å…¥ï¼ˆfrom ..shared_cache import get_shared_cacheï¼‰ï¼Œç¢ºä¿ core è¢«ç•¶ä½œå¥—ä»¶å®‰è£æˆ–ä»¥ repo æ ¹ç›®éŒ„åŸ·è¡Œæ™‚éƒ½èƒ½åŒ¯å…¥ã€‚

ä¸€æ¬¡åˆ°ä½çš„ã€Œå°é½Šæ–¹æ¡ˆã€

æŠŠã€Œè¨­å®šã€ã€Œè·¯å¾‘ã€ã€Œè£ç½®ã€ã€Œå¿«å–ã€çš„è®€å–é›†ä¸­åœ¨ coreï¼Œè®“å…¶ä»–æ¨¡çµ„åªæ‹¿ã€Œå€¼ã€ï¼Œä¸å†å„è‡ªæ±ºå®šã€‚

A. core/config.pyï¼ˆå–®ä¸€çœŸç›¸ä¾†æºï¼‰

ç›®å‰è¨­è¨ˆå·²å¾ˆå¥½ï¼ˆAppConfig èšåˆ api/model/rag/database/cache å­è¨­å®šï¼Œä¸¦å« YAML fallbackï¼‰ã€‚

çµ±ä¸€ç”¨æ³•ï¼šåœ¨æ‰€æœ‰æ¨¡çµ„åªç”¨ from core.config import get_config; cfg = get_config()ï¼Œç„¶å¾Œç”¨ cfg.cache.root / cfg.model.device_map / cfg.api.port é€™ç¨®çµæ§‹åŒ–å–å€¼ï¼Œä¸è¦ os.getenvã€‚

B. core/shared_cache.pyï¼ˆæ¨™æº–å€‰å„²ï¼‰

ä½ å·²å»ºç«‹ SharedCache èˆ‡ get_shared_cache() / bootstrap_cache()ï¼Œä¸¦å®šç¾©å„è³‡æ–™å¤¾éµå€¼ï¼ˆMODELS_SD, OUTPUT_LORA, RAG_INDEXâ€¦ï¼‰ã€‚

çµ±ä¸€ç”¨æ³•ï¼šæ‰€æœ‰éœ€è¦å¯¦é«”è·¯å¾‘çš„åœ°æ–¹ç”¨ cache = get_shared_cache(); path = cache.get_path("XXX")ã€‚ä¸è¦ç›´æ¥æ‹¼ f"{AI_CACHE_ROOT}/..."ã€‚å¦‚æ²’æœ‰éµå€¼å°±æ–°å¢éµåï¼Œé¿å…é­”æ³•å­—ä¸²ã€‚

C. core/performance.pyï¼ˆçµ±ä¸€æ•ˆèƒ½å·¥å…·ï¼‰

åªæš´éœ²ï¼šoptimizerï¼ˆå¯é¸ï¼‰ã€Timerã€gpu_memory_cleanup()ã€memory_monitorã€gpu_available()ï¼ˆä½ å¯ç”¨ torch.cuda.is_available() å°è£ä¸€å€‹ç°¡å–®å‡½å¼ï¼‰ã€‚

router ç«¯éœ€è¦é‡æ¸¬å°±ç”¨ with Timer() as t:ï¼›éœ€è¦å¥åº·æª¢æŸ¥å°±å›å ± torch.cuda.is_available() èˆ‡ get_shared_cache().get_summary()ã€‚

D. å•Ÿå‹•æµç¨‹ï¼ˆé¿å…å‰¯ä½œç”¨ï¼‰

åœ¨ api/main.py çš„ FastAPI startup eventï¼ˆæˆ–ç¬¬ä¸€æ¬¡å‘¼å« core.registry()ï¼‰å‘¼å« bootstrap_cache()ï¼›

å…¶é¤˜æ¨¡çµ„åªå‘¼å« get_shared_cache()ï¼Œä¸åš I/O å° logã€‚

å»ºè­°ä½ ç¾åœ¨å°±åšçš„ 6 å€‹ commitï¼ˆå¾ˆå°é¡†ï¼Œé¢¨éšªä½ï¼‰

config çµ±ä¸€å…¥å£

å°‡ repo å…§æ‰€æœ‰ os.getenv("AI_CACHE_ROOT"... å…¨éƒ¨æ›¿æ›ç‚º get_config().cache.root æˆ– get_shared_cache()ã€‚

Commit: refactor(core): unify cache root usage via get_config()/get_shared_cache()

shared_cache éµå€¼è¦†è“‹

åœ¨ SharedCache._create_directories() è£œè¶³ä½ å¯¦éš›æœƒç”¨åˆ°çš„éµï¼ˆä¾‹å¦‚ MODELS_CLIPï¼‰ã€‚

Commit: feat(core): add cache dir keys for clip/models and registry paths

performance è¨­å®šè·¯å¾‘ä¿®æ­£

performance.py å…§ Redis è®€å–æ”¹ç‚º get_config().cache.redis_urlï¼›é‡è¦ import ç§»å…¥æ–¹æ³•æˆ– try/exceptã€‚

Commit: fix(core): correct redis_url source and lazy heavy imports in performance

ç§»é™¤ import-time bootstrap

ç§»é™¤ t2i/pipeline.py é ‚éƒ¨çš„ bootstrap_cache() å‘¼å«ï¼›åœ¨ api/main.py çš„ startup æˆ– core.registry() å…§ lazy åŸ·è¡Œã€‚

Commit: refactor(core): avoid import-time bootstrap; run cache bootstrap on startup

è·¯å¾‘ hack ç§»é™¤

åˆªæ‰ sys.path.append èˆ‡ ROOT_DIRï¼›æ”¹æˆç›¸å°åŒ¯å…¥ã€‚

Commit: chore(core): remove sys.path hacks and use package-relative imports

health endpoint å°é½Š

GET /api/v1/health å›å‚³ï¼šgpu_available, cache_rootï¼ˆç”± get_shared_cache().cache_rootï¼‰ã€å¯èƒ½å†åŠ  env_vars ä¸­çš„ CUDA_VISIBLE_DEVICESã€‚

Commit: feat(api): enrich /health with gpu and cache summary

å°æé†’ï¼ˆä¹‹å¾Œå†åšä¹Ÿè¡Œï¼‰

å‹åˆ¥ï¼Protocolï¼šåœ¨ core/__init__.py å®£å‘Š T2IEngine/RAGEngine/... çš„ Protocolï¼Œè·¯ç”±åªä¾è³´ä»‹é¢ï¼Œä¸ä¾è³´å¯¦ä½œï¼Œå¾Œé¢æ›æ¨¡å‹ä¸æœƒå‹•åˆ° routersã€‚

æœ€å°å‡å¯¦ä½œï¼šè‹¥æœ‰æ¨¡çµ„é‚„æ²’å®Œæˆï¼Œå…ˆæ”¾å¯å› 200 çš„æœ€å° stubï¼ˆä¾‹å¦‚ t2i å…ˆç”¢ç”Ÿä¸€å¼µç´”è‰²åœ– + metadataï¼‰ï¼ŒæŠŠç®¡ç·šæ‰“é€šå†é€æ­¥å¼·åŒ–ã€‚

é¿å…å¾ªç’°ä¾è³´ï¼šcore/__init__.py ä¸è¦ import ä»»ä½•æœƒåå‘ import core çš„æª”ï¼ˆä¿æŒã€Œæœ€å…§å±¤ã€ï¼‰ã€‚

å¦‚æœä½ é¡˜æ„ï¼Œæˆ‘å¯ä»¥ç›´æ¥çµ¦ä½ æ¯æ”¯æª”æ¡ˆçš„ã€Œä¿®æ­£ç‰ˆæœ€å°éª¨æ¶ã€ï¼ˆconfig/shared_cache/performance/__init__ åŠ t2i/pipeline çš„ç„¡å‰¯ä½œç”¨ç‰ˆï¼‰ï¼Œä½ åªè¦è²¼ä¸Šå°±èƒ½æŠŠ core ç©©ä½ï¼Œå†ä¾†ä¿® routersã€‚

You said:
ç¾åœ¨lora_trainer.py ä¸­æœ‰å®šç¾© class TrainingConfig è€Œé€™å¾Œé¢çš„ç¨‹å¼ç¢¼ä¹Ÿéƒ½æœ‰é‹ç”¨åˆ°é€™å€‹å®šç¾©çš„class config å¯æ˜¯æˆ‘åœ¨æƒ³ ä¾ç…§æˆ‘å€‘å°ˆæ¡ˆçš„çµæ§‹ é€™å€‹å®šç¾©çš„ TrainingConfig æ˜¯ä¸æ˜¯æ‡‰è©²æ”¾åˆ°åˆ¥çš„æª”æ¡ˆç„¶å¾Œåšimport ä¾‹å¦‚åƒ config.py æˆ– shared_cache.py? æ­¤å¤– ç›®å‰ registry.py æœƒç”¨åˆ° model_registry.json é€™å€‹æ˜é¡¯å°±è¦æ”¾åœ¨ shared_cache.py è£¡é¢ è«‹å‘Šè¨´æˆ‘ä»¥ä¸Šé€™äº›å¯ä»¥å¦‚ä½•ä¿®æ”¹ ä¸¦å‘Šè¨´æˆ‘åŸå› ç­‰? æœ‰å…¶ä»–å»ºè­°ä¹Ÿä¸€ä¸¦èªªæ˜
ChatGPT said:
Thought for 10s

ä½ çš„æ–¹å‘æ²’éŒ¯ï¼Œè€Œä¸”è¦ã€Œåˆ†å±¤ã€æ€è€ƒï¼š

core/config.pyï¼šå±¬æ–¼ã€Œæ‡‰ç”¨å±¤è¨­å®šã€ï¼ˆAPI å‰ç¶´ã€deviceã€AI_CACHE_ROOTâ€¦ï¼‰ï¼å…¨ç³»çµ±å…±ç”¨çš„ç’°å¢ƒåƒæ•¸ã€‚

core/shared_cache.pyï¼šåªè² è²¬ã€Œè·¯å¾‘èˆ‡è³‡æ–™å¤¾çš„å–®ä¸€çœŸç›¸ä¾†æºã€èˆ‡å»ºç«‹ç›®éŒ„ï¼ˆmodels/ã€outputs/ã€rag/â€¦ï¼‰ï¼›ä¹Ÿæ‡‰è©²çµ±ä¸€æª”å/è·¯å¾‘è³‡æºï¼ˆåƒ model_registry.jsonï¼‰ã€‚

core/train/ï¼šæ˜¯ã€Œè¨“ç·´é ˜åŸŸé‚è¼¯ã€ã€‚é€™è£¡çš„ TrainingConfig å±¬æ–¼é ˜åŸŸæ¨¡å‹ï¼ˆDomain configï¼‰ï¼Œä¸æ‡‰æ”¾é€² app config æˆ– shared_cacheã€‚

æ‰€ä»¥ï¼š

æ”¾ç½®ä½ç½®çš„å»ºè­°

TrainingConfig ç•™åœ¨ core/train/
å»ºç«‹æ–°æª” core/train/config_schema.pyï¼ˆæˆ– schemas.pyï¼‰ï¼Œå°ˆé–€æ”¾ã€Œè¨“ç·´ç›¸é—œçš„è¨­å®šæ¨¡å‹ã€ã€‚

ç†ç”±ï¼šå®ƒæ˜¯è¨“ç·´é ˜åŸŸçš„ schemaï¼Œèˆ‡ app å±¤ï¼ˆAPI prefixã€cache rootâ€¦ï¼‰ä¸åŒï¼›æœªä¾†é‚„æœƒæœ‰ DreamBoothConfigã€LoRAConfig( sd15 / sdxl ) ä¹‹é¡ï¼Œé›†ä¸­åœ¨ train å¥—ä»¶æ¯”è¼ƒä¹¾æ·¨ã€‚

lora_trainer.py åª from .config_schema import TrainingConfigï¼Œé¿å… trainer æª”æ¡ˆå¤ªèƒ–ã€‚

model_registry.json çš„è·¯å¾‘äº¤çµ¦ shared_cache ç®¡

åœ¨ SharedCache._create_directories() è£œä¸€å€‹ keyï¼Œä¾‹å¦‚ "TRAIN_REGISTRY": f"{self.cache_root}/train/model_registry.json"ã€‚

core/train/registry.py æ”¹æˆ from core.shared_cache import get_shared_cache ç„¶å¾Œ cache.get_path("TRAIN_REGISTRY") å–å¾—è·¯å¾‘ï¼Œä¸è¦å† os.getenv(...) æˆ–æ‰‹åˆ» f-stringã€‚

é€™æ¨£å¯ä»¥ç¢ºä¿ï¼š

è·¯å¾‘åªåœ¨ shared_cache å®šç¾©ä¸€æ¬¡ï¼ˆæ”¹å/æ›å±¤ä¸æœƒç‚¸å…¨ repoï¼‰ã€‚

è¨“ç·´çš„è¨­å®šèˆ‡ç¨‹å¼ç¶­æŒã€Œé ˜åŸŸå…§èšã€ï¼Œapp/config ä¸æœƒè¶Šé•·è¶Šé›œã€‚

ä¾è³´æ–¹å‘æ¸…æ¥šï¼šconfig & shared_cacheï¼ˆåŸºåº§ï¼‰â†’ train/t2i/rag/vlm/storyï¼ˆé ˜åŸŸï¼‰â†’ apiï¼ˆä»‹é¢å±¤ï¼‰ã€‚

å…·é«”ä¿®æ”¹ï¼ˆå°è€Œç©©çš„ refactorï¼‰
1) æ–°å¢ core/train/config_schema.py

æŠŠä½ åœ¨ lora_trainer.py çš„ @dataclass TrainingConfig æ¬éä¾†ï¼ˆä¿ç•™ from_yamlï¼‰ï¼Œä¸¦åŠ ä¸Šé è¨­å€¼èˆ‡æœ€å°é©—è­‰ï¼š

# core/train/config_schema.py
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import yaml

@dataclass
class TrainingConfig:
    base_model: str
    model_type: str          # "sd15" | "sdxl"
    resolution: int
    rank: int
    alpha: float
    learning_rate: float
    text_encoder_lr: float
    train_steps: int
    batch_size: int
    gradient_accumulation_steps: int
    seed: int
    mixed_precision: str
    gradient_checkpointing: bool
    use8bit_adam: bool
    caption_dropout: float
    min_snr_gamma: float
    noise_offset: float
    validation_prompts: List[str]
    validation_steps: int
    num_validation_images: int
    dataset: Dict[str, Any]
    output: Dict[str, Any]
    optimizations: Dict[str, Any]

    @classmethod
    def from_yaml(cls, yaml_path: str) -> "TrainingConfig":
        with open(yaml_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)
        # ç°¡å–®é˜²å‘†ï¼šå¿…å¡«éµ
        required = ["base_model","model_type","resolution","rank","learning_rate","train_steps","batch_size","dataset","output"]
        missing = [k for k in required if k not in cfg]
        if missing:
            raise ValueError(f"TrainingConfig missing keys: {missing}")
        return cls(**cfg)


lora_trainer.py ä¸­æ”¹æˆï¼š

from .config_schema import TrainingConfig

2) shared_cache.py æ–°å¢è¨»å†Šæª” key
# core/shared_cache.py ï¼ˆç¯€éŒ„ï¼‰
self.app_dirs = {
    # ... æ—¢æœ‰ keys ...
    "TRAIN_REGISTRY": f"{self.cache_root}/train/model_registry.json",
}


ä¸¦ç¢ºä¿ _create_directories ä¸åªå»ºè³‡æ–™å¤¾ï¼Œä¹Ÿæœƒ touch ä¸Šå±¤è³‡æ–™å¤¾ï¼›TRAIN_REGISTRY æ˜¯æª”æ¡ˆï¼Œä¸éœ€è¦é å…ˆå»ºç«‹æª”æ¡ˆï¼Œä½†å…¶çˆ¶ç›®éŒ„éœ€å­˜åœ¨ã€‚

3) core/train/registry.py æ”¹ç‚ºä½¿ç”¨ shared_cache

æŠŠ AI_CACHE_ROOT / REGISTRY_FILE æ‹”æ‰ï¼Œæ”¹å¯«æˆï¼š

# core/train/registry.py
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from core.shared_cache import get_shared_cache

class ModelRegistry:
    """Keep track of trained models and their metadata"""

    def __init__(self):
        cache = get_shared_cache()
        self.registry_path = Path(cache.get_path("TRAIN_REGISTRY"))
        self.registry_path.parent.mkdir(parents=True, exist_ok=True)
        self.registry = self._load_registry()

    def _load_registry(self) -> Dict[str, Any]:
        if self.registry_path.exists():
            try:
                with open(self.registry_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return {"models": {}, "version": "1.0"}

    def _save_registry(self) -> None:
        with open(self.registry_path, "w", encoding="utf-8") as f:
            json.dump(self.registry, f, indent=2, ensure_ascii=False)

    def add(self, model_id: str, info: Dict[str, Any]) -> None:
        info["updated_at"] = datetime.utcnow().isoformat()
        self.registry["models"][model_id] = info
        self._save_registry()

    def get(self, model_id: str) -> Optional[Dict[str, Any]]:
        return self.registry["models"].get(model_id)

    def list(self) -> List[Dict[str, Any]]:
        return [
            {"id": k, **v} for k, v in sorted(self.registry["models"].items(),
            key=lambda kv: kv[1].get("updated_at",""))
        ]

4) å…¶ä»–æª”æ¡ˆçš„ç”¨æ³•çµ±ä¸€

ä»»ä½•éœ€è¦è·¯å¾‘çš„åœ°æ–¹ï¼Œéƒ½æ”¹æˆï¼š

from core.shared_cache import get_shared_cache
cache = get_shared_cache()
clip_dir = cache.get_path("MODELS_CLIP")
output_dir = cache.get_path("OUTPUT_DIR")


ä»»ä½•éœ€è¦ app è¨­å®šï¼ˆè£ç½®ã€prefixï¼‰ï¼š

from core.config import get_config
cfg = get_config()
device = cfg.model.device          # ä¾ä½ çš„çµæ§‹å‘½å
api_prefix = cfg.api.prefix

ç‚ºä»€éº¼ä¸æŠŠ TrainingConfig æ”¾åˆ° core/config.py æˆ– shared_cache.pyï¼Ÿ

core/config.py æ˜¯å¹³å°/æ‡‰ç”¨è¨­å®šï¼Œä¸æ‡‰æ··é€²é ˜åŸŸç‰¹æœ‰çš„è¨“ç·´åƒæ•¸ï¼›å¦å‰‡ config æœƒè¶Šä¾†è¶Šé¾é›œï¼Œä¸” APIã€RAGã€VLM ä¹Ÿæœƒè«åä¾è³´è¨“ç·´ç´°ç¯€ï¼Œå¢åŠ è€¦åˆã€‚

shared_cache.py çš„è²¬ä»»æ˜¯ã€Œè·¯å¾‘èˆ‡ç›®éŒ„ã€ï¼›å®ƒä¸è©²çŸ¥é“ã€Œè¨“ç·´æœ‰å“ªäº›è¶…åƒæ•¸ã€ã€‚ä¸ç„¶ shared_cache ä¸€æ—¦è®Šå‹•ï¼Œæœƒç‰½å‹•è¨“ç·´è¡Œç‚ºï¼Œè²¬ä»»é‚Šç•Œæ¨¡ç³Šã€‚

æŠŠ TrainingConfig ç•™åœ¨ core/train/ å¯ä»¥ï¼š

ä¿æŒé«˜å…§èšã€ä½è€¦åˆï¼šè¨“ç·´é‚è¼¯æ”¹å‹•ï¼Œä¸æœƒæ³¢åŠ app config/è·¯ç”±ã€‚

å…è¨±æœªä¾†å¤šç¨®è¨“ç·´ schemaï¼ˆDreamBoothã€Textual Inversionã€LoRA-SDXLï¼‰ä¸¦å­˜ï¼›lora_trainer.py ä¾è³´çš„åªæ˜¯ã€Œè¨“ç·´ç”¨ schemaã€ã€‚

å»ºè­°çš„æäº¤é †åºï¼ˆ4 é¡†å° commitï¼‰

feat(train): move TrainingConfig to core/train/config_schema.py and import from trainer

feat(core): add TRAIN_REGISTRY path in shared_cache and ensure parent directory

refactor(train): make ModelRegistry use shared_cache path instead of env

refactor(core): replace direct AI_CACHE_ROOT usages with get_shared_cache()/get_config() where present

åšå®Œé€™æ³¢ï¼Œä½ çš„ core åŸºå±¤å°±è®Šæˆã€Œå–®ä¸€çœŸç›¸ä¾†æºã€ï¼š

app åƒæ•¸ â†’ get_config()ï¼›

æ‰€æœ‰è·¯å¾‘/æª”æ¡ˆ â†’ get_shared_cache().get_path(KEY)ï¼›

è¨“ç·´é ˜åŸŸè¨­å®š â†’ core/train/config_schema.pyï¼›
æ¥ä¸‹ä¾†å†å»è£œ t2i/rag/vlm ç­‰æ¨¡çµ„ï¼Œåªè¦ä¹–ä¹–ä¾è³´ä¸Šè¿°å…¥å£ï¼ŒAPI/routers å°±èƒ½ç©©å®šå°é½Šã€‚

You said:
å¥½ æˆ‘ç¾åœ¨è¦æ›åˆ°è™•ç†api/routersçš„éƒ¨åˆ† ç¾åœ¨æ–°å¢èª¿æ•´å¾Œ æœ‰é€™äº›routers æ ¹æ“šæˆ‘å€‘çš„coreç›¸é—œæª”æ¡ˆç­‰ é€™äº›routerséƒ½æœ‰å…·å‚™å—? æ¥ä¸‹ä¾†æˆ‘è¦å„è‡ªdebugæ¯å€‹router ä¹Ÿè¦ç¢ºå®šå„è‡ªçš„è·¯ç”±è¨­å®šç­‰æ˜¯å¦æ­£ç¢º ä¹Ÿè¦ç¢ºä¿è·Ÿ åŸºæœ¬æª”æ¡ˆä¹Ÿå°±æ˜¯ dependencies.py main.py middleware.py schemas.py ä¹‹é–“çš„è¯çµæ€§ æˆ‘é€™æ¨£çš„ä½œæ³•æ˜¯æ­£ç¢ºæ–¹å‘å—?
ChatGPT said:
Thought for 12s

çµè«–ï¼šä½ çš„æ–¹å‘å®Œå…¨æ­£ç¢ºã€‚
å…ˆæŠŠ core ç©©åˆ°ã€Œå¯è¢«ä¾è³´ã€ï¼Œå†ä¾†ã€Œé€ä¸€æ ¡æ­£ routersã€ï¼Œä¸¦ç¢ºä¿ dependencies.py / main.py / middleware.py / schemas.py ä¸€èµ·æˆå¥—é‹ä½œã€‚ä¸‹é¢çµ¦ä½ ä¸€ä»½ã€Œå°ç…§è¡¨ + Debug æ¸…å–® + æ¥ç·šç¯„æœ¬ + é©—æ”¶æ­¥é©Ÿã€ï¼Œç…§é€™å€‹è·‘å°±èƒ½æŠŠ API ç·šè·¯ç©©ä½ã€‚

1) Router å°ç…§è¡¨ï¼ˆæ˜¯å¦æœ‰å°æ‡‰ core èƒ½åŠ›ï¼‰
Router	ç›®çš„	éœ€è¦çš„ core ä¾è³´	ä½ ç¾æ³ï¼ˆå·²è¦‹æª”ï¼‰	å‚™è¨»
health.py	å¥åº·æª¢æŸ¥	performance.gpu_available(), shared_cache	âœ…	æœ€å…ˆæ‰“é€š
t2i.py	æ–‡ç”Ÿåœ–	core.t2i(pipeline)	âœ…	æœ€æ ¸å¿ƒï¼›å…ˆåšæœ€å°å¯ç”¨
controlnet.py	æ¢ä»¶æ§åˆ¶	core.t2i.controlnet	âœ…	å¯å…ˆ stub å›æ‡‰
lora.py	LoRA ç®¡ç†	core.t2i.lora_manager	âœ…	å…ˆ list/load/unload
chat.py	å°è©±	core.llm	âœ…	ä¹Ÿå¯æ²¿ç”¨ llm.py
caption.py	åœ–èªª	core.vlm	âœ…
vqa.py	è¦–è¦ºå•ç­”	core.vlm	âœ…
game.py	åŠ‡æƒ…å¼•æ“	core.story	âœ…	èˆ‡ story.pyå°æ‡‰ï¼›ä¿ç•™ä¸€å€‹å³å¯
rag.py	æª¢ç´¢å•ç­”	core.rag	âœ…	å…ˆåšæœ€å°ç®¡ç·š
batch.py	æ‰¹æ¬¡	workers.tasks	âœ…(æª”)	å¯å…ˆå› mock job ç‹€æ…‹
admin.py / export.py / finetune.py / monitoring.py / safety.py / vlm.py / story.py / llm.py	å…§éƒ¨/æ“´å……	è¦–æƒ…æ³	âœ…	å…ˆæ¨™è¨˜ç‚ºå…§éƒ¨ route æˆ–åˆä½µåˆ°ä¸Šé¢åŠŸèƒ½

é‡é»ï¼šæ¯æ”¯ router éƒ½ä¸è¦ã€Œç›´æ¥ new å¼•æ“ã€ï¼Œä¸€å¾‹é€é dependencies.py æ‹¿å–®ä¾‹ã€‚

2) Debug æª¢æŸ¥æ¸…å–®ï¼ˆæ¯æ”¯ router éƒ½è·‘ä¸€æ¬¡ï¼‰

å‘½åèˆ‡è·¯å¾‘

 router = APIRouter(prefix="/xxx", tags=["xxx"])ï¼ˆæˆ–ç„¡ prefixï¼Œç”± main.py çµ±ä¸€ /api/v1ï¼‰

 endpoint path ä¸è¦é‡è¦†åŠ  /api/v1ï¼ˆé¿å…é›™é‡å‰ç¶´ï¼‰

DI æ³¨å…¥

 ä¾è³´çµ±ä¸€ä¾†è‡ª api/dependencies.pyï¼š
def get_t2i(...), get_lora(...), get_controlnet(...), get_rag(...), get_llm(...), get_vlm(...), get_game(...), get_cache(...), get_settings(...)

 åœ¨ handlerï¼šengine = Depends(get_t2i) ç­‰ç­‰

Schemas

 è«‹æ±‚/å›æ‡‰å‹åˆ¥ä½¿ç”¨ schemas.pyï¼ˆæˆ– api/schemas/ åˆ†æª”ï¼‰
ä¾‹å¦‚ T2IRequest/T2IResponseã€ChatRequest/Responseã€VQARequest/Responseã€BatchSubmit/Statusâ€¦

 å›å‚³ BaseModel æˆ– JSONResponseï¼Œä¸è¦è£¸ dict + æœªè²æ˜å‹åˆ¥

éŒ¯èª¤è™•ç†

 æ•æ‰é æœŸéŒ¯èª¤â†’ HTTPException(status_code=400|404, detail="...")

 ä»»ä½• I/O è®€å¯«éƒ½ try/except ä¸¦åŠ ä¸Šç°¡çŸ­ detail

æ•ˆèƒ½èˆ‡æ—¥èªŒ

 é—œéµè·¯ç”±åŠ  with Timer() as t: è¨ˆæ™‚ï¼›å›å‚³ elapsed_ms

 é—œéµ error logger.exception()ï¼Œé¿å…åéŒ¯

OpenAPI

 æ¯å€‹è·¯ç”±éƒ½æœ‰ response_model èˆ‡ summary/descriptionï¼Œexamples å¯ç°¡çŸ­å¡«ä¸€å€‹

3) dependencies.py / main.py / middleware.py / schemas.py çš„ã€Œæ¥ç·šç¯„æœ¬ã€

dependencies.pyï¼ˆé—œéµï¼ï¼‰

# api/dependencies.py
from fastapi import Depends
from core import registry        # ä½ çš„ lazy å–®ä¾‹å…¥å£
from core.config import get_config

def get_registry():
    return registry()

def get_settings():
    return get_config()

def get_cache(reg=Depends(get_registry)):
    return reg["cache"]

def get_t2i(reg=Depends(get_registry)):
    return reg["t2i"]

def get_lora(reg=Depends(get_registry)):
    return reg["lora"]

def get_controlnet(reg=Depends(get_registry)):
    return reg["controlnet"]

def get_llm(reg=Depends(get_registry)):
    return reg["llm"]

def get_vlm(reg=Depends(get_registry)):
    return reg["vlm"]

def get_game(reg=Depends(get_registry)):
    return reg["game"]

def get_rag(reg=Depends(get_registry)):
    return reg["rag"]


main.py

# api/main.py
from fastapi import FastAPI
from core.config import get_config
from core.shared_cache import bootstrap_cache
from .routers import routers         # ä½ çš„ __init__.py åŒ¯å‡º routers æ¸…å–®
from .middleware import add_middlewares

cfg = get_config()
app = FastAPI(title="Anime Adventure Lab API")

@app.on_event("startup")
async def _startup():
    bootstrap_cache()   # åœ¨é€™è£¡åšï¼Œä¸è¦ import time åš
    # ä¹Ÿå¯åœ¨é€™è£¡ warmup å°‘é‡è³‡æº

add_middlewares(app)

for r in routers:
    app.include_router(r, prefix=cfg.api_prefix)  # ç¢ºä¿çµ±ä¸€å‰ç¶´


middleware.pyï¼ˆä¾‹ï¼‰

# api/middleware.py
from fastapi.middleware.cors import CORSMiddleware

def add_middlewares(app):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"], allow_credentials=True,
        allow_methods=["*"], allow_headers=["*"],
    )


schemas.pyï¼ˆä¾‹ï¼‰

# api/schemas.py
from pydantic import BaseModel, Field
from typing import List, Optional

class T2IRequest(BaseModel):
    prompt: str
    negative: Optional[str] = None
    steps: int = 8
    width: int = 512
    height: int = 512
    seed: Optional[int] = None
    lora_ids: List[str] = []

class T2IResponse(BaseModel):
    image_path: str
    metadata_path: str
    elapsed_ms: int


router ç¯„æœ¬ï¼ˆt2iï¼‰

# api/routers/t2i.py
from fastapi import APIRouter, Depends
from ..schemas import T2IRequest, T2IResponse
from ..dependencies import get_t2i
from core.performance import Timer

router = APIRouter(tags=["t2i"])

@router.post("/txt2img", response_model=T2IResponse, summary="Text-to-Image")
def txt2img(req: T2IRequest, t2i = Depends(get_t2i)):
    with Timer() as t:
        result = t2i.txt2img(
            prompt=req.prompt, negative=req.negative, steps=req.steps,
            width=req.width, height=req.height, seed=req.seed, lora_ids=req.lora_ids
        )
    return T2IResponse(**result, elapsed_ms=t.ms)

4) é©—æ”¶æ­¥é©Ÿï¼ˆå¾ˆå¿«æ’æŸ¥ 80% å•é¡Œï¼‰

è·¯ç”±è¨»å†Š

å•Ÿå‹•å¾Œæ‰“ï¼šGET /openapi.jsonï¼Œæª¢æŸ¥æ˜¯å¦åŒ…å«
/api/v1/health, /api/v1/txt2img, /api/v1/chat, /api/v1/caption, /api/v1/vqa, /api/v1/lora/*, /api/v1/controlnet/*, /api/v1/rag/*, /api/v1/batch/*, /api/v1/game/*

è‹¥ç¼º â†’ é€šå¸¸æ˜¯ routers/__init__.py æ²’æŠŠè©² router æ”¾é€²æ¸…å–®ã€æˆ– prefix é‡è¦†/éŒ¯èª¤ã€‚

å¥åº·æª¢æŸ¥

GET /api/v1/health æ‡‰å›ï¼šgpu_availableã€cache_rootï¼ˆä¾†è‡ª shared_cacheï¼‰

æ ¸å¿ƒåŠŸèƒ½ smoke

POST /api/v1/txt2imgï¼ˆæœ€å°åƒæ•¸ï¼‰â†’ 200 + image_path

GET /api/v1/lora/list â†’ 200

POST /api/v1/chat â†’ 200ï¼ˆå¯å› stubï¼‰

POST /api/v1/rag/ask â†’ 200ï¼ˆå¯å› stubï¼‰

å°ç…§ä¾è³´

è‹¥è·‘åˆ° ImportError æˆ– NoneType has no attribute ...ï¼Œ90% æ˜¯ dependencies.py æ²’æ‹¿åˆ°å–®ä¾‹ï¼ˆregistry() æ²’å»ºç«‹ã€æˆ– key åç¨±æ‰“éŒ¯ï¼‰ã€‚

5) å¸¸è¦‹å‘ï¼ˆå…ˆé¿é–‹ï¼‰

é›™é‡ prefixï¼šåœ¨ router è¨­äº† prefix="/api/v1/t2i" åˆåœ¨ main.py ç”¨ prefix="/api/v1" â†’ è®Š /api/v1/api/v1/t2iã€‚å»ºè­°çµ±ä¸€äº¤çµ¦ main.pyã€‚

ç›´æ¥ os.getenvï¼šæ‰€æœ‰è·¯å¾‘/è£ç½®è«‹æ”¹ç”¨ get_config() èˆ‡ get_shared_cache()ã€‚

import æ™‚å°±åš I/Oï¼šé¿å…åœ¨ module é ‚éƒ¨å‘¼å« bootstrap_cache()ã€ä¸‹è¼‰æ¨¡å‹ã€é€£ Redisã€‚

æ²’å®šç¾© response_modelï¼šå°è‡´ Swagger ä¸å¥½ç”¨ã€å‰ç«¯é›£ä¸²æ¥ï¼›è«‹åœ¨æ¯å€‹ handler è¨­ response_modelã€‚

å¾ªç’°ä¾è³´ï¼šrouter åª import dependencies/schemas èˆ‡ core çš„ä»‹é¢ï¼Œä¸è¦ç›¸äº’ import å…¶ä»– router æˆ–æ·±å±¤å¯¦ä½œã€‚

6) æ¥ä¸‹ä¾†çš„åŸ·è¡Œé †åºï¼ˆå»ºè­°ï¼‰

å…ˆé€šéï¼šhealth â†’ t2iï¼ˆæœ€å°ï¼‰â†’ chat â†’ caption/vqa â†’ lora/controlnet â†’ rag â†’ batch â†’ game

æ¯é€šéä¸€å€‹ï¼Œå°±åœ¨ routers/__init__.py è£œä¸Šä¸¦ commitï¼š

feat(api): wire /t2i endpoints to core.t2i via dependencies

æœ€å¾Œå›é ­è£œ schemas çš„æ¬„ä½èˆ‡éŒ¯èª¤è™•ç†ç´°ç¯€ï¼Œæ›´æ–° README çš„ curl ç¯„ä¾‹ã€‚

éœ€è¦çš„è©±ï¼Œæˆ‘å¯ä»¥å¹«ä½ æƒä¸€éæ¯æ”¯ router çš„ç¾æœ‰ç¨‹å¼ï¼ˆè²¼å‡ºé‡é»æ®µè½ä¹Ÿè¡Œï¼‰ï¼Œé€æ”¯åˆ—å‡ºã€Œå·®å“ªäº›ä¾è³´ã€è©²åŠ çš„ response_modelã€éœ€è¦çš„éŒ¯èª¤è™•ç†ã€â€”æŒ‰é€™å€‹ checklist ä¿®ï¼Œå¾ˆå¿«å°±æœƒç©©ã€‚

You said:
dependencies.py
Python
main.py
Python
ä½ å‰›æ‰èªªåˆ°é‡é» æ¯æ”¯ router éƒ½ä¸è¦ã€Œç›´æ¥ new å¼•æ“ã€ï¼Œä¸€å¾‹é€é dependencies.py æ‹¿å–®ä¾‹ã€‚ ç›®å‰çœ‹ä¾† é‡å° dependencies.py ä¹Ÿå°±æ˜¯é™„æª” é€™éƒ¨åˆ†å°±æ²’å®Œæˆ æˆ‘ä¹Ÿç™¼ç¾ä¸å°‘å•é¡Œ åŒ…åƒæ˜¯ æ²’æœ‰çµ±ä¸€ç”¨ä¹‹å‰çš„shared_cache åˆåœ¨é€™è£¡é‡æ–°å®šç¾©ï¼Œæ­¤å¤–æ¯å€‹ å„è‡ªroutersæ‰€éœ€è¦çš„dependency ä¹Ÿæ²’æœ‰éƒ½å®Œæˆã€‚ è«‹å†å¹«æˆ‘æª¢æŸ¥å…¶ä»–å•é¡Œ ä¸¦å‘Šè¨´æˆ‘å¦‚ä½•è£œé½Šé€™äº›ç¼ºå¤±å¯¦ä½œ åŠå•é¡Œç­‰
ChatGPT said:
Thought for 27s

ä½ æŠ“åˆ°é—œéµäº†ï¼šç¾åœ¨çš„ dependencies.py è‡ªæˆä¸€å¥—ã€Œå°å®‡å®™ã€ï¼ˆè‡ªå·±å®šç¾© AI_CACHE_ROOTã€APP_DIRSã€ç’°å¢ƒè®Šæ•¸ã€ç›´æ¥ new å¼•æ“ï¼‰ï¼Œé€™æœƒå’Œ core/config.pyã€core/shared_cache.pyã€ä»¥åŠä¹‹å¾Œçš„ registry() äº’ç›¸æ‰“æ¶ã€‚å†åŠ ä¸Š main.py ä¹Ÿæœ‰ä¸€äº›å•Ÿå‹•/å¥åº·æª¢æŸ¥çš„é‡è¤‡é‚è¼¯ï¼Œæ•´å€‹ API æœƒè®Šå¾—é›£ä»¥ç¶­è­·ã€‚ä»¥ä¸‹æ˜¯å•é¡Œé»èˆ‡ã€Œå¯ç›´æ¥æ›¿æ›ã€çš„ä¿®æ­£ç‰ˆç¯„æœ¬ã€‚

é—œéµå•é¡Œï¼ˆä½ é™„çš„å…©å€‹æª”æ¡ˆï¼‰

dependencies.pyï¼ˆç¯€éŒ„é‡é»ï¼‰

åœ¨é€™å€‹æª”æ¡ˆè£¡åˆé‡æ–°å®šç¾© AI_CACHE_ROOTã€APP_DIRSï¼Œä¸¦åœ¨ import æœŸé–“å°± mkdir èˆ‡è¨­å®šä¸€å †ç’°å¢ƒè®Šæ•¸ï¼ˆå‰¯ä½œç”¨éå¤§ã€èˆ‡ shared_cache é‡è¤‡ï¼‰ã€‚

åªåšäº† VLM/WD14 çš„å–®ä¾‹ï¼Œå…¶ä»– router éœ€è¦çš„ä¾è³´ï¼ˆt2iã€loraã€controlnetã€ragã€llmã€gameã€cacheã€settingsâ€¦ï¼‰æ²’æœ‰çµ±ä¸€å‡ºå£ã€‚

get_vlm_config() æŠŠ get_config() ç•¶ dict ç”¨ï¼ˆconfig.get("vlm", {})ï¼‰ï¼Œè‹¥ get_config() å›å‚³çš„æ˜¯ Pydantic ç‰©ä»¶æœƒå‡ºéŒ¯ï¼ä¸ä¸€è‡´ã€‚

get_rag_engine() ç›´æ¥ new ChineseRAGEngine(APP_DIRS["RAG_INDEX"])ï¼Œç¹é core çš„å–®ä¾‹èˆ‡å¿«å–è·¯å¾‘é›†ä¸­ç®¡ç†ã€‚

main.pyï¼ˆç¯€éŒ„é‡é»ï¼‰

ä½¿ç”¨ sys.path.append(ROOT_DIR) çš„åŒ¯å…¥ hackã€‚

å®šç¾©äº† /healthzï¼Œä½†åŒæ™‚ä½ ä¹ŸæŠŠ health.router ä»¥ /api/v1 prefix æ›é€²ä¾†ï¼Œå¥åº·æª¢æŸ¥è·¯ç”±é‡è¤‡ã€è·¯å¾‘é¢¨æ ¼ä¸ä¸€è‡´ï¼Œä¸” /healthz å›å‚³çš„ cache_root ç›´æ¥è®€ os.getenv è€Œé shared_cacheã€‚

lifespan ä¸­å·²ç¶“åš bootstrap_cache()ï¼Œä½† dependencies.py åˆè‡ªè¡Œå»ºè³‡æ–™å¤¾èˆ‡ç’°å¢ƒè®Šæ•¸ï¼Œé€ æˆé‡è¤‡å‰¯ä½œç”¨ã€‚

ä½ è©²æ€éº¼ä¿®ï¼ˆç²¾ç°¡ä¸€è‡´çš„ä¾è³´è¨­è¨ˆï¼‰
1) dependencies.pyï¼šåªåšã€Œæ‹¿å–®ä¾‹ã€èˆ‡ã€Œè½‰äº¤è¨­å®šã€

ç”¨ core çš„å–®ä¸€å…¥å£ï¼Œä¸è¦è‡ªå·±ç®¡ç†å¿«å–èˆ‡è·¯å¾‘ï¼Œä¸è¦åœ¨åŒ¯å…¥éšæ®µåšå‰¯ä½œç”¨ã€‚

æ›¿æ›ç¯„æœ¬ï¼ˆå¯ç›´æ¥è¦†è“‹ä½ çš„ api/dependencies.pyï¼‰ï¼š

# api/dependencies.py
from functools import lru_cache
from fastapi import Depends
from typing import Any, Dict

# 1) å…¨éƒ¨å¾ core å–å–®ä¾‹/è¨­å®š/å¿«å–ï¼Œé€™æ˜¯å”¯ä¸€çœŸç›¸ä¾†æº
from core import registry                    # lazy å–®ä¾‹ï¼št2i/rag/llm/vlm/game/lora/control/cache
from core.config import get_config           # App/Pydantic è¨­å®š
from core.shared_cache import get_shared_cache  # æ¨™æº–è·¯å¾‘ï¼ˆç„¡å‰¯ä½œç”¨ï¼‰

# ---- åŸºæœ¬æ³¨å…¥ ----
def get_registry() -> Dict[str, Any]:
    return registry()

def get_settings():
    return get_config()

def get_cache():
    return get_shared_cache()

# ---- å„å¼•æ“æ³¨å…¥ï¼ˆæ‰€æœ‰ router éƒ½ç”¨é€™äº›ï¼Œä¸è¦ newï¼‰ ----
def get_t2i(reg=Depends(get_registry)):         return reg["t2i"]
def get_lora(reg=Depends(get_registry)):        return reg["lora"]
def get_controlnet(reg=Depends(get_registry)):  return reg["controlnet"]
def get_llm(reg=Depends(get_registry)):         return reg["llm"]
def get_vlm(reg=Depends(get_registry)):         return reg["vlm"]
def get_game(reg=Depends(get_registry)):        return reg["game"]
def get_rag(reg=Depends(get_registry)):         return reg["rag"]

# ----ï¼ˆå¯é¸ï¼‰æä¾›çµ±ä¸€çš„ VLM è¨­å®šå­˜å–ï¼Œè€Œä¸æ˜¯åœ¨æ­¤åšé è¨­åˆä½µ ----
@lru_cache
def get_vlm_settings():
    cfg = get_config()
    # ä¾ä½ çš„ AppSettings çµæ§‹å›å‚³éœ€è¦çš„æ¬„ä½ï¼Œé¿å…åœ¨é€™è£¡å¯« dict-merge é‚è¼¯
    return getattr(cfg, "vlm", None)


é€™æ¨£ä¸€ä¾†ï¼š

ä¸å†æœ‰ AI_CACHE_ROOTã€APP_DIRS çš„é‡è¤‡å®šç¾©èˆ‡å‰¯ä½œç”¨ã€‚

ä»»ä½• router æƒ³è¦å¼•æ“ï¼è¨­å®šï¼è·¯å¾‘ï¼Œåªé€éé€™è£¡æ‹¿ã€‚

çœŸå¯¦çš„è·¯å¾‘å»ºç«‹èˆ‡ç’°å¢ƒè®Šæ•¸ç”± bootstrap_cache()/shared_cache ç®¡ï¼›ä¾è³´æ³¨å…¥æª”æ¡ˆä¿æŒã€Œç´”æ·¨ã€ã€‚

ï¼ˆå°ç…§ä½ çš„ç¾æ³å¯è¦‹ï¼šä½ ç¾åœ¨åœ¨ dependencies å…§å»º APP_DIRSã€new RAG/Captioner ç­‰ï¼Œé€™äº›éƒ½æ‡‰ç§»é™¤ã€‚
ï¼‰

2) main.pyï¼šçµ±ä¸€è·¯å¾‘èˆ‡å•Ÿå‹•ï¼Œç§»é™¤ hack

ç§»é™¤ sys.path.appendã€‚

/health èµ° health.routerï¼Œä¸è¦å†å¦å¤–ç¶­è­· /healthzï¼›è‹¥è¦ä¿ç•™èˆŠè·¯å¾‘ï¼Œå¯åœ¨ health router è£¡åŠ  aliasã€‚

lifespan ç¶­æŒ bootstrap_cache()ï¼›dependencies ä¸å†åšä»»ä½• mkdir/env è¨­å®šã€‚

prefix çµ±ä¸€ cfg.api_prefixï¼ˆæˆ–å›ºå®š /api/v1ï¼‰ã€‚

é‡é»ä¿®æ­£ï¼ˆå°ç…§ä½ çš„æª”æ¡ˆï¼‰ï¼š

# api/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

from core.shared_cache import bootstrap_cache
from core.config import get_config
from api.routers import routers   # åœ¨ routers/__init__.py åŒ¯å‡ºæ‰€æœ‰ router ç‰©ä»¶

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ API starting...")
    bootstrap_cache()   # é€™è£¡åšå°±å¥½
    yield
    logger.info("ğŸ›‘ API shutdown")

def create_app() -> FastAPI:
    cfg = get_config()
    app = FastAPI(title="Anime Adventure Lab API", lifespan=lifespan)

    app.add_middleware(
        CORSMiddleware,
        allow_origins=cfg.api.cors_origins_list,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # çµ±ä¸€åœ¨é€™è£¡æ›æ‰€æœ‰ routerï¼Œå‰ç¶´ä¹Ÿçµ±ä¸€
    for r in routers:
        app.include_router(r, prefix=cfg.api.prefix)

    return app

app = create_app()


ä½ çš„ main.py ç›®å‰åŒæ™‚æœ‰ /healthz èˆ‡ health.routerï¼Œè€Œä¸”å¥åº·æª¢æŸ¥è®€ os.getenv("AI_CACHE_ROOT")ï¼Œé€™æœƒå’Œ shared_cache åˆ†é›¢ã€‚å»ºè­°åˆªæ‰ /healthzï¼Œæ‰€æœ‰å¥åº·æª¢æŸ¥èµ° health.routerï¼ˆè®“å®ƒå»è®€ get_shared_cache() èˆ‡ gpu_available()ï¼‰ã€‚

Router ç«¯éœ€è¦æ€éº¼æ”¹ï¼Ÿ

æ¯æ”¯ router ä¾è³´éƒ½å¾ api/dependencies.py æ‹¿ï¼ˆä¸Šé¢æ–°çš„ get_xxxï¼‰ã€‚

çµ•å°ä¸è¦ from core.xxx import Something å¾Œè‡ªå·± Something()ã€‚

å›å‚³å‹åˆ¥ç”¨ schemas.py çš„ Pydantic modelsã€‚

è‹¥æŸ router éœ€è¦è·¯å¾‘ï¼Œæ‹¿ cache = Depends(get_cache)ï¼Œå† cache.get_path("OUTPUT_DIR") ç­‰ã€‚

è‹¥éœ€è¦è¨­å®šï¼Œæ‹¿ cfg = Depends(get_settings)ã€‚

ç¯„ä¾‹ï¼š

# api/routers/lora.py
from fastapi import APIRouter, Depends, HTTPException
from ..dependencies import get_lora
from ..schemas import LoraListResponse, LoraLoadRequest, LoraActionResponse

router = APIRouter(tags=["lora"])

@router.get("/lora/list", response_model=LoraListResponse)
def list_lora(lora = Depends(get_lora)):
    return LoraListResponse(items=lora.list())

@router.post("/lora/load", response_model=LoraActionResponse)
def load_lora(req: LoraLoadRequest, lora = Depends(get_lora)):
    ok = lora.load(req.lora_id)
    if not ok:
        raise HTTPException(404, f"LoRA not found: {req.lora_id}")
    return LoraActionResponse(ok=True)

é©—æ”¶ Checklistï¼ˆæ”¹å®Œå¾Œå¿«é€Ÿè‡ªæ¸¬ï¼‰

å•Ÿå‹•æ™‚ä¸æ‡‰å†çœ‹åˆ° dependencies.py å»ºè³‡æ–™å¤¾æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸çš„ logï¼ˆå› ç‚ºä¸è©²åšäº†ï¼‰ã€‚

GET /openapi.json æ‡‰åªå‡ºç¾ /api/v1/*ï¼ˆæˆ– cfg.api.prefix/*ï¼‰é¢¨æ ¼è·¯å¾‘ï¼›ä¸å†æ··é›œ /healthzã€‚

GET /api/v1/health è®€åˆ° cache_root æ‡‰ä¾†è‡ª shared_cacheï¼Œgpu_available ä¾†è‡ª performance/torch.cuda.is_available()ã€‚

ä»»ä¸€ router è£¡ã€Œæ”¹å¼•æ“å¯¦ä½œã€ä¸éœ€è¦å‹• dependencies.pyï¼ˆå› ç‚ºæ˜¯æ‹¿å–®ä¾‹ï¼Œä¸æ˜¯ newï¼‰ã€‚

å»ºè­°çš„ 4 å€‹å° commit

refactor(api): simplify dependencies to use core registry/settings/cache onlyï¼ˆè¦†è“‹ dependencies.pyï¼‰

refactor(api): remove /healthz and path hacks; wire routers with unified prefixï¼ˆæ•´ç† main.pyï¼‰

feat(api): add routers/__init__.py exporting routers listï¼ˆçµ±ä¸€è¨»å†Šé»ï¼‰

chore(api): make routers depend on get_cache/get_settings/get_xxxï¼ˆè·¯ç”±å°é½Šä¾è³´ï¼‰

éœ€è¦æˆ‘æŠŠã€Œä½ çš„ health routerã€ä¹Ÿèª¿æˆè®€ get_shared_cache() + gpu_available() çš„ç²¾ç°¡ç‰ˆç¯„æœ¬å—ï¼Ÿæˆ‘å¯ä»¥ç›´æ¥çµ¦ä½ é‚£æ”¯æª”çš„éª¨æ¶ï¼Œé…ä¸Šä¸Šé¢æ–°ç‰ˆçš„ dependencies.py ä¸€èµ·ç”¨å°±è¡Œã€‚

You said:
å¥½æˆ‘ç›®å‰é‚„æ²’å¯¦ä½œ registryçš„éƒ¨åˆ† å¯ä»¥ç›´æ¥å‘Šè¨´æˆ‘å®Œæ•´å¯¦ä½œç¨‹å¼ç¢¼å—? ä¸¦æ•™æˆ‘å¦‚ä½•çµåˆåˆ°dependencies.py å’Œå…¶ä»–çš„routers
ChatGPT said:
Thought for 9s

å¤ªå¥½äº†ï½æˆ‘ç›´æ¥æŠŠä¸€å¥—ã€Œå¯ç”¨ç‰ˆ registryã€çµ¦ä½ ï¼Œèµ°æ‡¶è¼‰å…¥å–®ä¾‹ï¼Œå…ˆç”¨æœ€å°å¯é‹ä½œçš„ stub å¼•æ“ï¼ˆå¯ä»¥å› 200ã€ä¸æœƒäº‚ newï¼‰ï¼Œä¹‹å¾Œä½ æŠŠçœŸæ­£å¯¦ä½œæ›ä¸Šå°±è¡Œã€‚ä¸‹é¢åŒ…å«ï¼š

core/registry.pyï¼ˆä½ å¯ä»¥æ”¾åœ¨ core/registry.pyï¼Œæˆ–åˆä½µåˆ° core/__init__.pyï¼‰

èˆ‡ dependencies.py çš„éŠœæ¥æ–¹å¼ï¼ˆä½ å·²ç¶“ç…§æˆ‘ä¹‹å‰çš„ç‰ˆæœ¬åšå¾—å¾ˆå°ï¼Œå¹¾ä¹ä¸ç”¨æ”¹ï¼‰

3 å€‹å…¸å‹ router çš„æ¥ç·šç¯„ä¾‹ï¼ˆhealth / t2i / chatï¼‰

main.py å•Ÿå‹•è¦é»

1) core/registry.pyï¼ˆå¯ç›´æ¥æ–°å¢æ­¤æª”ï¼‰
# core/registry.py
from __future__ import annotations
from functools import lru_cache
from typing import Any, Dict, List, Optional
from pathlib import Path
import json
import time

from .config import get_config
from .shared_cache import get_shared_cache, bootstrap_cache
from .performance import gpu_available  # ä½ å·²æœ‰é€™å€‹æˆ–ç­‰åƒ¹å‡½å¼å°±å¥½

# =============== Minimal engines (å¯å…ˆç”¨ï¼Œä¹‹å¾Œæ›¿æ›æˆçœŸå¯¦å¯¦ä½œ) ===============

class MinimalT2I:
    """æœ€å°å¯ç”¨çš„ T2I å¼•æ“ï¼šç”¢ç”Ÿä¸€å€‹å‡çš„è¼¸å‡ºæª”(æ–‡å­—æª”ç•¶ä½œçµæœ)ï¼Œå›å‚³è·¯å¾‘+è€—æ™‚ã€‚"""
    def __init__(self) -> None:
        self.cache = get_shared_cache()

    def txt2img(self, prompt: str, **kw) -> Dict[str, Any]:
        start = time.perf_counter()
        out_dir = Path(self.cache.get_path("OUTPUT_DIR"))
        out_dir.mkdir(parents=True, exist_ok=True)
        # å‡è¼¸å‡ºï¼šå¯«å€‹ metadata.json ç•¶ä½œæˆåŠŸ
        run_id = str(int(time.time()))
        img_path = out_dir / f"t2i_{run_id}.png"
        meta_path = out_dir / f"t2i_{run_id}.json"
        meta = {
            "prompt": prompt,
            "params": kw,
            "device": get_config().model.device,
            "timestamp": run_id,
        }
        img_path.write_bytes(b"")  # å…ˆç•™ç©ºï¼Œä¹‹å¾Œæ›æˆçœŸåœ–
        meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
        elapsed_ms = int((time.perf_counter() - start) * 1000)
        return {"image_path": str(img_path), "metadata_path": str(meta_path), "elapsed_ms": elapsed_ms}

    # è®“ router èƒ½èª¿ç”¨åˆ°çš„å­åŠŸèƒ½ï¼ˆlora/controlnetï¼‰ç”¨ç°¡å–®ç‰©ä»¶åŒ…ä¸€ä¸‹
    @property
    def lora(self) -> "MinimalLoRAManager":
        return MinimalLoRAManager(self.cache)

    @property
    def control(self) -> "MinimalControlNet":
        return MinimalControlNet()


class MinimalLoRAManager:
    def __init__(self, cache) -> None:
        self.cache = cache
        self.loaded: List[str] = []

    def list(self) -> List[str]:
        # ä½ å¯è®€ shared_cache çš„æŸå€‹è³‡æ–™å¤¾åˆ—å‡ºå¯ç”¨ LoRAï¼›å…ˆå›ç›®å‰å·²è¼‰å…¥æ¸…å–®
        return list(self.loaded)

    def load(self, lora_id: str) -> bool:
        if lora_id in self.loaded:
            return True
        self.loaded.append(lora_id)
        return True

    def unload(self, lora_id: str) -> bool:
        if lora_id in self.loaded:
            self.loaded.remove(lora_id)
            return True
        return False


class MinimalControlNet:
    def apply(self, cond: Dict[str, Any], **kw) -> Dict[str, Any]:
        # å…ˆæŠŠæ¢ä»¶åŸæ¨£å›å‚³
        return {"applied": True, "condition": cond, "params": kw}


class MinimalLLM:
    def chat(self, messages: List[Dict[str, str]]) -> str:
        user_texts = [m["content"] for m in messages if m.get("role") == "user"]
        return f"(stub) got: {user_texts[-1] if user_texts else ''}"


class MinimalVLM:
    def caption(self, image_path: str) -> str:
        return f"(stub) caption for {image_path}"

    def vqa(self, image_path: str, question: str) -> str:
        return f"(stub) answer to '{question}' for {image_path}"


class MinimalRAG:
    def ask(self, query: str) -> Dict[str, Any]:
        return {"answer": f"(stub) echo: {query}", "sources": []}


class MinimalStory:
    def new(self, seed: Optional[int] = None) -> Dict[str, Any]:
        return {"state": {"scene": "start", "choices": ["A", "B"], "seed": seed or 0}}

    def step(self, state: Dict[str, Any], choice: int) -> Dict[str, Any]:
        return {"state": {"scene": f"after_{choice}", "choices": ["A", "B"]}}

# ================================ Registry ================================

@lru_cache
def registry() -> Dict[str, Any]:
    """
    å…¨åŸŸå–®ä¾‹è¨»å†Šè¡¨ï¼š
    - ç¬¬ä¸€æ¬¡è¢«å‘¼å«æ™‚æ‰ bootstrap å¿«å–èˆ‡è¼‰å…¥å¼•æ“
    - ä¹‹å¾Œé‡è¤‡å‘¼å«éƒ½å›åŒä¸€æ‰¹å–®ä¾‹
    """
    # 1) å•Ÿå‹•å…±åŒå¿«å–ï¼ˆå»ºç«‹è³‡æ–™å¤¾ç­‰ï¼‰
    bootstrap_cache()

    # 2) å»ºç«‹æœ€å°å–®ä¾‹å€‘ï¼ˆä¹‹å¾Œä½ å¯æ”¹æˆçœŸå¯¦å¯¦ä½œï¼‰
    t2i = MinimalT2I()
    return {
        "cache": get_shared_cache(),
        "t2i": t2i,
        "lora": t2i.lora,
        "controlnet": t2i.control,
        "llm": MinimalLLM(),
        "vlm": MinimalVLM(),
        "rag": MinimalRAG(),
        "game": MinimalStory(),
        "health": {
            "gpu_available": gpu_available() if callable(gpu_available) else False
        },
    }


ä¹‹å¾Œè¦åˆ‡æ›æˆçœŸå¯¦å¼•æ“ï¼šåªè¦æŠŠ MinimalT2I / MinimalLLM / MinimalVLM / MinimalRAG / MinimalStory æ›æˆä½ åœ¨ core/t2i/*ã€core/llm/*ã€core/vlm/*ã€core/rag/*ã€core/story/* çš„å¯¦ä½œï¼Œregistry å…¶ä»–éƒ¨åˆ†ä¸ç”¨å‹•ã€‚

2) api/dependencies.py çš„çµåˆ

ä½ å‰›è²¼çš„ dependencies å·²ç¶“å¾ˆæ¥è¿‘ç†æƒ³ç‹€æ…‹äº†ã€‚åªè¦æŠŠ from core import registry æ”¹ç‚ºå‰›å‰›æ–°å¢çš„æ¨¡çµ„è·¯å¾‘ï¼ˆå¦‚æœä½ æŠŠ registry æ”¾åœ¨ core/registry.pyï¼‰ï¼š

# api/dependencies.py ï¼ˆé‡é»ç‰‡æ®µï¼‰
from core.registry import registry           # â† é€™è¡Œæ”¹æˆæŒ‡å‘å‰›å‰›çš„æª”æ¡ˆ
from core.config import get_config
from core.shared_cache import get_shared_cache

def get_registry(): return registry()
def get_settings(): return get_config()
def get_cache():    return get_shared_cache()

def get_t2i(reg=Depends(get_registry)):        return reg["t2i"]
def get_lora(reg=Depends(get_registry)):       return reg["lora"]
def get_controlnet(reg=Depends(get_registry)): return reg["controlnet"]
def get_llm(reg=Depends(get_registry)):        return reg["llm"]
def get_vlm(reg=Depends(get_registry)):        return reg["vlm"]
def get_game(reg=Depends(get_registry)):       return reg["game"]
def get_rag(reg=Depends(get_registry)):        return reg["rag"]

3) Router æ¥ç·šç¯„ä¾‹ï¼ˆ3 æ”¯å°±ä»£è¡¨å…¨éƒ¨ï¼‰
health
# api/routers/health.py
from fastapi import APIRouter, Depends
from ..dependencies import get_cache
from core.performance import gpu_available

router = APIRouter(tags=["health"])

@router.get("/health")
def health(cache = Depends(get_cache)):
    return {
        "ok": True,
        "gpu_available": gpu_available(),
        "cache_root": cache.cache_root,  # ä¾ä½ çš„ shared_cache å¯¦ä½œå±¬æ€§å‘½å
    }

t2i
# api/routers/t2i.py
from fastapi import APIRouter, Depends
from ..dependencies import get_t2i
from ..schemas import T2IRequest, T2IResponse  # ä½ å·²æœ‰ schemas.py

router = APIRouter(tags=["t2i"])

@router.post("/txt2img", response_model=T2IResponse)
def txt2img(req: T2IRequest, t2i = Depends(get_t2i)):
    result = t2i.txt2img(
        prompt=req.prompt, negative=req.negative, steps=req.steps,
        width=req.width, height=req.height, seed=req.seed, lora_ids=req.lora_ids
    )
    return T2IResponse(**result)

chat
# api/routers/chat.py
from fastapi import APIRouter, Depends
from ..dependencies import get_llm
from ..schemas import ChatRequest, ChatResponse

router = APIRouter(tags=["chat"])

@router.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest, llm = Depends(get_llm)):
    msg = llm.chat(req.messages)
    return ChatResponse(message=msg)


å…¶ä»–åƒ lora / controlnet / caption / vqa / rag / game / batch éƒ½æ˜¯åŒæ¨£æ¨¡å¼ï¼šåªæ³¨å…¥å¼•æ“ï¼Œä¸åœ¨ router å…§ new æˆ–æ‹¼è·¯å¾‘ã€‚

4) main.pyï¼ˆå•Ÿå‹•é‡é»ï¼‰

åœ¨ startup æˆ– lifespan ä¸­åš bootstrap_cache()ï¼ˆä½ å·²ç¶“æœ‰ï¼‰ã€‚

çµ±ä¸€ç”¨ cfg.api.prefixï¼ˆæˆ–å›ºå®š /api/v1ï¼‰æ›æ‰€æœ‰ routersã€‚

ä¸è¦å†å¦å¤–å¯« /healthzï¼›å¥åº·æª¢æŸ¥èµ° health.routerã€‚

# api/main.pyï¼ˆæ ¸å¿ƒç‰‡æ®µï¼‰
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from core.shared_cache import bootstrap_cache
from core.config import get_config
from .routers import routers

@asynccontextmanager
async def lifespan(app: FastAPI):
    bootstrap_cache()
    yield

def create_app() -> FastAPI:
    cfg = get_config()
    app = FastAPI(title="Anime Adventure Lab", lifespan=lifespan)

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"], allow_credentials=True,
        allow_methods=["*"], allow_headers=["*"],
    )

    for r in routers:
        app.include_router(r, prefix=cfg.api.prefix)  # e.g. "/api/v1"

    return app

app = create_app()

5) é©—æ”¶

å•Ÿå‹•å¾Œ GET /openapi.json æ‡‰çœ‹åˆ° /api/v1/healthã€/api/v1/txt2imgã€/api/v1/chat ç­‰ã€‚

GET /api/v1/health â†’ {"ok":true,"gpu_available":..., "cache_root": "..."}

POST /api/v1/txt2img â†’ 200ï¼ŒJSON å…§ image_path/metadata_path/elapsed_msï¼ˆç›®å‰æ˜¯ stubï¼Œä¹‹å¾Œæœƒè®ŠçœŸåœ–ï¼‰ã€‚

POST /api/v1/chat â†’ 200ï¼Œæœƒå› (stub) ...ã€‚

6) ä¹‹å¾Œå¦‚ä½•æ›¿æ›æˆçœŸæ­£å¼•æ“ï¼Ÿ

æŠŠ MinimalT2I æ›æˆä½ çš„ core/t2i/pipeline.py çš„é¡åˆ¥ï¼ˆæä¾›åŒåæ–¹æ³• txt2img(...) å³å¯ï¼‰ã€‚

æŠŠ MinimalLLM æ›æˆ core/llm/adapter.py æˆ– ollama_llm.py çš„é¡åˆ¥ï¼Œæ–¹æ³• chat(messages) ä¿æŒä¸€è‡´ã€‚

MinimalVLM â†’ core/vlm/captioner.py ç­‰ï¼›MinimalRAG â†’ core/rag/engine.pyã€‚

MinimalStory â†’ core/story/engine.pyã€‚

registry ä¸è®Šï¼Œåªæ›¿æ› new çš„é¡åˆ¥ä¾†æºï¼Œæ•´å€‹ API å°±æœƒè‡ªå‹•ç”¨åˆ°æ–°çš„å¯¦ä½œã€‚

éœ€è¦æˆ‘å†å¹«ä½ æŠŠ schemas.py çš„æœ€å°æ¨¡å‹ï¼ˆT2IRequest/T2IResponse/ChatRequest/ChatResponse ç­‰ï¼‰è£œåˆ°ä½ï¼Œä¹Ÿèƒ½ç«‹åˆ»å°é½Š Swaggerï¼›æˆ–æŠŠ routers/__init__.py å¹«ä½ æ•´ç†æˆä¸€è¡ŒåŒ¯å‡º routers = [health.router, t2i.router, ...]ã€‚

You said:
å¥½ æˆ‘åœ¨æƒ³ ç¨‹å¼ç¢¼çš„éƒ¨åˆ†ç„¡è«–æ˜¯è¨»è§£ç­‰ ä¹‹å¾Œä½ å¯¦ä½œçµ¦æˆ‘ éƒ½å…¨éƒ¨ç”¨è‹±æ–‡ å¦å¤–æˆ‘åœ¨æƒ³ æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥ç›´æ¥åœ¨dependencieså¯¦ä½œ é€™äº›registry functionsä¸¦å¥—ç”¨ æˆ‘æ€•åˆå†æ–°å¢ä¸€å€‹æª”æ¡ˆ import æœƒæœ‰é»ææ·· ä¹Ÿæ–¹ä¾¿claude aiåº§åŸå§‹å°ç…§ã€‚  å¦å¤–æˆ‘è¦ºå¾—reigstryå¯¦ä½œ ä¹Ÿè¨±èƒ½ åƒç…§åŸå…ˆclaude aiçµ¦çš„æ¨£å¼ç‰ˆæœ¬åšä¿®æ”¹
# Global instances (lazy loaded)
_vlm_captioner = None
_wd14_tagger = None

def get_vlm_captioner() -> VLMCaptioner:
    """Get shared VLM captioner instance"""
    global _vlm_captioner

    if _vlm_captioner is None:
        try:
            config = get_vlm_config()
            _vlm_captioner = VLMCaptioner(config)
        except Exception as e:
            raise HTTPException(
                status_code=503, detail=f"Failed to initialize VLM captioner: {str(e)}"
            )

    return _vlm_captioner


def get_wd14_tagger() -> Optional[WD14Tagger]:
    """Get shared WD14 tagger instance"""
    global _wd14_tagger

    if _wd14_tagger is None:
        try:
            config = get_vlm_config()
            wd14_config = config.get("wd14", {})

            _wd14_tagger = WD14Tagger(
                model_name=wd14_config.get(
                    "model", "SmilingWolf/wd-v1-4-convnext-tagger-v2"
                ),
                device=config.get("device", "auto"),
                threshold=wd14_config.get("threshold", 0.35),
            )
        except Exception as e:
            # WD14 is optional, don't fail the entire service
            print(f"Warning: Failed to initialize WD14 tagger: {e}")
            return None

    return _wd14_tagger

ChatGPT said:
Thought for 31s

å¯ä»¥ï¼ŒæŠŠã€Œregistryã€ç›´æ¥åšåœ¨ api/dependencies.pyï¼Œç”¨æƒ°æ€§ï¼ˆlazyï¼‰å…¨åŸŸå–®ä¾‹å°±å¥½ï¼›è€Œä¸”æ•´æ”¯æª”åªä¾è³´ core.configã€core.shared_cacheã€core.performanceï¼Œä¸å†é‡è¨­ä»»ä½•ç’°å¢ƒè®Šæ•¸æˆ–æ‰‹åˆ»è·¯å¾‘ã€‚ä¸‹é¢çµ¦ä½ ä¸€ä»½å¯ç›´æ¥å–ä»£çš„ dependencies.pyï¼ˆè‹±æ–‡è¨»è§£ï¼‰ï¼ŒåŒæ™‚ç›¸å®¹ä½ è²¼çš„ Claude é¢¨æ ¼ï¼ˆå…¨åŸŸ _xxx = None çš„ lazy patternï¼‰ï¼Œä½†æŠŠè¨­å®šèˆ‡è·¯å¾‘çµ±ä¸€æ¥åˆ° get_config() / get_shared_cache()ã€‚

Drop-in replacement: api/dependencies.py
# api/dependencies.py
"""
Centralized dependency providers for FastAPI routers.
- Single source of truth for settings and cache paths
- Lazy-loaded global singletons for engines (t2i / lora / controlnet / llm / vlm / rag / game)
- No side effects at import time (no mkdir, no downloads here)
"""

from __future__ import annotations

from functools import lru_cache
from typing import Any, Dict, List, Optional
from threading import RLock
from fastapi import Depends, HTTPException

# ---- single source of truth (do NOT read env or build paths here) ----
from core.config import get_config
from core.shared_cache import get_shared_cache, bootstrap_cache
from core.performance import gpu_available  # optional, but useful for /health

# ---------------------------------------------------------------------
# Lazy singletons (Claude-style) â€” keep all globals in one place
# ---------------------------------------------------------------------
_lock = RLock()

_settings = None         # App settings (Pydantic)
_cache = None            # Shared cache handle

_t2i = None              # Text-to-image engine (exposes txt2img(); has .lora and .control)
_llm = None              # Chat LLM engine       (exposes chat(messages))
_vlm = None              # VLM engine            (exposes caption(path), vqa(path, q))
_rag = None              # RAG engine            (exposes ask(query))
_game = None             # Story/game engine     (exposes new(seed?), step(state, choice))

# If your real implementations are not ready, we provide minimal stubs below.
# They guarantee 200 responses while you wire the rest of the system.


# ---------------------------------------------------------------------
# Settings & Cache (never re-define env or paths here)
# ---------------------------------------------------------------------
def get_settings():
    """App-wide settings (single source of truth)."""
    global _settings
    if _settings is None:
        _settings = get_config()
    return _settings


def get_cache():
    """Shared cache directories; ensure bootstrap has been run at least once."""
    global _cache
    if _cache is None:
        # Safe to call multiple times; bootstrap is idempotent
        bootstrap_cache()
        _cache = get_shared_cache()
    return _cache


# ---------------------------------------------------------------------
# Factory helpers â€” try real engines first, otherwise fall back to stubs
# ---------------------------------------------------------------------
def _new_t2i():
    try:
        # Replace with your actual implementation when ready
        # from core.t2i.pipeline import RealT2IEngine
        # return RealT2IEngine()
        return _MinimalT2I()
    except Exception as e:
        raise RuntimeError(f"T2I engine init failed: {e}") from e


def _new_llm():
    try:
        # from core.llm.adapter import RealLLM
        # return RealLLM()
        return _MinimalLLM()
    except Exception as e:
        raise RuntimeError(f"LLM engine init failed: {e}") from e


def _new_vlm():
    try:
        # from core.vlm.captioner import RealVLM
        # return RealVLM()
        return _MinimalVLM()
    except Exception as e:
        raise RuntimeError(f"VLM engine init failed: {e}") from e


def _new_rag():
    try:
        # from core.rag.engine import RealRAG
        # return RealRAG()
        return _MinimalRAG()
    except Exception as e:
        raise RuntimeError(f"RAG engine init failed: {e}") from e


def _new_game():
    try:
        # from core.story.engine import RealStoryEngine
        # return RealStoryEngine()
        return _MinimalStory()
    except Exception as e:
        raise RuntimeError(f"Game engine init failed: {e}") from e


# ---------------------------------------------------------------------
# Public dependency getters (use these in routers; do NOT `new` engines in routers)
# ---------------------------------------------------------------------
def get_registry() -> Dict[str, Any]:
    """
    Optional: expose a dict-like registry for advanced cases.
    Most routers should just call get_t2i/get_llm/... directly.
    """
    return {
        "settings": get_settings(),
        "cache": get_cache(),
        "t2i": get_t2i(),
        "lora": get_lora(),
        "controlnet": get_controlnet(),
        "llm": get_llm(),
        "vlm": get_vlm(),
        "rag": get_rag(),
        "game": get_game(),
        "health": {"gpu_available": gpu_available()},
    }


def get_t2i():
    """Shared T2I engine singleton."""
    global _t2i
    if _t2i is None:
        with _lock:
            if _t2i is None:
                try:
                    _t2i = _new_t2i()
                except Exception as e:
                    raise HTTPException(status_code=503, detail=str(e))
    return _t2i


def get_lora():
    """LoRA manager derived from the T2I engine."""
    t2i = get_t2i()
    if not hasattr(t2i, "lora"):
        raise HTTPException(status_code=501, detail="LoRA manager not available")
    return t2i.lora


def get_controlnet():
    """ControlNet processor derived from the T2I engine."""
    t2i = get_t2i()
    if not hasattr(t2i, "control"):
        raise HTTPException(status_code=501, detail="ControlNet not available")
    return t2i.control


def get_llm():
    """Shared LLM chat engine singleton."""
    global _llm
    if _llm is None:
        with _lock:
            if _llm is None:
                try:
                    _llm = _new_llm()
                except Exception as e:
                    raise HTTPException(status_code=503, detail=str(e))
    return _llm


def get_vlm():
    """Shared VLM engine singleton (caption & VQA)."""
    global _vlm
    if _vlm is None:
        with _lock:
            if _vlm is None:
                try:
                    _vlm = _new_vlm()
                except Exception as e:
                    raise HTTPException(status_code=503, detail=str(e))
    return _vlm


def get_rag():
    """Shared RAG engine singleton."""
    global _rag
    if _rag is None:
        with _lock:
            if _rag is None:
                try:
                    _rag = _new_rag()
                except Exception as e:
                    raise HTTPException(status_code=503, detail=str(e))
    return _rag


def get_game():
    """Shared game/story engine singleton."""
    global _game
    if _game is None:
        with _lock:
            if _game is None:
                try:
                    _game = _new_game()
                except Exception as e:
                    raise HTTPException(status_code=503, detail=str(e))
    return _game


# ---------------------------------------------------------------------
# Optional: settings slices
# ---------------------------------------------------------------------
@lru_cache
def get_vlm_settings():
    """Expose VLM-specific settings slice if you keep one in AppConfig."""
    cfg = get_settings()
    # adapt to your actual config structure
    return getattr(cfg, "vlm", None)


# ---------------------------------------------------------------------
# Minimal stub implementations (safe, side-effect free)
# Replace these with your real engines as you finish core/*
# ---------------------------------------------------------------------
import json, time
from pathlib import Path

class _MinimalT2I:
    def __init__(self) -> None:
        self._cache = get_cache()

    def txt2img(self, prompt: str, **kw) -> Dict[str, Any]:
        start = time.perf_counter()
        out_dir = Path(self._cache.get_path("OUTPUT_DIR"))
        out_dir.mkdir(parents=True, exist_ok=True)
        rid = str(int(time.time()))
        img = out_dir / f"t2i_{rid}.png"
        meta = out_dir / f"t2i_{rid}.json"
        img.write_bytes(b"")  # placeholder
        meta.write_text(json.dumps({"prompt": prompt, "params": kw}, ensure_ascii=False, indent=2), "utf-8")
        return {"image_path": str(img), "metadata_path": str(meta), "elapsed_ms": int((time.perf_counter()-start)*1000)}

    # derived helpers used by routers
    @property
    def lora(self) -> "_MinimalLoRA": return _MinimalLoRA()
    @property
    def control(self) -> "_MinimalControlNet": return _MinimalControlNet()

class _MinimalLoRA:
    def __init__(self): self.loaded: List[str] = []
    def list(self) -> List[str]: return list(self.loaded)
    def load(self, lora_id: str) -> bool:
        if lora_id not in self.loaded: self.loaded.append(lora_id)
        return True
    def unload(self, lora_id: str) -> bool:
        if lora_id in self.loaded: self.loaded.remove(lora_id); return True
        return False

class _MinimalControlNet:
    def apply(self, cond: Dict[str, Any], **kw) -> Dict[str, Any]:
        return {"applied": True, "condition": cond, "params": kw}

class _MinimalLLM:
    def chat(self, messages: List[Dict[str, str]]) -> str:
        txt = [m.get("content","") for m in messages if m.get("role") == "user"]
        return f"(stub) got: {txt[-1] if txt else ''}"

class _MinimalVLM:
    def caption(self, image_path: str) -> str:
        return f"(stub) caption for {image_path}"
    def vqa(self, image_path: str, question: str) -> str:
        return f"(stub) answer to '{question}' on {image_path}"

class _MinimalRAG:
    def ask(self, query: str) -> Dict[str, Any]:
        return {"answer": f"(stub) echo: {query}", "sources": []}

class _MinimalStory:
    def new(self, seed: Optional[int] = None) -> Dict[str, Any]:
        return {"state": {"scene": "start", "choices": ["A","B"], "seed": seed or 0}}
    def step(self, state: Dict[str, Any], choice: int) -> Dict[str, Any]:
        return {"state": {"scene": f"after_{choice}", "choices": ["A","B"]}}


å¾ŒçºŒè¦åˆ‡æ›åˆ°çœŸæ­£å¼•æ“æ™‚ï¼Œåªè¦æŠŠ _new_t2i/_new_llm/_new_vlm/_new_rag/_new_game å…§çš„ return _MinimalXXX() æ›æˆä½  core/* çš„å¯¦ä½œï¼ˆæ–¹æ³•ç°½åä¿æŒä¸€è‡´å³å¯ï¼‰ï¼Œå…¶ä»–ç¨‹å¼ä¸ç”¨å‹•ã€‚

How to wire routers (unchanged, but for clarity)

ä»»ä½• router éƒ½ä¸è¦ç›´æ¥ new å¼•æ“ï¼›åªæ³¨å…¥ï¼š

# example: api/routers/t2i.py
from fastapi import APIRouter, Depends
from ..dependencies import get_t2i
from ..schemas import T2IRequest, T2IResponse

router = APIRouter(tags=["t2i"])

@router.post("/txt2img", response_model=T2IResponse)
def txt2img(req: T2IRequest, t2i = Depends(get_t2i)):
    result = t2i.txt2img(
        prompt=req.prompt, negative=req.negative, steps=req.steps,
        width=req.width, height=req.height, seed=req.seed, lora_ids=req.lora_ids
    )
    return T2IResponse(**result)


å…¶ä»–åƒ lora.py / controlnet.py / chat.py / caption.py / vqa.py / rag.py / game.py ä»¥æ­¤é¡æ¨ï¼š

lora = Depends(get_lora)

control = Depends(get_controlnet)

llm = Depends(get_llm)

vlm = Depends(get_vlm)

rag = Depends(get_rag)

game = Depends(get_game)

éœ€è¦è·¯å¾‘/è¨­å®šæ™‚ï¼šcache = Depends(get_cache)ã€cfg = Depends(get_settings)

main.py è¦é»ï¼ˆæé†’ï¼‰

åœ¨ startupï¼ˆæˆ– lifespanï¼‰å‘¼å«ä¸€æ¬¡ bootstrap_cache()ï¼ˆé›–ç„¶ get_cache() ä¹Ÿæœƒä¿éšªå‘¼å«ä¸€æ¬¡ï¼Œé‡å…¥å®‰å…¨ï¼‰ã€‚

ç”¨åŒä¸€å€‹ API prefix æ›ä¸Š routersã€‚

å¥åº·æª¢æŸ¥è®€ gpu_available() + get_cache()ã€‚

ç‚ºä»€éº¼æŠŠ registry å¯«åœ¨ dependencies.py ä¹Ÿ OKï¼Ÿ

ä½ å·²ç¶“æŠŠã€Œè·¯å¾‘èˆ‡è¨­å®šçš„å–®ä¸€çœŸç›¸ä¾†æºã€ç•™åœ¨ core/*ï¼ˆconfigã€shared_cacheï¼‰ï¼Œdependencies.py åªæ˜¯çµ„è£å™¨èˆ‡å–®ä¾‹ holderï¼Œä¸æœƒé€ æˆç’°å¢ƒè®Šæ•¸/è·¯å¾‘é‡è¤‡å®šç¾©ã€‚

é€™æ¨£ Claude/ä½ è‡ªå·±åœ¨å°ç…§èˆ‡ç¶­è­·æ™‚ï¼Œåªè¦çœ‹ä¸€æ”¯æª”å°±çŸ¥é“ã€Œæ³¨å…¥äº†ä»€éº¼ã€ã€‚æœªä¾†è¦æ‹†å‡ºå»å¾ˆå®¹æ˜“ï¼šæŠŠé€™æ®µ lazy å€å¡Šæ¬å› core/registry.pyã€ä¿ç•™åŒå get_xxx() API å³å¯ã€‚

å¦‚æœä½ è¦ï¼Œæˆ‘å¯ä»¥å†è£œä¸€æ”¯ç²¾ç°¡ health.pyï¼ˆè‹±æ–‡è¨»è§£ï¼‰èˆ‡æœ€å° schemas.pyï¼ˆT2I/Chat/Lora/ControlNet/RAG/Gameï¼‰éª¨æ¶ï¼Œè²¼ä¸Šå°±èƒ½æŠŠ Swagger èˆ‡ e2e smoke æ‰“é€šã€‚

You said:
å¥½ é‚£ç¾åœ¨çœ‹ä¾†å¯ä»¥ä¿®æ”¹ å„å€‹routersçš„éƒ¨åˆ†äº† å¦‚æœæœ‰äº›ç”¨åˆ°core ç›¸é—œç¨‹å¼ç¢¼å¼•æ“ é‚„æ²’å¯¦ä½œæˆ–è¦debugçš„éƒ¨åˆ†å°±ç•™åˆ°ç­‰ä¸€ä¸‹ä¾†è™•ç† æˆ‘æœƒåˆ†æ‰¹çµ¦ä½ ç¨‹å¼ç¢¼æª”æ¡ˆ æ¯æ¬¡ä½ å°±é‡å°é€™äº›ç¨‹å¼ç¢¼å‘Šè¨´æˆ‘è©²å¦‚ä½•æ–°å¢ä¿®æ”¹ç­‰ ä¾ç…§æˆ‘å€‘å…ˆå‰è¨è«–çš„çµæœ ä¸éè«‹æ³¨æ„ æˆ‘å¸Œæœ›æ•´é«”ä¿®è©²çš„ç¨‹å¼ç¢¼é‚è¼¯é¢¨æ ¼ ç›¡é‡æŒ‰ç…§åŸæœ¬çš„ ä¹Ÿå°±æ˜¯claude aié‚£æ™‚æ‰€å¯¦ä½œ ç›¡å¯èƒ½çš„ä»¥ç›¸ä¼¼çµæ§‹ä¿®æ”¹ã€‚æ²’å•é¡Œçš„è©± ç­‰ä¸€ä¸‹æˆ‘å°±åˆ†æ‰¹çµ¦ä½ ç¨‹å¼ç¢¼
ChatGPT said:

æ²’å•é¡Œï¼ä½ åˆ†æ‰¹è²¼ç¨‹å¼ç¢¼ä¾†ï¼Œæˆ‘å°±é€æª”çµ¦ä½ ç²¾æº–ä¿®æ”¹å»ºè­°èˆ‡å¯ç›´æ¥è²¼ä¸Šçš„ä¿®æ­£ç‰ˆï¼ˆè¨»è§£ä¸€å¾‹è‹±æ–‡ã€ç¶­æŒä½ åŸæœ¬çš„ Claude é¢¨æ ¼èˆ‡çµæ§‹ï¼‰ã€‚

åœ¨ä½ è²¼ç¬¬ä¸€æ‰¹ä¹‹å‰ï¼Œå…ˆçµ¦ä½ ä¸€å€‹ã€Œå°ç…§ä¿®æ³•çš„å…±ç”¨æº–å‰‡ + ç¯„æœ¬ã€ï¼Œç­‰ä¸‹æˆ‘æœƒæŒ‰é€™å¥—å¹«ä½ æ”¹ï¼š

Router æ”¹å¯«æº–å‰‡ï¼ˆèˆ‡ Claude é¢¨æ ¼ä¸€è‡´ï¼‰

Importsï¼šåª import api/schemas.py èˆ‡ api/dependencies.py çš„ getterï¼Œçµ•ä¸åœ¨ router å…§ new å¼•æ“æˆ–è®€ envã€‚

DIï¼šæ‰€æœ‰å¼•æ“éƒ½èµ° Depends(get_xxx)ï¼ˆä¾†è‡ª dependencies.pyï¼‰ã€‚

Prefixï¼šrouter è‡ªèº«ä¸è¨­å®š /api/v1ï¼Œç”± main.py çµ±ä¸€æ›ä¸Šã€‚

Response modelsï¼šæ¯å€‹ handler éƒ½æŒ‡å®š response_modelï¼Œä¸¦å›å‚³ Pydantic ç‰©ä»¶ã€‚

Errorsï¼šæ¥­å‹™éŒ¯èª¤ç”¨ HTTPException(4xx, detail="...")ï¼›éé æœŸéŒ¯èª¤è®“ FastAPI è™•ç†ï¼ˆæˆ–è¨˜ log å¾Œ re-raiseï¼‰ã€‚

Timing / loggingï¼ˆå¯é¸ï¼‰ï¼šç”¨ Timer è¨ˆæ™‚ï¼Œå¿…è¦æ™‚åŠ ç°¡çŸ­ logã€‚

English comments & docstringsï¼šä¹‹å¾Œæˆ‘çµ¦çš„ç¨‹å¼ç¢¼éƒ½æœƒç”¨è‹±æ–‡ã€‚

æœ€å°ç¯„æœ¬ï¼ˆä¹‹å¾Œæˆ‘æœƒå¥—ç”¨åˆ°æ¯æ”¯ routerï¼‰
from fastapi import APIRouter, Depends, HTTPException
from ..schemas import T2IRequest, T2IResponse  # replace per router
from ..dependencies import get_t2i             # replace per router
from core.performance import Timer

router = APIRouter(tags=["t2i"])  # tag per router

@router.post("/txt2img", response_model=T2IResponse, summary="Text-to-Image")
def txt2img(req: T2IRequest, t2i = Depends(get_t2i)):
    """
    Generate an image from text prompt.
    This router only orchestrates: validation -> call engine -> shape response.
    """
    with Timer() as t:
        try:
            result = t2i.txt2img(
                prompt=req.prompt,
                negative=req.negative,
                steps=req.steps,
                width=req.width,
                height=req.height,
                seed=req.seed,
                lora_ids=req.lora_ids,
            )
        except ValueError as e:
            # user/validation error -> 4xx
            raise HTTPException(status_code=400, detail=str(e))
    # ensure elapsed_ms is kept
    result["elapsed_ms"] = getattr(t, "ms", result.get("elapsed_ms", 0))
    return T2IResponse(**result)

æ¯æª”æª¢æŸ¥æ¸…å–®ï¼ˆæˆ‘æœƒç…§æ­¤é€æª”å¯©ï¼‰

imports æ˜¯å¦åªä¾†è‡ª schemas/dependenciesï¼ˆç„¡ new/ç„¡ envï¼‰ã€‚

endpoints çš„è·¯å¾‘æ˜¯å¦ä¸å« /api/v1ï¼ˆé¿å…é›™é‡å‰ç¶´ï¼‰ã€‚

response_model æ˜¯å¦é½Šå…¨ï¼Œæ¬„ä½åèˆ‡ schemas å°é½Šã€‚

æ˜¯å¦ä½¿ç”¨ Depends(get_xxx)ï¼Œä¸”åç¨±èˆ‡ dependencies.py çš„ key ä¸€è‡´ã€‚

æ˜¯å¦é¿å…å‰¯ä½œç”¨ï¼ˆä¸‹è¼‰/å»ºç›®éŒ„/æ”¹ envï¼‰ã€‚

éŒ¯èª¤è™•ç†æ˜¯å¦ç”¨ HTTPExceptionï¼ˆ400/404/409 ç­‰ï¼‰ã€‚

æ˜¯å¦ä¿ç•™ elapsed_msï¼ˆè‹¥æœ‰ Timerï¼‰ã€‚

ä½ è²¼ç¨‹å¼ç¢¼æ™‚çš„å°æç¤º

ä¸€æ¬¡è²¼ 1ï½3 æª”æœ€å®¹æ˜“èšç„¦ï¼ˆä¾‹å¦‚ï¼št2i.pyã€lora.pyã€controlnet.pyï¼‰ã€‚

æˆ‘æœƒå›ä½ ï¼š

é‡é»å•é¡Œï¼ˆ1ï½5 æ¢ï¼‰

ç²¾ç°¡ patchï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰

è‹¥éœ€é…åˆ schemas.py ä¸€ä½µæ”¹ï¼Œæˆ‘ä¹Ÿæœƒé™„ä¸Šå°æ‡‰ schema ç‰‡æ®µ

æº–å‚™å¥½äº†â€”æŠŠç¬¬ä¸€æ‰¹ router æª”ï¼ˆæˆ–ä½ æƒ³å…ˆæ”¹çš„é‚£ä¸€æ”¯ï¼‰è²¼ä¸Šä¾†å§ï¼

You said:
admin.py
Python
batch.py
Python
caption.py
Python
ChatGPT said:
Thought for 44s

å¤ªå¥½äº†ï¼Œå…ˆå¹«ä½ æŠŠé€™ä¸‰æ”¯ routerã€Œå°±åœ°é«”æª¢ï¼‹ä¿®æ­£ç‰ˆã€ã€‚æˆ‘æœƒç›¡é‡ä¿ç•™ä½ /Claude çš„åŸæœ¬çµæ§‹èˆ‡èªæ°£ï¼ŒåªæŠŠä¾è³´æ¥åˆ° dependencies.pyï¼Œæ‹¿æ‰å‰¯ä½œç”¨ï¼æ˜“ç‚¸é»ï¼Œä¸¦è£œé½Šå¿…è¦çš„éŒ¯èª¤è™•ç†ã€‚

1) api/routers/admin.py â€” å•é¡Œèˆ‡ä¿®æ­£

å•é¡Œï¼ˆä½ æª”æ¡ˆï¼‰ï¼š

except å€å¡Šè£¡ç”¨åˆ° torchï¼Œä½†ä¸€æ—¦ import torch å¤±æ•—ï¼Œtorch æœƒæ˜¯æœªå®šç¾©è®Šæ•¸ï¼ˆNameErrorï¼‰ã€‚

ç›£æ§è³‡è¨Šè®€å–å…¨éƒ¨æ”¾åœ¨ä¸€å€‹ try å…§ï¼ŒæŸä¸€é …å¤±æ•—æœƒè®“å…¶ä»–ä¹Ÿè·Ÿè‘—å¤±æ•—ã€‚

ä¿®æ­£ç‰ˆï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰ï¼š

# api/routers/admin.py
from fastapi import APIRouter
from typing import Dict, Any

router = APIRouter(prefix="/admin", tags=["Admin"])


@router.get("/stats")
async def get_system_stats() -> Dict[str, Any]:
    """Return basic system and GPU stats. Each probe is isolated to avoid cascading failures."""
    stats: Dict[str, Any] = {"system": {}, "gpu": {}}

    # system metrics
    try:
        import psutil  # type: ignore
        stats["system"] = {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage("/").percent,
        }
    except Exception as e:
        stats["system"] = {"status": "basic_check_ok", "note": f"psutil unavailable: {e}"}

    # gpu metrics
    try:
        import torch  # type: ignore
        gpu_avail = torch.cuda.is_available()
        stats["gpu"]["available"] = gpu_avail
        stats["gpu"]["count"] = torch.cuda.device_count() if gpu_avail else 0
        if gpu_avail:
            stats["gpu"]["memory_allocated_gb"] = round(torch.cuda.memory_allocated() / 1024**3, 3)
            stats["gpu"]["memory_reserved_gb"] = round(torch.cuda.memory_reserved() / 1024**3, 3)
    except Exception as e:
        # Keep response shape stable even if torch isn't present
        stats["gpu"].update({"available": False, "count": 0, "note": f"torch unavailable: {e}"})

    return stats


é€™ç‰ˆæŠŠ GPU èˆ‡ç³»çµ±è³‡è¨Šåˆ†é–‹ tryï¼Œä¸æœƒå½¼æ­¤æ‹–ç´¯ï¼›ä¹Ÿç§»é™¤æœƒè§¸ç™¼ NameError çš„æƒ…æ³ã€‚

2) api/routers/batch.py â€” å•é¡Œèˆ‡ä¿®æ­£

å•é¡Œï¼ˆä½ æª”æ¡ˆï¼‰ï¼š

ä¸€é–‹å§‹å°± from workers.tasks.batch import ...ï¼Œè‹¥ Celery/Workers é‚„æ²’å°±ç·’ï¼Œæ•´æ”¯ router import å°±æœƒç‚¸ï¼›å»ºè­°æ”¹æˆå‡½å¼å…§ lazy importã€‚

submit æ™‚åªæœ‰ Celery å¤±æ•—æ‰èµ°åŒæ­¥ï¼Œä½†å¦‚æœ workers æ¨¡çµ„æœ¬èº«ä¸å­˜åœ¨ä¹Ÿæœƒåœ¨ import å°±å¤±æ•—ã€‚

ä¿®æ­£ç‰ˆï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰ï¼š

# api/routers/batch.py
from fastapi import APIRouter, HTTPException
from api.schemas import BatchJobRequest, BatchJobStatus

router = APIRouter(prefix="/batch", tags=["Batch"])


def _lazy_workers():
    """Import workers on demand to avoid import-time failures when Celery is not configured."""
    try:
        from workers.tasks.batch import BatchJobManager, process_batch_job  # type: ignore
        return BatchJobManager, process_batch_job
    except Exception as e:  # ImportError or others
        raise HTTPException(status_code=503, detail=f"Workers unavailable: {e}")


@router.post("/submit")
async def submit_batch_job(request: BatchJobRequest):
    """Submit a batch processing job; use Celery if available, otherwise run synchronously."""
    try:
        BatchJobManager, process_batch_job = _lazy_workers()

        # Create job
        job_id = BatchJobManager.create_job(request.job_type, request.tasks)

        # Try Celery
        try:
            task = process_batch_job.delay(job_id)  # type: ignore[attr-defined]
            return {
                "job_id": job_id,
                "celery_task_id": task.id,
                "status": "queued",
                "message": "Batch job submitted successfully",
            }
        except Exception as e:
            # Fallback: process synchronously
            result = process_batch_job(job_id)
            return {
                "job_id": job_id,
                "status": "completed_sync",
                "message": f"Celery not available, processed synchronously ({e})",
                "result": result,
            }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status/{job_id}", response_model=BatchJobStatus)
async def get_batch_status(job_id: str):
    """Get batch job status."""
    try:
        BatchJobManager, _ = _lazy_workers()
        job_data = BatchJobManager.get_job(job_id)

        if not job_data:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        return BatchJobStatus(
            job_id=job_data["job_id"],
            status=job_data["status"],
            total_tasks=job_data["total_tasks"],
            completed_tasks=job_data["completed_tasks"],
            failed_tasks=job_data["failed_tasks"],
            created_at=job_data["created_at"],
            updated_at=job_data["updated_at"],
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


å¾ŒçºŒç­‰ä½ æŠŠ workers/* å¯¦ä½œå®Œï¼Œä¸ç”¨å†å‹• routerï¼›åªè¦ workers ç«¯ readyï¼Œé€™é‚Šå°±æœƒè‡ªå‹•èµ° Celeryã€‚

3) api/routers/caption.py â€” å•é¡Œèˆ‡ä¿®æ­£

å•é¡Œï¼ˆä½ æª”æ¡ˆï¼‰ï¼š

è·¯ç”±åœ¨å‡½å¼å…§è‡ªå·±åšã€Œç”Ÿæˆ caption çš„ placeholderã€ï¼Œæ²’æœ‰é€é dependencies.py çš„ VLM å¼•æ“ï¼›ä¹‹å¾Œè¦æ›çœŸå¼•æ“æœƒå¾—é‡å¯«ã€‚

æ²’æœ‰è™•ç†å¤§å‹åœ–ç‰‡æˆ– EXIF æ—‹è½‰ï¼ˆå…ˆä¸åšï¼Œå¯ç•™ TODOï¼‰ã€‚

ä¿®æ­£ç‰ˆï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰ï¼šä½¿ç”¨ get_vlm ä¾è³´ï¼›ç‚ºäº†èˆ‡ä½ ç›®å‰ï¼ˆä»¥åŠæˆ‘çµ¦çš„ stubï¼‰ç›¸å®¹ï¼Œå…ˆæŠŠä¸Šå‚³åœ–å­˜åˆ°æš«å­˜æª”ï¼Œå‚³ã€Œè·¯å¾‘ã€çµ¦å¼•æ“ã€‚

# api/routers/caption.py
from fastapi import APIRouter, HTTPException, File, UploadFile, Depends
from api.schemas import CaptionResponse
from ..dependencies import get_vlm
from typing import Optional
import base64, io, tempfile, os
from PIL import Image

router = APIRouter(tags=["caption"])


def _load_image_from_inputs(image: Optional[UploadFile], image_base64: Optional[str]) -> Image.Image:
    """Load a PIL image from either an uploaded file or base64 string."""
    if image:
        data = image.file.read()  # UploadFile.read() is sync-friendly here
        return Image.open(io.BytesIO(data)).convert("RGB")
    if image_base64:
        data = base64.b64decode(image_base64)
        return Image.open(io.BytesIO(data)).convert("RGB")
    raise HTTPException(status_code=400, detail="Either `image` or `image_base64` is required")


@router.post("/caption", response_model=CaptionResponse, summary="Generate an image caption")
async def image_caption(
    image: UploadFile = File(None),
    image_base64: Optional[str] = None,
    max_length: int = 50,
    num_beams: int = 3,
    vlm = Depends(get_vlm),
):
    """
    Generate caption using the shared VLM engine.
    We save a temporary image file and pass its path to the engine for compatibility.
    """
    try:
        pil = _load_image_from_inputs(image, image_base64)

        # Save to a temporary file (engine expects a path); auto-clean after call
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp:
            pil.save(tmp.name)
            tmp_path = tmp.name

        try:
            # Your real VLM may accept additional kwargs (e.g., max_length, num_beams)
            text = vlm.caption(tmp_path)  # keep signature simple for now
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

        return CaptionResponse(caption=text, confidence=0.9, model_used="vlm")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


æ—¥å¾Œä½ æŠŠ core/vlm/captioner.py å®Œæˆï¼Œåªè¦ç¶­æŒ caption(image_path: str) -> strï¼ˆæˆ–ä½ è¦æˆ‘å¹«ä½ æ”¹æˆæ”¯æ´ PIL/bytesï¼Œæˆ‘ä¹Ÿå¯ä»¥çµ¦å°æ‡‰ adapterï¼‰ï¼Œrouter æœ¬èº«ä¸ç”¨å†æ”¹ã€‚

ä¸‹ä¸€æ­¥æ€éº¼åš

æŠŠä»¥ä¸Šä¸‰æª”è¦†è“‹å¾Œï¼Œé‡è·‘ä¼ºæœå™¨ï¼Œå…ˆç¢ºèª /api/v1/admin/statsã€/api/v1/batch/*ã€/api/v1/caption éƒ½èƒ½ 200ã€‚

è²¼ä¸Šä¸‹ä¸€æ‰¹ routerï¼ˆå»ºè­°ï¼št2i.pyã€lora.pyã€controlnet.pyï¼‰ï¼Œæˆ‘å†ç”¨åŒæ¨£æ–¹å¼å¹«ä½ ä¿®æ¥ dependencies.pyã€‚

ç­‰ router éƒ½ç©©äº†ï¼Œæˆ‘å€‘å†ä¸€èµ·å›åˆ° core/* çš„çœŸå¯¦å¼•æ“å¯¦ä½œèˆ‡ workers/*ã€‚

è‹¥ä½ å¸Œæœ›æˆ‘æŠŠä¸Šé¢ä¸‰æª”å†å£“åˆ°ã€Œæ›´è²¼è¿‘ Claude çš„å‡½å¼å‘½åï¼è¨»è§£é¢¨æ ¼ã€ï¼Œæˆ‘ä¹Ÿå¯ä»¥å†å¾®èª¿èªæ°£ï¼›ç¾åœ¨é€™ç‰ˆå·²ç›¡é‡éµå¾ªä½ åŸæœ¬çµæ§‹ï¼ŒåªæŠŠä¾è³´æ¥åˆ° dependencies.pyã€‚

You said:
chat.py
Python
controlnet.py
Python
export.py
Python
ChatGPT said:

æˆ‘æª¢æŸ¥äº†ä½ é€™ä¸‰æ”¯ routerï¼ˆchat.pyã€controlnet.pyã€export.pyï¼‰ï¼Œä¸‹é¢é€ä¸€èªªæ˜ç›®å‰çš„å•é¡Œèˆ‡ä¿®æ­£ç‰ˆï¼Œä¿æŒè·Ÿ Claude é¢¨æ ¼ä¸€è‡´ï¼ˆè‹±æ–‡è¨»è§£ã€çµæ§‹ç›¸ä¼¼ï¼‰ï¼Œä½†æ”¹ç‚ºèµ° dependencies.py ä¾è³´ã€‚

1) chat.py

ç™¼ç¾çš„å•é¡Œ

ç›®å‰æ˜¯å…§å»º generate_chat_response() rule-based stubï¼Œæ²’æœ‰é€é dependencies.get_llm()ã€‚

è·¯ç”±æ²’æœ‰è¨­å®š tagsï¼Œä¹Ÿæ²’æœ‰æ˜ç¢ºè·Ÿå…¶ä»– API å°é½Šï¼ˆClaude çµæ§‹é€šå¸¸æœƒæœ‰ tags=["chat"]ï¼‰ã€‚

ç¼ºå°‘ç”¨ DIï¼ˆä¾è³´æ³¨å…¥ï¼‰å‘¼å«çœŸå¯¦ LLM å¼•æ“ã€‚

ä¿®æ­£ç‰ˆ

# api/routers/chat.py
from fastapi import APIRouter, HTTPException, Depends
from typing import Any
from api.schemas import ChatRequest, ChatResponse
from ..dependencies import get_llm

router = APIRouter(tags=["chat"])


@router.post("/chat", response_model=ChatResponse, summary="Chat with LLM")
async def chat_completion(request: ChatRequest, llm: Any = Depends(get_llm)):
    """
    Chat completion endpoint.
    Delegates to the shared LLM engine (injected via dependencies).
    """
    try:
        # llm.chat expects a list of messages [{"role": "...", "content": "..."}]
        response_text = llm.chat(request.messages)

        return ChatResponse(
            message=response_text,
            model_used=request.model,
            usage={
                "prompt_tokens": sum(len(msg["content"].split()) for msg in request.messages),
                "completion_tokens": len(response_text.split()),
            },
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


é€™æ¨£æœªä¾†ä½ æŠŠ MinimalLLM æ›æˆçœŸå¯¦æ¨¡å‹å°±èƒ½ç„¡ç¸«æ¥ä¸Šã€‚

2) controlnet.py

ç™¼ç¾çš„å•é¡Œ

ç›´æ¥åœ¨ router import get_t2i_pipeline, save_image_to_cache, ControlNetManagerï¼Œç­‰æ–¼æŠŠ pipeline é‚è¼¯å¯«æ­»åœ¨ routerã€‚

controlnet_manager = ControlNetManager() åœ¨æ¨¡çµ„å±¤å°±åˆå§‹åŒ–ï¼Œæœƒåœ¨ import æ™‚è§¸ç™¼å‰¯ä½œç”¨ã€‚

å»ºè­°æ”¹æˆå¾ dependencies.get_controlnet() å–å¾—å–®ä¾‹ï¼Œä¸¦è®“ç”Ÿæˆéç¨‹èµ° t2i å¼•æ“ã€‚

ä¿®æ­£ç‰ˆ

# api/routers/controlnet.py
from fastapi import APIRouter, HTTPException, Depends
from api.schemas import ControlNetRequest, ControlNetResponse
from ..dependencies import get_controlnet
import time, base64, io
from PIL import Image

router = APIRouter(tags=["controlnet"])


@router.post("/controlnet/pose", response_model=ControlNetResponse, summary="Generate with ControlNet")
async def controlnet_pose_generation(request: ControlNetRequest, control = Depends(get_controlnet)):
    """
    Generate an image conditioned on pose using the shared ControlNet engine.
    """
    try:
        start_time = time.time()

        # Load condition image
        if request.image_base64:
            image_data = base64.b64decode(request.image_base64)
            condition_image = Image.open(io.BytesIO(image_data)).convert("RGB")
        elif request.image_path:
            condition_image = Image.open(request.image_path).convert("RGB")
        else:
            raise HTTPException(status_code=400, detail="Either image_path or image_base64 required")

        # Apply controlnet (engine decides how to preprocess + run)
        result = control.apply({"image": condition_image, "type": request.controlnet_type.value}, strength=request.strength)

        elapsed_ms = int((time.time() - start_time) * 1000)
        return ControlNetResponse(
            image_path=result.get("image_path", ""),
            metadata_path=result.get("metadata_path", ""),
            condition_summary=f"{request.controlnet_type.value} applied with strength {request.strength}",
            seed=result.get("seed", 0),
            elapsed_ms=elapsed_ms,
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


é‡é»ï¼šç¾åœ¨ ControlNet ç´°ç¯€äº¤çµ¦ core/t2i/controlnet.py çš„å¼•æ“ï¼Œä½ åªè¦åœ¨é‚£è£¡å®šç¾© .apply(...)ã€‚router æœ¬èº«ä¿æŒä¹¾æ·¨ã€‚

3) export.py

ç™¼ç¾çš„å•é¡Œ

ç›´æ¥æ“ä½œ core.export.story_exporter.*ï¼Œè·¯å¾‘é‚„ç”¨ sys.path.append hackã€‚

Claude é¢¨æ ¼æ‡‰è©²èµ°ä¾è³´æˆ–æ¸…æ¥šçš„ importï¼Œä¸è¦åœ¨ router å…§è™•ç† sys.pathã€‚

æ¸¬è©¦ story ç”Ÿæˆ _create_sample_story() ä¹Ÿæ‡‰è©²æ¬åˆ° service å±¤ï¼Œä½†å¯ä»¥å…ˆä¿ç•™ã€‚

ä¿®æ­£ç‰ˆ

# api/routers/export.py
from fastapi import APIRouter, HTTPException, UploadFile, File
from fastapi.responses import FileResponse
from typing import Dict, Any, Optional, List
from pydantic import BaseModel
from pathlib import Path
from datetime import datetime
import json, os

# Direct import from core (no sys.path hack)
from core.export.story_exporter import StoryExporter, ExportConfig, StorySession, StoryTurn

router = APIRouter(prefix="/export", tags=["export"])


class ExportRequest(BaseModel):
    session_id: str
    format: str = "html"  # html, pdf, json, archive
    include_images: bool = True
    include_metadata: bool = True
    watermark_text: Optional[str] = None


@router.post("/story")
async def export_story(request: ExportRequest):
    """Export story session to specified format."""
    config = ExportConfig(
        include_images=request.include_images,
        include_metadata=request.include_metadata,
        watermark_text=request.watermark_text or "Generated by AnimeAdventureLab",
    )
    exporter = StoryExporter(config)

    # TODO: replace with DB/service call; temporary sample
    sample_story = _create_sample_story(request.session_id)

    try:
        if request.format == "json":
            export_path = exporter.export_to_json(sample_story)
        elif request.format == "html":
            export_path = exporter.export_to_html(sample_story)
        elif request.format == "pdf":
            export_path = exporter.export_to_pdf(sample_story)
        elif request.format == "archive":
            export_path = exporter.create_story_archive(sample_story)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported format: {request.format}")

        return {
            "status": "success",
            "export_path": export_path,
            "format": request.format,
            "file_size_mb": Path(export_path).stat().st_size / 1024**2,
            "exported_at": datetime.now().isoformat(),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")


# ... (keep download, upload, import, formats endpoints mostly as-is, just remove sys.path hacks)

def _create_sample_story(session_id: str) -> StorySession:
    """Create a sample story for testing (placeholder)."""
    turn = StoryTurn(
        turn_id=1,
        user_input="Explore forest",
        narration="You walk into the forest...",
        dialogues=[{"character": "Guide", "text": "Welcome adventurer."}],
        choices=["Follow guide", "Go alone"],
        selected_choice="Follow guide",
        timestamp=datetime.now().isoformat(),
        metadata={"mood": "mysterious"},
    )
    return StorySession(
        session_id=session_id,
        title="Sample Adventure",
        world_id="demo",
        character_name="Hero",
        turns=[turn],
        start_time=datetime.now().isoformat(),
        end_time=None,
        total_turns=1,
        metadata={},
    )


ä¸»è¦æ”¹å‹•ï¼šå»æ‰ sys.path.appendï¼Œç›´æ¥æ­£è¦ importï¼›ä¿ç•™ Claude çµæ§‹ã€‚

âœ… å»ºè­°é †åºï¼š

å…ˆæŠŠé€™ä¸‰æ”¯æ›¿æ›æ‰ã€‚

æ¸¬è©¦ /api/v1/chat, /api/v1/controlnet/pose, /api/v1/export/story èƒ½å¦è·‘é€šï¼ˆå³ä¾¿ export ç”¨ sample storyï¼‰ã€‚

ä¸‹ä¸€æ‰¹ä½ å¯ä»¥ä¸Ÿ t2i.pyã€lora.pyï¼Œæˆ‘å€‘å†ç”¨åŒæ¨£æ–¹å¼ä¿®ã€‚

è¦ä¸è¦æˆ‘å¹«ä½ è£œä¸€å€‹æœ€å°çš„ schemas ç‰‡æ®µï¼ˆChatRequest/Responseã€ControlNetRequest/Responseã€ExportResponseï¼‰ä¾†ä¿è­‰é€™ä¸‰æ”¯ router swagger æ­£å¸¸ï¼Ÿ

You said:
finetune.py
Python
game.py
Python
health.py
Python
ç¾åœ¨æ˜¯é€™å¹¾å€‹æª”æ¡ˆ
ChatGPT said:
Thought for 2m 15s

å¤ªå¥½äº†ï¼æˆ‘å…ˆé‡å°ä½ è²¼çš„ä¸‰æ”¯ routerâ€”finetune.pyã€game.pyã€health.pyâ€”é€ä¸€æŒ‡å‡ºé—œéµå•é¡Œï¼Œæ¥è‘—çµ¦ä½ å¯ç›´æ¥è¦†è“‹çš„ä¿®æ­£ç‰ˆï¼ˆè‹±æ–‡è¨»è§£ã€ç¶­æŒåŸæœ¬ Claude çš„çµæ§‹ï¼‰ï¼Œä¸¦çµ±ä¸€æ¥åˆ° dependencies.pyï¼ˆä¸å†è‡ªè¡Œè®€ç’°å¢ƒè®Šæ•¸æˆ–åš sys.path hackï¼‰ã€‚

api/routers/finetune.pyï¼ˆå•é¡Œ & ä¿®æ­£ï¼‰

ä¸»è¦å•é¡Œ

åœ¨æª”æ¡ˆé ‚éƒ¨åš sys.path.append èˆ‡ workers åŒ¯å…¥ï¼Œè‹¥ Celery/Workers æœªå°±ç·’ï¼Œæ•´æ”¯ router import å°±æœƒå¤±æ•—ã€‚

å¤šè™•ä»¥ os.getenv("AI_CACHE_ROOT", ...) ç¡¬ç·¨è·¯å¾‘ï¼›æ‡‰æ”¹ç”¨ get_cache().get_path(KEY) çš„å–®ä¸€çœŸç›¸ä¾†æºã€‚

upload-dataset èˆ‡ presets ç³»åˆ—è·¯ç”±çš„è·¯å¾‘æ‡‰çµ±ä¸€äº¤çµ¦ shared_cacheï¼ˆå¦‚ DATASETS_RAWã€MODELS_LORAï¼‰ã€‚

Drop-in ç‰ˆï¼ˆç›´æ¥è¦†è“‹ï¼‰

# api/routers/finetune.py
"""
Fine-tuning API endpoints for LoRA training.
This version uses lazy imports for workers and unified cache paths via shared_cache.
"""

from __future__ import annotations

import uuid
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from datetime import datetime

from fastapi import APIRouter, HTTPException, BackgroundTasks, UploadFile, File, Form, Depends
from pydantic import BaseModel, Field
import yaml

from ..dependencies import get_cache  # <-- unified cache provider

router = APIRouter(prefix="/finetune", tags=["fine-tuning"])

# ----------------------------- Schemas -----------------------------

class LoRATrainingRequest(BaseModel):
    """LoRA training request schema"""
    config_path: str = Field(..., description="Path to training config YAML file")
    run_id: Optional[str] = Field(None, description="Custom run ID (auto-generated if not provided)")
    character_name: Optional[str] = Field(None, description="Character name for preset registration")
    notes: Optional[str] = Field(None, description="Training notes")


class LoRATrainingResponse(BaseModel):
    """LoRA training response schema"""
    job_id: str
    status: str
    config_path: str
    estimated_duration_minutes: int
    created_at: str


class TrainingStatus(BaseModel):
    """Training status response schema"""
    job_id: str
    status: str  # "pending", "running", "completed", "failed"
    progress: Optional[Dict[str, Any]] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: str
    updated_at: str


# --------------------------- Lazy workers --------------------------

_job_tracker = None  # singleton instance


def _lazy_training_modules():
    """Import training task and job tracker on demand (avoid import-time failures)."""
    try:
        from workers.tasks.training import train_lora_task  # type: ignore
        from workers.utils.job_tracker import JobTracker  # type: ignore
        return train_lora_task, JobTracker
    except Exception as e:  # ImportError or others
        raise HTTPException(status_code=503, detail=f"Training workers unavailable: {e}")


def _get_job_tracker():
    global _job_tracker
    if _job_tracker is None:
        _, JobTracker = _lazy_training_modules()
        _job_tracker = JobTracker()
    return _job_tracker


# ----------------------------- Routes ------------------------------

@router.post("/lora", response_model=LoRATrainingResponse)
async def submit_lora_training(
    background_tasks: BackgroundTasks,
    request: LoRATrainingRequest,
):
    """
    Submit LoRA training job via Celery.
    Uses the returned job_id to poll status.
    """
    config_path = Path(request.config_path)
    if not config_path.exists():
        raise HTTPException(status_code=404, detail=f"Config file not found: {request.config_path}")

    # Load config
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid config file: {e}")

    # Validate dataset path
    dataset_root = Path(config["dataset"]["root"])
    if not dataset_root.exists():
        raise HTTPException(status_code=404, detail=f"Dataset not found: {dataset_root}")

    # Estimate duration (rough heuristic)
    train_steps = config.get("train_steps", 4000)
    grad_accum = config.get("gradient_accumulation_steps", 8)
    effective_batches = max(train_steps / max(grad_accum, 1), 1)
    estimated_minutes = int(effective_batches * 1.2 / 60)  # ~1.2s / batch

    job_id = str(uuid.uuid4())
    job_data = {
        "job_id": job_id,
        "config_path": str(config_path),
        "run_id": request.run_id,
        "character_name": request.character_name,
        "notes": request.notes,
        "config": config,
        "estimated_duration_minutes": estimated_minutes,
    }

    # Enqueue via Celery (lazy import)
    train_lora_task, _ = _lazy_training_modules()
    task = train_lora_task.delay(job_data)

    # Track job
    jt = _get_job_tracker()
    jt.create_job(job_id=job_id, task_id=task.id, job_type="lora_training", config=job_data)

    return LoRATrainingResponse(
        job_id=job_id,
        status="pending",
        config_path=str(config_path),
        estimated_duration_minutes=estimated_minutes,
        created_at=datetime.now().isoformat(),
    )


@router.get("/jobs/{job_id}", response_model=TrainingStatus)
async def get_training_status(job_id: str):
    """Get training job status and progress."""
    jt = _get_job_tracker()
    job = jt.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return TrainingStatus(
        job_id=job_id,
        status=job["status"],
        progress=job.get("progress"),
        result=job.get("result"),
        error=job.get("error"),
        created_at=job["created_at"],
        updated_at=job["updated_at"],
    )


@router.delete("/jobs/{job_id}")
async def cancel_training_job(job_id: str):
    """Cancel a running training job."""
    jt = _get_job_tracker()
    job = jt.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    if job["status"] not in ["pending", "running"]:
        raise HTTPException(status_code=400, detail=f"Cannot cancel job with status: {job['status']}")

    # Revoke Celery task
    from workers.celery_app import celery_app  # type: ignore
    celery_app.control.revoke(job["task_id"], terminate=True)

    jt.update_job(job_id, {"status": "cancelled"})
    return {"message": "Job cancelled successfully"}


@router.get("/jobs")
async def list_training_jobs(status: Optional[str] = None, limit: int = 20, offset: int = 0):
    """List training jobs with optional filtering."""
    jt = _get_job_tracker()
    jobs = jt.list_jobs(job_type="lora_training", status=status, limit=limit, offset=offset)
    return {"jobs": jobs, "total": len(jobs), "limit": limit, "offset": offset}


@router.post("/upload-dataset")
async def upload_dataset(
    files: list[UploadFile] = File(...),
    character_name: str = Form(...),
    instance_token: str = Form(default="<token>"),
    notes: Optional[str] = Form(default=None),
    cache = Depends(get_cache),
):
    """
    Upload dataset files for training.
    Expected:
      - images: .png/.jpg
      - captions: .txt (same basename as image)
      - splits: train.txt, val.txt (optional)
    """
    # Use unified cache path (DATASETS_RAW)
    dataset_root = Path(cache.get_path("DATASETS_RAW")) / f"anime-char-{character_name}"
    image_dir = dataset_root / "images"
    caption_dir = dataset_root / "captions"
    splits_dir = dataset_root / "splits"
    for d in (image_dir, caption_dir, splits_dir):
        d.mkdir(parents=True, exist_ok=True)

    image_files: list[str] = []
    caption_files: list[str] = []
    split_files: list[str] = []

    for file in files:
        if not file.filename:
            continue
        suffix = Path(file.filename).suffix.lower()
        content = await file.read()

        if suffix in {".png", ".jpg", ".jpeg"}:
            (image_dir / file.filename).write_bytes(content)
            image_files.append(file.filename)
        elif suffix == ".txt" and Path(file.filename).stem not in {"train", "val", "test"}:
            (caption_dir / file.filename).write_text(content.decode("utf-8"), encoding="utf-8")
            caption_files.append(file.filename)
        elif Path(file.filename).name in {"train.txt", "val.txt", "test.txt"}:
            (splits_dir / file.filename).write_text(content.decode("utf-8"), encoding="utf-8")
            split_files.append(Path(file.filename).name)

    # Auto-generate train.txt if missing
    if "train.txt" not in split_files and image_files:
        (splits_dir / "train.txt").write_text("\n".join(sorted(image_files)))

    metadata = {
        "character_name": character_name,
        "instance_token": instance_token,
        "notes": notes,
        "files": {"images": len(image_files), "captions": len(caption_files), "splits": split_files},
        "created_at": datetime.now().isoformat(),
    }
    (dataset_root / "metadata.json").write_text(json.dumps(metadata, indent=2), encoding="utf-8")

    return {"message": "Dataset uploaded successfully", "dataset_path": str(dataset_root), "metadata": metadata}


@router.get("/presets")
async def list_lora_presets(cache = Depends(get_cache)):
    """List available LoRA presets (scanned from MODELS_LORA)."""
    lora_root = Path(cache.get_path("MODELS_LORA"))
    if not lora_root.exists():
        return {"presets": []}

    presets = []
    for preset_dir in lora_root.iterdir():
        if not preset_dir.is_dir():
            continue
        # prefer /final, otherwise latest checkpoint
        final_dir = preset_dir / "final"
        if final_dir.exists():
            model_path = final_dir
        else:
            ckpts = sorted(preset_dir.glob("checkpoints/step_*"))
            if not ckpts:
                continue
            model_path = ckpts[-1]

        metadata = {}
        meta_file = preset_dir / "metadata.json"
        if meta_file.exists():
            metadata = json.loads(meta_file.read_text(encoding="utf-8"))

        presets.append({
            "id": preset_dir.name,
            "path": str(model_path),
            "metadata": metadata,
            "created_at": metadata.get("created_at", "unknown"),
        })

    return {"presets": sorted(presets, key=lambda x: x["created_at"], reverse=True)}


@router.post("/presets/{preset_id}/register")
async def register_lora_preset(
    preset_id: str,
    character_name: str = Form(...),
    base_model: str = Form(...),
    lora_scale: float = Form(default=0.75),
    description: Optional[str] = Form(default=None),
    cache = Depends(get_cache),
):
    """Register a trained LoRA as a style preset for T2I generation."""
    lora_dir = Path(cache.get_path("MODELS_LORA")) / preset_id
    if not lora_dir.exists():
        raise HTTPException(status_code=404, detail="LoRA not found")

    final_dir = lora_dir / "final"
    if final_dir.exists():
        model_path = final_dir / "unet_lora"
    else:
        ckpts = sorted(lora_dir.glob("checkpoints/step_*/unet_lora"))
        if not ckpts:
            raise HTTPException(status_code=400, detail="No trained model found")
        model_path = ckpts[-1]

    preset_config = {
        "style_id": preset_id,
        "character_name": character_name,
        "base_model": base_model,
        "lora_path": str(model_path),
        "lora_scale": lora_scale,
        "description": description,
        "registered_at": datetime.now().isoformat(),
    }

    presets_dir = Path("configs/presets")
    presets_dir.mkdir(parents=True, exist_ok=True)
    preset_file = presets_dir / f"{preset_id}.yaml"
    preset_file.write_text(yaml.safe_dump(preset_config, sort_keys=False), encoding="utf-8")

    return {"message": "LoRA preset registered successfully", "preset_id": preset_id, "config_path": str(preset_file), "config": preset_config}

api/routers/game.pyï¼ˆå•é¡Œ & ä¿®æ­£ï¼‰

ä¸»è¦å•é¡Œ

ç›®å‰ç”¨è¨˜æ†¶é«”å­—å…¸ç¶­æŒéŠæˆ²ç‹€æ…‹ï¼Œå¯å…ˆç”¨ï¼›ä¹‹å¾Œå»ºè­°æ”¹æˆé€é get_game() æ³¨å…¥ story engineï¼ˆcore/story/*ï¼‰ï¼Œé¿å… router æ‰¿æ“”é‚è¼¯ã€‚

Drop-in ç‰ˆï¼ˆä¿ç•™ä½ åŸæœ¬è¡Œç‚ºï¼ŒåŠ å…¥è‹±æ–‡è¨»è§£èˆ‡å°å¹…å¼·åŒ–ï¼‰

# api/routers/game.py
from fastapi import APIRouter, HTTPException
from api.schemas import GameStartRequest, GameStepRequest, GameResponse
import uuid
from typing import Dict

router = APIRouter(prefix="/game", tags=["Game"])

# NOTE: In-memory sessions for now; consider moving to core.story engine later.
game_sessions: Dict[str, dict] = {}


@router.post("/new", response_model=GameResponse)
async def start_new_game(request: GameStartRequest):
    """Start a new text adventure game (simple in-memory prototype)."""
    try:
        session_id = str(uuid.uuid4())
        session = {
            "id": session_id,
            "persona": request.persona,
            "setting": request.setting,
            "difficulty": request.difficulty,
            "status": "active",
            "turn": 1,
            "inventory": ["basic clothes", "small pouch"],
            "health": 100,
            "history": [],
        }
        scene = (
            f"Welcome to the {request.setting} adventure! "
            f"You are a traveler who has just arrived at a mysterious village. "
            f"The {request.persona} persona guides your journey. What would you like to do?"
        )
        choices = [
            "Explore the village center",
            "Talk to the local villagers",
            "Check the nearby forest",
            "Rest at the tavern",
        ]
        session["current_scene"] = scene
        session["available_choices"] = choices
        game_sessions[session_id] = session

        return GameResponse(
            session_id=session_id,
            scene=scene,
            choices=choices,
            status="active",
            inventory=session["inventory"],
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/step", response_model=GameResponse)
async def game_step(request: GameStepRequest):
    """Take an action in the game."""
    try:
        if request.session_id not in game_sessions:
            raise HTTPException(status_code=404, detail="Game session not found")

        session = game_sessions[request.session_id]
        session["turn"] += 1
        session["history"].append({"turn": session["turn"] - 1, "action": request.action})

        action = request.action.lower()
        if "explore" in action:
            scene = (
                "You explore the area and discover interesting landmarks. "
                f"Your {session['persona']} intuition guides you to notice hidden details."
            )
            choices = ["Continue exploring", "Return to safety", "Investigate further", "Rest"]
        elif "talk" in action:
            scene = "You approach the villagers. They seem friendly but speak in whispers about strange occurrences."
            choices = ["Ask about the occurrences", "Offer help", "Leave politely", "Buy supplies"]
        elif "forest" in action:
            scene = "The forest is dark and mysterious. You hear strange sounds in the distance."
            choices = ["Enter deeper", "Turn back", "Listen carefully", "Light a torch"]
        else:
            scene = f"You decide to {request.action}. The world responds to your choice in unexpected ways."
            choices = ["Continue", "Reconsider", "Try something else", "Take a break"]

        session["current_scene"] = scene
        session["available_choices"] = choices

        return GameResponse(
            session_id=request.session_id,
            scene=scene,
            choices=choices,
            status=session["status"],
            inventory=session["inventory"],
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


æœªä¾†è¦åˆ‡æ›åˆ°çœŸæ­£çš„ story engineï¼šæŠŠ in-memory å€å¡Šæ›æˆ game = Depends(get_game)ï¼Œå‘¼å« game.new() / game.step(...) å³å¯ã€‚

api/routers/health.pyï¼ˆå•é¡Œ & ä¿®æ­£ï¼‰

ä¸»è¦å•é¡Œ

router = APIRouter(prefix="/health") åŒæ™‚ endpoint åˆæ˜¯ @router.get("/health") â†’ å¯¦éš›è·¯å¾‘æœƒè®Šæˆ /api/v1/health/healthï¼ˆé›™é‡ segmentï¼‰ã€‚

è®€ request.app.state.cache/config æœƒä¾è³´ main.py æ˜¯å¦æœ‰æŠŠç‰©ä»¶å¡é€² app.stateï¼›æ›´ç©©çš„åšæ³•æ˜¯ç›´æ¥ç”¨ dependencies.py çš„ get_cache()/get_settings()ã€‚

å…¶é¤˜çµæ§‹å¯ä¿ç•™ï¼›ç³»çµ±è³‡è¨Šç”¨ psutil OKã€‚

Drop-in ç‰ˆï¼ˆç›´æ¥è¦†è“‹ï¼‰

# api/routers/health.py
"""
Health Check Router
Provides system health and status information.
"""

from __future__ import annotations

import time
import psutil
from datetime import datetime
from typing import Dict, Any
from fastapi import APIRouter, Depends
from pydantic import BaseModel
import sys
import platform

from ..dependencies import get_cache, get_settings
from core.performance import gpu_available

router = APIRouter(tags=["health"])  # no prefix; keep path clean via main.py


class HealthResponse(BaseModel):
    """Health check response model"""
    status: str
    timestamp: datetime
    uptime_seconds: float
    version: str
    system: Dict[str, Any]
    cache: Dict[str, Any]
    config: Dict[str, Any]


_start_time = time.time()


@router.get("/health", response_model=HealthResponse)
async def health_check(cache = Depends(get_cache), settings = Depends(get_settings)):
    """
    Comprehensive health information including GPU, memory, and cache status.
    """
    uptime = time.time() - _start_time

    mem = psutil.virtual_memory()
    disk = psutil.disk_usage("/")

    system_info = {
        "cpu_percent": psutil.cpu_percent(interval=0.3),
        "memory_percent": mem.percent,
        "memory_available_gb": round(mem.available / 1024**3, 2),
        "memory_total_gb": round(mem.total / 1024**3, 2),
        "disk_free_gb": round(disk.free / 1024**3, 2),
        "disk_total_gb": round(disk.total / 1024**3, 2),
        "python_version": f"{sys.version_info.major}.{sys.version_info.minor}",
        "platform": platform.system(),
        "platform_release": platform.release(),
        "platform_machine": platform.machine(),
        "gpu_available": gpu_available(),
    }

    # Cache summary (shared_cache provides a summary method)
    try:
        cs = cache.get_summary()
        cache_info = {
            "root": cs["cache_root"],
            "directories_created": len(cs["directories"]),
            "gpu_info": cs.get("gpu_info", {}),
        }
    except Exception as e:
        cache_info = {"error": str(e)}

    # Config summary (if your config has a .get_summary)
    try:
        cfg = getattr(settings, "get_summary", None)
        summary = cfg() if callable(cfg) else {}
        config_info = {
            "app": summary.get("app", {}),
            "features": summary.get("features", {}),
            "model": summary.get("model", {}),
            "api": summary.get("api", {}),
        }
    except Exception as e:
        config_info = {"error": str(e)}

    return HealthResponse(
        status="healthy",
        timestamp=datetime.now(),
        uptime_seconds=round(uptime, 2),
        version="0.1.0",
        system=system_info,
        cache=cache_info,
        config=config_info,
    )


@router.get("/ready")
async def readiness_check(cache = Depends(get_cache), settings = Depends(get_settings)):
    """
    Readiness probe for Kubernetes/Docker.
    Returns 200 if essential components are available.
    """
    issues = []
    ready = True

    try:
        _ = cache.get_path  # callable attribute presence
    except Exception:
        ready = False
        issues.append("Shared cache not initialized")

    if settings is None:
        ready = False
        issues.append("Configuration not loaded")

    return {"status": "ready" if ready else "not_ready", "issues": issues, "timestamp": datetime.now()}


@router.get("/metrics")
async def metrics_endpoint():
    """
    Minimal metrics for scraping (Prometheus-style).
    """
    uptime = time.time() - _start_time
    mem = psutil.virtual_memory()

    metrics = {
        "saga_forge_uptime_seconds": uptime,
        "saga_forge_cpu_usage_percent": psutil.cpu_percent(interval=0.0),
        "saga_forge_memory_usage_percent": mem.percent,
        "saga_forge_memory_available_bytes": mem.available,
    }
    return metrics

é©—æ”¶å»ºè­°

é‡å•Ÿ API å¾Œæª¢æŸ¥ GET /openapi.jsonï¼Œæ‡‰çœ‹åˆ° /api/v1/healthï¼ˆä¸å†æ˜¯ /api/v1/health/healthï¼‰ã€/api/v1/finetune/*ã€/api/v1/game/*ã€‚

POST /api/v1/finetune/upload-dataset â†’ å›å‚³çš„ dataset_path æ‡‰ä½æ–¼ get_shared_cache().get_path("DATASETS_RAW") ä¸‹ã€‚

GET /api/v1/finetune/presets â†’ æƒåˆ° get_shared_cache().get_path("MODELS_LORA") ä¸‹çš„æ¨¡å‹ã€‚

GET /api/v1/health â†’ æœ‰ gpu_availableã€cache.root ç­‰æ¬„ä½ã€‚

æº–å‚™å¥½ä¸‹ä¸€æ‰¹æª”æ¡ˆå°±ä¸Ÿä¸Šä¾†ï¼Œæˆ‘æœƒç…§åŒä¸€å¥—æº–å‰‡å¹«ä½ æŠŠæ¯æ”¯ router éƒ½æ¥åˆ° dependencies.pyã€ä¿æŒ Claude çš„é¢¨æ ¼ã€è€Œä¸”å¯ç›´æ¥é‹è¡Œã€‚

You said:
llm.py
Python
lora.py
Python
monitoring.py
Python
ChatGPT said:
Thought for 1m 20s

å¤ªå¥½äº†ï¼æˆ‘é‡å°ä½ å‰›è²¼çš„ä¸‰æ”¯ routerï¼ˆllm.py / lora.py / monitoring.pyï¼‰åšäº†é«”æª¢ï¼Œä¸¦çµ¦å‡ºã€Œå¯ç›´æ¥è¦†è“‹ã€çš„ä¿®æ­£ç‰ˆï¼Œå…¨éƒ¨æ”¹æˆé€é api/dependencies.py å–å¾—å–®ä¾‹ï¼Œä¸å†åœ¨ router å…§ new å¼•æ“ã€ä¹Ÿç§»é™¤ sys.path hackï¼æ¨¡çµ„è¼‰å…¥å‰¯ä½œç”¨ã€‚ä¸‹æ–¹æ¯æ”¯æª”æ¡ˆéƒ½å…ˆåˆ—å‡ºé—œéµå•é¡Œï¼ˆå«å¼•ç”¨ï¼‰ï¼Œå†çµ¦ Drop-in ç‰ˆæœ¬ã€‚

1) api/routers/llm.pyï¼ˆStory turn / LLMï¼‰

å•é¡Œé‡é»

ç›´æ¥ sys.path.append èˆ‡åœ¨æ¨¡çµ„å±¤ bootstrap_cache()ï¼Œæœƒé€ æˆ import å‰¯ä½œç”¨ã€èˆ‡ core çš„çµ±ä¸€å•Ÿå‹•æµç¨‹è¡çªã€‚

åœ¨ router å…§è‡ªè¡Œ new TransformersLLM() èˆ‡ StoryEngine()ï¼Œæ²’æœ‰é€é dependencies.py å–®ä¾‹ï¼›å¦å¤– router = APIRouter(prefix="/api/v1", ...) æœƒå’Œ main.py çš„å…¨åŸŸ prefix ç–ŠåŠ ã€‚

Drop-inï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰

# api/routers/llm.py
"""LLM & story-turn router (DI-based)."""

from __future__ import annotations
from typing import List, Dict, Any, Optional
from threading import RLock

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field

from ..dependencies import get_llm  # shared LLM singleton
from core.story.engine import StoryEngine  # use core engine, no sys.path hacks

router = APIRouter(tags=["llm"])

# ----- API models (kept local to match your previous structure) -----

class PersonaAPI(BaseModel):
    name: str
    age: Optional[int] = None
    personality: List[str] = Field(default_factory=list)
    background: str = ""
    speaking_style: str = ""
    appearance: str = ""
    goals: List[str] = Field(default_factory=list)
    secrets: List[str] = Field(default_factory=list)
    memory_preferences: Dict[str, float] = Field(default_factory=dict)

class GameStateAPI(BaseModel):
    scene_id: str = "prologue"
    turn_count: int = 0
    flags: Dict[str, Any] = Field(default_factory=dict)
    inventory: List[str] = Field(default_factory=list)
    current_location: str = ""
    timeline_notes: List[str] = Field(default_factory=list)

class TurnRequestAPI(BaseModel):
    player_input: str
    persona: PersonaAPI
    game_state: GameStateAPI
    choice_id: Optional[str] = None

class DialogueAPI(BaseModel):
    speaker: str
    text: str
    emotion: Optional[str] = None

class ChoiceAPI(BaseModel):
    id: str
    text: str
    description: str = ""

class TurnResponseAPI(BaseModel):
    narration: str
    dialogues: List[DialogueAPI] = Field(default_factory=list)
    choices: List[ChoiceAPI] = Field(default_factory=list)
    updated_state: Optional[GameStateAPI] = None
    scene_change: Optional[str] = None

# ----- Lazy StoryEngine bound to the shared LLM -----

_story_engine: Optional[StoryEngine] = None
_lock = RLock()

def _ensure_engine(llm) -> StoryEngine:
    global _story_engine
    if _story_engine is None:
        with _lock:
            if _story_engine is None:
                _story_engine = StoryEngine(llm)
    return _story_engine

@router.get("/llm/health")
async def llm_health(llm = Depends(get_llm)):
    """Health check for LLM-backed story engine."""
    eng = _ensure_engine(llm)
    is_available = getattr(eng.llm, "is_available", lambda: True)()
    model_name = getattr(eng.llm, "model_name", "unknown")
    return {"status": "ok", "llm_available": is_available, "model": model_name}

@router.post("/llm/turn", response_model=TurnResponseAPI, summary="Process a story turn")
async def process_turn(request: TurnRequestAPI, llm = Depends(get_llm)):
    """Orchestrate a story turn with the shared LLM."""
    try:
        eng = _ensure_engine(llm)
        if hasattr(eng.llm, "is_available") and not eng.llm.is_available():
            raise HTTPException(status_code=503, detail="LLM model not available")

        # Build core objects using engine helpers to avoid tight coupling
        persona = eng.build_persona(**request.persona.dict())
        game_state = eng.build_game_state(**request.game_state.dict())

        turn_req = eng.build_turn_request(
            player_input=request.player_input,
            persona=persona,
            game_state=game_state,
            choice_id=request.choice_id,
        )
        resp = eng.process_turn(turn_req)

        api_resp = TurnResponseAPI(
            narration=resp.narration,
            dialogues=[DialogueAPI(speaker=d.speaker, text=d.text, emotion=d.emotion) for d in resp.dialogues],
            choices=[ChoiceAPI(id=c.id, text=c.text, description=c.description) for c in resp.choices],
            scene_change=resp.scene_change,
        )
        if resp.updated_state:
            api_resp.updated_state = GameStateAPI(**resp.updated_state.dict())
        return api_resp
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Turn processing failed: {e}")


ï¼ˆç§»é™¤æ¨¡çµ„å±¤ bootstrap_cache() èˆ‡ sys.path æ“ä½œï¼›ä¸å†åœ¨ router å…§ new LLMã€‚å°ç…§ä½ åŸæª”å¯è¦‹ä¸Šè¿°å•é¡Œã€‚
ï¼‰

2) api/routers/lora.pyï¼ˆLoRA ç®¡ç†ï¼‰

å•é¡Œé‡é»

åœ¨ router æ¨¡çµ„å±¤ç›´æ¥ lora_manager = LoRAManager()ï¼Œä¸”å¾ core.t2i åŒ¯å…¥é¡åˆ¥ï¼›é€™æœƒèˆ‡æˆ‘å€‘çš„ DI/å–®ä¾‹è¨­è¨ˆè¡çªï¼Œä¸”åœ¨ import æ™‚å°±ç”¢ç”Ÿå‰¯ä½œç”¨ã€‚

åŒ¯å…¥äº†æœªä½¿ç”¨çš„æ¨¡çµ„ï¼ˆpsutil, torch, timeï¼‰ã€‚

Drop-inï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰

# api/routers/lora.py
"""LoRA management router (DI-based)."""

from __future__ import annotations
from fastapi import APIRouter, Depends, HTTPException
from api.schemas import LoRAListResponse, LoRALoadRequest, LoRAInfo
from ..dependencies import get_lora  # <- use DI
from typing import List

router = APIRouter(prefix="/lora", tags=["lora"])

@router.get("/list", response_model=LoRAListResponse, summary="List available LoRA models")
async def list_loras(lora = Depends(get_lora)):
    try:
        items = lora.list()  # expected to return a list of dicts or ids
        infos: List[LoRAInfo] = []
        for it in items:
            if isinstance(it, str):
                infos.append(LoRAInfo(id=it, name=it, model_type="unknown", rank=0, path="", loaded=False))
            else:
                infos.append(LoRAInfo(
                    id=it.get("id", it.get("name", "")),
                    name=it.get("name", it.get("id", "")),
                    model_type=it.get("model_type", "unknown"),
                    rank=it.get("rank", 0),
                    path=it.get("path", ""),
                    loaded=it.get("loaded", False),
                ))
        return LoRAListResponse(loras=infos)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/load", summary="Load a LoRA model")
async def load_lora(req: LoRALoadRequest, lora = Depends(get_lora)):
    try:
        ok = lora.load(req.lora_id)  # optional: pass req.weight if your manager supports it
        if not ok:
            raise HTTPException(status_code=404, detail=f"LoRA '{req.lora_id}' not found or failed to load")
        return {"message": f"LoRA {req.lora_id} loaded", "weight": req.weight, "status": "success"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


ï¼ˆæ”¹ç‚ºé€é get_lora() æ³¨å…¥å–®ä¾‹ï¼Œç§»é™¤æ¨¡çµ„å±¤ new èˆ‡å¤šé¤˜ importã€‚
ï¼‰

3) api/routers/monitoring.pyï¼ˆç›£æ§ / æ•ˆèƒ½ï¼‰

å•é¡Œé‡é»

sys.path.append èˆ‡æ¨¡çµ„å±¤å…¨åŸŸç‰©ä»¶ï¼ˆmemory_manager, cache_managerï¼‰åœ¨ import æ™‚åˆå§‹åŒ–ï¼Œå®¹æ˜“ç‚¸ï¼›å¦å¤–ç¨‹å¼å…§å¤§é‡å‡è¨­å¤–éƒ¨æœå‹™ï¼ˆDB/Redis/Celeryï¼‰éƒ½å­˜åœ¨ï¼Œéœ€åœ¨ router ç«¯é™ç´šè™•ç†ã€‚

å¤šè™•ç›´æ¥ä½¿ç”¨ redis ä½†æœªç¢ºä¿åŒ¯å…¥æˆåŠŸã€get_config().redis_url ä¹Ÿå¯èƒ½ä¸å­˜åœ¨ï¼›éœ€è¦ try/exceptã€‚

Drop-inï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰

# api/routers/monitoring.py
"""Monitoring & performance router (lazy managers, graceful fallbacks)."""

from __future__ import annotations
from typing import Dict, Any, Optional, List
from datetime import datetime
import time

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import psutil
import torch

from core.config import get_config
from core.performance.memory_manager import MemoryManager, MemoryConfig
from core.performance.cache_manager import CacheManager, CacheConfig

# Celery is optional; import lazily in endpoints
try:
    from workers.celery_app import celery_app  # type: ignore
except Exception:  # pragma: no cover
    celery_app = None

router = APIRouter(prefix="/monitor", tags=["Monitoring"])

# ---------- Schemas ----------
class HealthStatus(BaseModel):
    service: str
    status: str  # "healthy", "degraded", "unhealthy", "unavailable", "error"
    latency_ms: Optional[float] = None
    details: Optional[Dict[str, Any]] = None

class SystemMetrics(BaseModel):
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    gpu_info: Optional[Dict[str, Any]] = None
    active_workers: int
    queue_depth: int
    uptime_seconds: float

# ---------- Lazy managers ----------
_mm: Optional[MemoryManager] = None
_cm: Optional[CacheManager] = None

def _get_managers():
    global _mm, _cm
    if _mm is None: _mm = MemoryManager(MemoryConfig())
    if _cm is None: _cm = CacheManager(CacheConfig())
    return _mm, _cm

_start = time.time()

@router.get("/models")
def get_loaded_models():
    """List currently loaded models and memory footprints."""
    mm, _ = _get_managers()
    info: Dict[str, Any] = {}
    for key, model in mm.loaded_models.items():
        try:
            item = {
                "device": str(getattr(model, "device", "unknown")),
                "dtype": str(getattr(model, "dtype", "unknown")),
                "memory_usage": mm.memory_usage.get(key, {}),
            }
            if hasattr(model, "num_parameters"):
                item["parameters"] = model.num_parameters()
            elif hasattr(model, "get_memory_footprint"):
                item["memory_footprint"] = model.get_memory_footprint()
            info[key] = item
        except Exception as e:
            info[key] = {"error": str(e)}
    return {"timestamp": datetime.now().isoformat(), "loaded_models": info, "total_loaded": len(info)}

@router.post("/cleanup")
def force_cleanup():
    """Force memory+cache cleanup."""
    mm, cm = _get_managers()
    before = mm.get_memory_info()
    cm.cleanup_expired()
    mm.cleanup_all()
    after = mm.get_memory_info()
    return {
        "status": "completed",
        "timestamp": datetime.now().isoformat(),
        "memory_before": before,
        "memory_after": after,
        "gpu_memory_freed_gb": before.get("gpu_allocated_gb", 0) - after.get("gpu_allocated_gb", 0),
    }

@router.post("/models/{model_key}/unload")
def unload_model(model_key: str):
    mm, _ = _get_managers()
    if model_key not in mm.loaded_models:
        raise HTTPException(status_code=404, detail=f"Model {model_key} not found")
    before = mm.get_memory_info()
    mm.unload_model(model_key)
    after = mm.get_memory_info()
    return {
        "status": "unloaded",
        "model_key": model_key,
        "timestamp": datetime.now().isoformat(),
        "memory_freed_gb": before.get("gpu_allocated_gb", 0) - after.get("gpu_allocated_gb", 0),
    }

@router.get("/cache/stats")
def cache_stats():
    _, cm = _get_managers()
    stats = cm.get_cache_stats()
    breakdown: Dict[str, Any] = {}
    try:
        from pathlib import Path
        cache_dir = Path(cm.config.disk_cache_dir)
        for prefix in ("emb:", "img:", "kv:"):
            files = list(cache_dir.glob(f"{prefix}*.json")) + list(cache_dir.glob(f"{prefix}*.pkl"))
            breakdown[prefix.rstrip(":")] = {"files": len(files), "size_mb": sum(f.stat().st_size for f in files) / 1024**2}
    except Exception as e:
        breakdown = {"error": str(e)}
    return {"timestamp": datetime.now().isoformat(), "overview": stats, "breakdown": breakdown}

@router.post("/cache/cleanup")
def cache_cleanup():
    _, cm = _get_managers()
    before = cm.get_cache_stats()
    cm.cleanup_expired()
    after = cm.get_cache_stats()
    return {
        "status": "completed",
        "timestamp": datetime.now().isoformat(),
        "files_before": before.get("disk_cache_files", 0),
        "files_after": after.get("disk_cache_files", 0),
        "size_freed_mb": before.get("disk_cache_size_mb", 0) - after.get("disk_cache_size_mb", 0),
    }

@router.get("/performance/recommendations")
def performance_recommendations():
    mm, cm = _get_managers()
    recs: List[Dict[str, Any]] = []
    mem = mm.get_memory_info()
    cache = cm.get_cache_stats()

    gpu_total = max(mem.get("gpu_total_gb", 8), 1)
    gpu_pct = (mem.get("gpu_allocated_gb", 0) / gpu_total) * 100
    if gpu_pct > 80:
        recs.append({"type":"memory","priority":"high","message":"High GPU memory usage.","action":"Enable CPU offload or reduce batch size."})
    if mem.get("memory_percent", 0) > 85:
        recs.append({"type":"memory","priority":"medium","message":"High RAM usage.","action":"Free system RAM."})
    if cache.get("disk_cache_size_mb", 0) > 1000:
        recs.append({"type":"cache","priority":"low","message":"Large disk cache.","action":"Run cache cleanup."})
    if not cache.get("redis_available", False):
        recs.append({"type":"cache","priority":"medium","message":"Redis not available.","action":"Start Redis server."})
    if not mm.config.enable_xformers:
        recs.append({"type":"performance","priority":"medium","message":"xformers disabled.","action":"Enable xformers in config."})

    return {"timestamp": datetime.now().isoformat(), "recommendations": recs, "total_recommendations": len(recs)}

@router.get("/health", response_model=Dict[str, HealthStatus])
async def health_check():
    """Comprehensive health check for DB/Redis/GPU/Workers (best-effort)."""
    out: Dict[str, HealthStatus] = {}
    cfg = get_config()

    # Database (optional async engine)
    try:
        from core.database import get_db_session  # type: ignore
        async with get_db_session() as db:
            start = time.time()
            await db.execute("SELECT 1")
            out["database"] = HealthStatus(service="database", status="healthy", latency_ms=(time.time() - start) * 1000)
    except Exception as e:
        out["database"] = HealthStatus(service="database", status="unhealthy", details={"error": str(e)})

    # Redis
    try:
        import redis  # type: ignore
        start = time.time()
        r = redis.from_url(getattr(cfg, "redis_url", "redis://localhost:6379/0"))
        r.ping()
        out["redis"] = HealthStatus(service="redis", status="healthy", latency_ms=(time.time() - start) * 1000)
    except Exception as e:
        out["redis"] = HealthStatus(service="redis", status="unhealthy", details={"error": str(e)})

    # GPU
    try:
        if torch.cuda.is_available():
            props = torch.cuda.get_device_properties(0)
            alloc = torch.cuda.memory_allocated(0)
            out["gpu"] = HealthStatus(
                service="gpu",
                status="healthy" if alloc / max(props.total_memory, 1) < 0.9 else "degraded",
                details={
                    "device_name": props.name,
                    "memory_used_gb": alloc / 1024**3,
                    "memory_total_gb": props.total_memory / 1024**3,
                },
            )
        else:
            out["gpu"] = HealthStatus(service="gpu", status="unavailable", details={"message": "No CUDA devices"})
    except Exception as e:
        out["gpu"] = HealthStatus(service="gpu", status="unhealthy", details={"error": str(e)})

    # Celery workers
    try:
        if celery_app is None:
            raise RuntimeError("Celery not configured")
        insp = celery_app.control.inspect()
        active = insp.active() or {}
        out["workers"] = HealthStatus(
            service="workers",
            status="healthy" if len(active) > 0 else "degraded",
            details={"active_workers": len(active), "worker_nodes": list(active.keys())},
        )
    except Exception as e:
        out["workers"] = HealthStatus(service="workers", status="unhealthy", details={"error": str(e)})

    return out

@router.get("/metrics", response_model=SystemMetrics)
def system_metrics():
    """System metrics for dashboards/scrapers."""
    cpu = psutil.cpu_percent(interval=1)
    mem = psutil.virtual_memory()
    disk = psutil.disk_usage("/")
    gpu_info: Optional[Dict[str, Any]] = None
    if torch.cuda.is_available():
        gpu_info = {
            "device_count": torch.cuda.device_count(),
            "devices": [
                {
                    "id": i,
                    "name": torch.cuda.get_device_properties(i).name,
                    "memory_total_gb": torch.cuda.get_device_properties(i).total_memory / 1024**3,
                    "memory_allocated_gb": torch.cuda.memory_allocated(i) / 1024**3,
                    "memory_reserved_gb": torch.cuda.memory_reserved(i) / 1024**3,
                }
                for i in range(torch.cuda.device_count())
            ],
        }

    # Celery queue depth via Redis (best-effort)
    active_count = 0
    queue_depth = 0
    try:
        if celery_app is not None:
            insp = celery_app.control.inspect()
            active = insp.active() or {}
            active_count = sum(len(v) for v in active.values())
        import redis  # type: ignore
        r = redis.from_url(getattr(get_config(), "redis_url", "redis://localhost:6379/0"))
        queue_depth = int(r.llen("celery"))
    except Exception:
        pass

    return SystemMetrics(
        timestamp=datetime.now(),
        cpu_percent=cpu,
        memory_percent=mem.percent,
        disk_percent=disk.percent,
        gpu_info=gpu_info,
        active_workers=active_count,
        queue_depth=queue_depth,
        uptime_seconds=time.time() - _start,
    )


ï¼ˆå»é™¤äº† sys.path.appendã€æ¨¡çµ„å±¤åˆå§‹åŒ–èˆ‡å°å¤–éƒ¨æœå‹™çš„ç¡¬ä¾è³´ï¼›æ‰€æœ‰æ˜‚è²´ä¾è³´/æœå‹™éƒ½ä»¥ lazy & best-effort æ–¹å¼è™•ç†ã€‚
ï¼‰

å°çµ & ä¸‹ä¸€æ­¥

æŠŠä¸Šé¢ä¸‰æ”¯è¦†è“‹å¾Œï¼Œé‡å•Ÿæœå‹™ï¼š

GET /api/v1/llm/health

POST /api/v1/llm/turnï¼ˆç”¨ä½ åŸæœ¬çš„ sample è«‹æ±‚ï¼‰

GET /api/v1/lora/listã€POST /api/v1/lora/load

GET /api/v1/monitor/healthã€GET /api/v1/monitor/metrics

è‹¥ core/story/engine.py çš„æ–¹æ³•ç°½åå’Œä¸Šé¢ç•¥æœ‰å·®ç•°ï¼Œæˆ‘å¯ä»¥å†å°æ‡‰èª¿æ•´ï¼ˆæˆ‘ä¿ç•™äº†ä½ åŸä¾† process_turn çš„çµæ§‹ï¼‰ã€‚

ä¸‹ä¸€æ‰¹æ­¡è¿ä¸Ÿ t2i.py / rag.py / safety.py / vlm.py ç­‰ï¼Œæˆ‘æœƒç”¨åŒæ¨£æ–¹å¼ä¿®åˆ°å¯é€šã€ä¸¦è·Ÿ dependencies.py å®Œæ•´å°é½Šã€‚

You said:
rag.py
Python
safety.py
Python
story.py
Python
ChatGPT said:
Thought for 1m 31s

æˆ‘çœ‹éä½ é€™ä¸‰æ”¯ routersï¼ˆrag.py / safety.py / story.pyï¼‰ã€‚ä¸‹é¢å…ˆé»å‡ºå„è‡ªçš„é—œéµå•é¡Œï¼ˆæœ‰å¼•ç”¨ä½ åŸæª”ï¼‰ï¼Œå†çµ¦ã€Œå¯ç›´æ¥è¦†è“‹ã€çš„ä¿®æ­£ç‰ˆã€‚ä¿®æ³•å®Œå…¨å°é½Šæˆ‘å€‘ä¹‹å‰å®šä¸‹çš„åŸå‰‡ï¼šä¸åœ¨ router å…§ new å¼•æ“ã€ä¸è®€ envã€ä¸åšå‰¯ä½œç”¨ï¼›ä¸€å¾‹é€é api/dependencies.py å–å–®ä¾‹èˆ‡è¨­å®š/å¿«å–ã€‚

1) rag.py â€” å•é¡Œé‡é»

æ··ç”¨ APIRouter èˆ‡åœ¨åŒæª”å…§å»ºç«‹ FastAPI appï¼Œè·¯ç”±åˆ†æ•£ä¸”æœƒé€ æˆé›™é‡è¨»å†Š / å‰ç¶´æ··äº‚ã€‚

åŒ¯å…¥æœŸåšå‰¯ä½œç”¨ï¼šbootstrap_cache()ã€ä»¥ sys.path.append ä¿®è·¯å¾‘ï¼Œä»¥åŠç›´æ¥ä½¿ç”¨ AI_CACHE_ROOT / APP_DIRSã€‚é€™äº›éƒ½æ‡‰äº¤ç”± dependencies.py èˆ‡ core.shared_cache è™•ç†ã€‚

ç›´æ¥åœ¨æª”æ¡ˆå±¤ç´š new ChineseRAGEngineã€DocumentMemoryï¼Œèˆ‡æˆ‘å€‘çš„å–®ä¾‹/DI è¨­è¨ˆè¡çªã€‚

âœ… Drop-in ç‰ˆï¼ˆç›´æ¥è¦†è“‹ api/routers/rag.pyï¼‰
# api/routers/rag.py
"""
RAG router (DI-based).
- No import-time side effects
- Engines injected from api/dependencies.py
- Unified request/response models
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional
from datetime import datetime
import base64
import io
import tempfile
import time

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile
from pydantic import BaseModel, Field

from ..dependencies import get_rag, get_llm  # shared singletons
from api.schemas import RAGRequest, RAGResponse, RAGSource  # keep your existing schemas

# Optional core types if your engine needs them
try:
    from core.rag.engine import DocumentMetadata  # type: ignore
except Exception:
    DocumentMetadata = None  # type: ignore

try:
    import fitz  # PyMuPDF for PDF parsing
except Exception:
    fitz = None  # graceful fallback

router = APIRouter(prefix="/rag", tags=["rag"])


# ----- Local models for /retrieve -----
class RetrieveRequest(BaseModel):
    query: str
    world_id: Optional[str] = None
    top_k: int = Field(8, ge=1, le=50)
    alpha: float = Field(0.7, ge=0.0, le=1.0)

class RetrieveItem(BaseModel):
    chunk_id: str
    doc_id: str
    text: str
    score: float
    section_title: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class RetrieveResponse(BaseModel):
    query: str
    results: List[RetrieveItem]
    total_found: int
    processing_time_ms: float


# ----- Helpers -----
def _parse_pdf_bytes(content: bytes) -> str:
    """Best-effort PDF to text."""
    if fitz is None:
        raise HTTPException(status_code=400, detail="PDF support is not available (PyMuPDF not installed)")
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(content)
            tmp.flush()
            doc = fitz.open(tmp.name)
            text = "".join(page.get_text() for page in doc)
            doc.close()
            return text
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF parsing failed: {e}")


# ----- Endpoints -----
@router.get("/health")
async def rag_health(rag = Depends(get_rag)):
    """Lightweight health info for RAG engine."""
    # Engine may expose stats attributes; keep best-effort
    chunks = getattr(rag, "num_chunks", lambda: None)()
    docs = getattr(rag, "num_documents", lambda: None)()
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "rag_chunks": chunks,
        "rag_documents": docs,
    }


@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    world_id: str = Form(...),
    title: Optional[str] = Form(None),
    license: str = Form("unspecified"),
    rag = Depends(get_rag),
):
    """Upload and index a document into RAG."""
    try:
        doc_id = f"{world_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        content = await file.read()

        if file.filename.lower().endswith(".pdf"):
            text_content = _parse_pdf_bytes(content)
        elif file.filename.lower().endswith((".md", ".txt")):
            text_content = content.decode("utf-8", errors="ignore")
        else:
            raise HTTPException(status_code=400, detail="Unsupported file type")

        meta_dict = {
            "doc_id": doc_id,
            "title": title or file.filename,
            "source": file.filename,
            "world_id": world_id,
            "upload_time": datetime.now().isoformat(),
            "license": license,
        }
        metadata = DocumentMetadata(**meta_dict) if DocumentMetadata else meta_dict  # tolerate absent class

        result = rag.add_document(text_content, metadata)
        return {
            "success": True,
            "doc_id": doc_id,
            "chunks_added": result.get("chunks_added", 0) if isinstance(result, dict) else result,
            "message": f"Document '{file.filename}' uploaded and indexed",
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {e}")


@router.post("/retrieve", response_model=RetrieveResponse)
async def retrieve_documents(request: RetrieveRequest, rag = Depends(get_rag)):
    """Retrieve relevant chunks with simple formatting."""
    start = time.time()
    try:
        results = rag.retrieve(
            query=request.query,
            world_id=request.world_id,
            top_k=request.top_k,
            alpha=request.alpha,
        )
        items: List[RetrieveItem] = []
        for r in results:
            items.append(
                RetrieveItem(
                    chunk_id=str(getattr(r, "chunk_id", "")),
                    doc_id=str(getattr(r, "doc_id", "")),
                    text=str(getattr(r, "text", "")),
                    score=round(float(getattr(r, "score", 0.0)), 4),
                    section_title=getattr(r, "section_title", None),
                    metadata=getattr(r, "metadata", {}) or {},
                )
            )
        return RetrieveResponse(
            query=request.query,
            results=items,
            total_found=len(items),
            processing_time_ms=round((time.time() - start) * 1000, 2),
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Retrieval failed: {e}")


@router.post("/ask", response_model=RAGResponse)
async def rag_ask(request: RAGRequest, rag = Depends(get_rag), llm = Depends(get_llm)):
    """
    Answer a question using retrieved context.
    Keeps LLM call minimal so it works with your stub or real engine.
    """
    try:
        hits = rag.retrieve(query=request.question, world_id=None, top_k=request.top_k, alpha=0.7)
        sources: List[RAGSource] = []
        context_snippets: List[str] = []

        for r in hits:
            text = str(getattr(r, "text", ""))
            meta = getattr(r, "metadata", {}) or {}
            sources.append(
                RAGSource(
                    content=text[:200] + ("..." if len(text) > 200 else ""),
                    score=float(getattr(r, "score", 0.0)),
                    metadata=meta,
                )
            )
            context_snippets.append(text)

        # Compose a very small prompt (works with MinimalLLM)
        context = "\n\n".join(context_snippets[:5]) if context_snippets else ""
        messages = [
            {"role": "system", "content": "Answer concisely using the provided context. If unsure, say you don't know."},
            {"role": "user", "content": f"Question: {request.question}\n\nContext:\n{context}"},
        ]
        answer = llm.chat(messages)

        return RAGResponse(answer=answer, sources=sources, model_used=getattr(llm, "model_name", "llm"))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"RAG ask failed: {e}")


@router.get("/stats")
async def rag_stats(rag = Depends(get_rag)):
    """Return internal stats if engine exposes them."""
    try:
        stats = getattr(rag, "get_stats", lambda: {})()
        return stats or {"status": "ok", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

2) safety.py â€” å•é¡Œé‡é»

åœ¨æª”æ¡ˆå…§ç›´æ¥ sys.path.append èˆ‡ get_cache_root()ï¼Œåˆåœ¨æœ¬æª” new SafetyEngine / LicenseManager / AttributionManagerï¼Œèˆ‡æˆ‘å€‘ã€Œä¾è³´æ³¨å…¥ + å–®ä¾‹ã€åŸå‰‡ä¸ä¸€è‡´ã€‚

å»ºè­°ï¼šæ”¹ç‚ºè‡ª dependencies.py æ³¨å…¥ get_safety_engine / get_license_manager / get_attribution_manager / get_compliance_logger å››å€‹å–®ä¾‹ï¼Œä¸¦ä½¿ç”¨ get_cache() æä¾›è·¯å¾‘ä¾†æºã€‚

éœ€è¦å…ˆåœ¨ api/dependencies.py å¢åŠ é€™äº› getterï¼ˆå¾ˆçŸ­ï¼‰
# --- add to api/dependencies.py (near other getters) ---
from threading import RLock
from core.shared_cache import get_shared_cache
from core.safety.detector import SafetyEngine
from core.safety.license import LicenseManager
from core.safety.watermark import AttributionManager, ComplianceLogger

_safety = None
_license_mgr = None
_attr = None
_compliance = None
_s_lock = RLock()

def get_safety_engine():
    global _safety
    if _safety is None:
        with _s_lock:
            if _safety is None:
                _safety = SafetyEngine()
    return _safety

def get_license_manager():
    global _license_mgr
    if _license_mgr is None:
        with _s_lock:
            if _license_mgr is None:
                cache = get_shared_cache()
                _license_mgr = LicenseManager(cache.cache_root)
    return _license_mgr

def get_attribution_manager():
    global _attr
    if _attr is None:
        with _s_lock:
            if _attr is None:
                cache = get_shared_cache()
                _attr = AttributionManager(cache.cache_root)
    return _attr

def get_compliance_logger():
    global _compliance
    if _compliance is None:
        with _s_lock:
            if _compliance is None:
                cache = get_shared_cache()
                _compliance = ComplianceLogger(cache.cache_root)
    return _compliance

âœ… Drop-in ç‰ˆï¼ˆç›´æ¥è¦†è“‹ api/routers/safety.pyï¼‰
# api/routers/safety.py
"""Safety & licensing router (DI-based)."""

from __future__ import annotations
from typing import Dict, Any, List, Optional
from pathlib import Path
import shutil
import json

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from PIL import Image

from ..dependencies import (
    get_safety_engine,
    get_license_manager,
    get_attribution_manager,
    get_compliance_logger,
    get_cache,  # if you need raw paths
)

router = APIRouter(prefix="/safety", tags=["safety"])


# ----- Schemas -----
class LicenseInfoRequest(BaseModel):
    license_type: str
    attribution_required: bool = True
    commercial_use: bool = True
    derivative_works: bool = True
    share_alike: bool = False
    source_url: Optional[str] = None
    author: Optional[str] = None
    license_text: Optional[str] = None
    restrictions: List[str] = Field(default_factory=list)

class SafetyCheckRequest(BaseModel):
    prompt: str
    check_nsfw: bool = True
    check_injection: bool = True

class WatermarkRequest(BaseModel):
    add_visible: bool = True
    position: str = "bottom_right"
    opacity: float = 0.6

class SafetyCheckResponse(BaseModel):
    is_safe: bool
    prompt_check: Dict[str, Any]
    image_check: Optional[Dict[str, Any]] = None
    actions_taken: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)

class UploadResponse(BaseModel):
    file_id: str
    safety_check: Dict[str, Any]
    license_check: Dict[str, Any]
    metadata_path: str


# ----- Endpoints -----
@router.post("/check/prompt", response_model=SafetyCheckResponse)
async def check_prompt_safety(
    request: SafetyCheckRequest, safety = Depends(get_safety_engine)
):
    """Check prompt safety for injection and harmful content."""
    try:
        prompt_result = safety.check_prompt_safety(request.prompt)
        is_safe = bool(prompt_result.get("is_safe", True))
        warnings = prompt_result.get("warnings", [])
        actions = ["prompt_blocked"] if not is_safe else []
        return SafetyCheckResponse(is_safe=is_safe, prompt_check=prompt_result, actions_taken=actions, warnings=warnings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Safety check failed: {e}")


@router.post("/upload", response_model=UploadResponse)
async def upload_with_safety_check(
    file: UploadFile = File(...),
    license_info: str = Form(..., description="JSON string of license information"),
    uploader_id: str = Form(...),
    auto_blur_faces: bool = Form(False),
    license_manager = Depends(get_license_manager),
    safety = Depends(get_safety_engine),
    compliance = Depends(get_compliance_logger),
    cache = Depends(get_cache),
):
    """Upload file with safety and license checks."""
    try:
        license_data = json.loads(license_info)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid license information JSON")

    try:
        cache_root = cache.cache_root
        temp_dir = Path(cache_root) / "uploads" / "temp"
        temp_dir.mkdir(parents=True, exist_ok=True)
        temp_file = temp_dir / f"temp_{file.filename}"

        with open(temp_file, "wb") as buf:
            shutil.copyfileobj(file.file, buf)

        # Image safety checks
        safety_result = {"is_safe": True}
        if file.content_type and file.content_type.startswith("image/"):
            try:
                image = Image.open(temp_file)
                safety_result = safety.check_image_safety(image)
                if auto_blur_faces and "processed_image" in safety_result:
                    processed = safety_result["processed_image"]
                    processed.save(temp_file)
            except Exception as e:
                safety_result = {"is_safe": False, "error": f"Image processing failed: {e}"}

        # Register upload & move if safe
        upload_meta = license_manager.register_upload(str(temp_file), license_data, uploader_id, safety_result)
        if safety_result.get("is_safe", False):
            final_dir = Path(cache_root) / "datasets" / "raw"
            final_dir.mkdir(parents=True, exist_ok=True)
            final_path = final_dir / f"{upload_meta.file_id}_{file.filename}"
            shutil.move(str(temp_file), str(final_path))
        else:
            temp_file.unlink(missing_ok=True)
            compliance.log_safety_violation("unsafe_upload",
                                            {"filename": file.filename, "content_type": file.content_type},
                                            "file_rejected")
            raise HTTPException(status_code=400, detail=f"Upload rejected: {safety_result}")

        validation = license_manager.validator.validate_license(license_data)
        compliance.log_upload(upload_meta.file_id, {"uploader_id": uploader_id, "license_info": license_data}, safety_result)

        return UploadResponse(
            file_id=upload_meta.file_id,
            safety_check=safety_result,
            license_check=validation,
            metadata_path=str(final_path.with_suffix(".json")),
        )
    except HTTPException:
        raise
    except Exception as e:
        # best-effort temp cleanup
        try: temp_file.unlink(missing_ok=True)  # type: ignore
        except Exception: pass
        raise HTTPException(status_code=500, detail=f"Upload failed: {e}")


@router.get("/health")
async def safety_health(
    safety = Depends(get_safety_engine),
    license_manager = Depends(get_license_manager),
    attribution = Depends(get_attribution_manager),
    compliance = Depends(get_compliance_logger),
):
    """Check status of safety components."""
    try:
        return {
            "status": "healthy",
            "components": {
                "safety_engine": "operational",
                "license_manager": "operational",
                "attribution_manager": "operational",
                "compliance_logger": "operational",
            },
        }
    except Exception as e:
        return JSONResponse(status_code=503, content={"status": "unhealthy", "error": str(e)})


ä»¥ä¸Šå®Œæ•´æ”¹æˆ é€é dependencies.py æ³¨å…¥ï¼Œä¸¦ç§»é™¤è·¯å¾‘/ç’°å¢ƒ/å‰¯ä½œç”¨å•é¡Œã€‚ä½ åŸæª”çš„æ‰€æœ‰å•é¡Œéƒ½å‡ºåœ¨ã€Œåœ¨ router å…§åšç³»çµ±åˆå§‹åŒ–ã€èˆ‡ã€Œè‡ªå·±ç®¡ç†è·¯å¾‘ã€ï¼Œç¾å·²è§£æ±ºã€‚

3) story.py â€” å•é¡Œé‡é»

åŒ¯å…¥æœŸå‰¯ä½œç”¨ï¼šbootstrap_cache()ã€sys.path.appendã€‚æ‡‰æ¸…é™¤ã€‚

å…©å€‹ dependencies (get_story_engine / get_persona_manager) åªå¯« passï¼Œå°è‡´å¯¦éš›åŸ·è¡Œæ™‚ä¸å¯ç”¨ï¼›æ‡‰æ¥åˆ° dependencies.py æˆ–ä»¥å–®ä¾‹æ³¨å…¥ã€‚

å…ˆåœ¨ api/dependencies.py å¢åŠ é€™å…©å€‹ getter
# --- add to api/dependencies.py ---
from core.story.engine import StoryEngine
from core.story.persona import PersonaManager

_story_engine = None
_persona_mgr = None

def get_story_engine(llm = None):
    # If llm DI is needed, import here to avoid circulars
    from .dependencies import get_llm  # local import to prevent cycle
    global _story_engine
    if _story_engine is None:
        _story_engine = StoryEngine(get_llm())
    return _story_engine

def get_persona_manager():
    global _persona_mgr
    if _persona_mgr is None:
        _persona_mgr = PersonaManager()
    return _persona_mgr

âœ… Drop-in ç‰ˆï¼ˆç›´æ¥è¦†è“‹ api/routers/story.pyï¼‰
# api/routers/story.py
"""Story & persona router (DI-based)."""

from __future__ import annotations

from typing import Dict, List, Optional, Any
from datetime import datetime
import logging

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field

from ..dependencies import get_story_engine, get_persona_manager
from core.story.game_state import GameState

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/story", tags=["story"])


# ----- API models -----
class TurnRequestModel(BaseModel):
    player_input: str = Field(..., description="Player input")
    choice_id: Optional[str] = Field(None, description="Choice ID if chosen")
    world_id: str = Field(..., description="World ID")
    session_id: str = Field(..., description="Session ID")

class GameStateResponse(BaseModel):
    session_id: str
    world_id: str
    current_scene: str
    current_location: str
    turn_count: int
    player_name: str
    player_health: int
    player_energy: int
    relationships: Dict[str, Any]
    inventory: Dict[str, Any]
    world_flags: Dict[str, Any]
    recent_events: List[Dict[str, Any]]

class StoryResponseModel(BaseModel):
    narration: str
    dialogues: List[Dict[str, str]]
    choices: List[Dict[str, Any]]
    citations: List[str]
    game_state_changes: Dict[str, Any]
    metadata: Dict[str, Any]

class SaveGameRequest(BaseModel):
    session_id: str
    save_name: str

class LoadGameRequest(BaseModel):
    session_id: str
    save_name: Optional[str] = None

class PersonaCreateRequest(BaseModel):
    character_id: str
    name: str
    description: str
    background: Optional[str] = ""
    personality: Optional[Dict[str, Any]] = {}
    speech_style: Optional[str] = ""
    appearance: Optional[str] = ""


# ----- In-memory stores for demo -----
game_sessions: Dict[str, GameState] = {}
save_files: Dict[str, Dict[str, GameState]] = {}


@router.post("/turn", response_model=StoryResponseModel)
async def process_turn(
    request: TurnRequestModel, story_engine = Depends(get_story_engine)
):
    """Process a story turn using DI-injected engine."""
    try:
        # get or create state
        game_state = game_sessions.get(request.session_id) or GameState(
            session_id=request.session_id, world_id=request.world_id
        )
        game_sessions[request.session_id] = game_state

        # Build and process
        turn_req = story_engine.build_turn_request(
            player_input=request.player_input,
            choice_id=request.choice_id,
            world_id=request.world_id,
            session_id=request.session_id,
        )
        response = await story_engine.process_turn(turn_req, game_state)

        return StoryResponseModel(
            narration=response.narration,
            dialogues=response.dialogues,
            choices=response.choices,
            citations=response.citations,
            game_state_changes=response.game_state_changes,
            metadata=response.metadata,
        )
    except Exception as e:
        logger.exception("Error processing turn")
        raise HTTPException(status_code=500, detail=f"Story processing error: {e}")


@router.get("/state/{session_id}", response_model=GameStateResponse)
async def get_game_state(session_id: str):
    """Get current game state."""
    if session_id not in game_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    gs = game_sessions[session_id]

    recent_events = [
        {
            "event_id": ev.event_id,
            "type": ev.event_type.value,
            "description": ev.description,
            "timestamp": ev.timestamp.isoformat(),
            "characters": ev.characters_involved,
        }
        for ev in gs.timeline[-5:]
    ]
    return GameStateResponse(
        session_id=gs.session_id,
        world_id=gs.world_id,
        current_scene=gs.current_scene,
        current_location=gs.current_location,
        turn_count=gs.turn_count,
        player_name=gs.player_name,
        player_health=gs.player_health,
        player_energy=gs.player_energy,
        relationships={k: f"{v.relation_type.value} (aff:{v.affinity}, trust:{v.trust})" for k, v in gs.relationships.items()},
        inventory={k: f"{v.name} x{v.quantity}" for k, v in gs.inventory.items()},
        world_flags={k: v.value for k, v in gs.world_flags.items()},
        recent_events=recent_events,
    )


@router.post("/save")
async def save_game(request: SaveGameRequest):
    """Save current session state."""
    if request.session_id not in game_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    save_files.setdefault(request.session_id, {})[request.save_name] = game_sessions[request.session_id]
    return {"success": True, "save_name": request.save_name, "timestamp": datetime.now().isoformat()}


@router.post("/load")
async def load_game(request: LoadGameRequest):
    """Load a saved session or list saves."""
    if request.session_id not in save_files:
        raise HTTPException(status_code=404, detail="No saves for this session")
    saves = save_files[request.session_id]
    if request.save_name:
        if request.save_name not in saves:
            raise HTTPException(status_code=404, detail="Save file not found")
        game_sessions[request.session_id] = saves[request.save_name]
        return {"success": True, "loaded_save": request.save_name, "turn_count": saves[request.save_name].turn_count}
    return {
        "saves": [
            {
                "save_name": name,
                "turn_count": state.turn_count,
                "last_updated": state.last_updated.isoformat(),
                "world_id": state.world_id,
                "current_scene": state.current_scene,
            }
            for name, state in saves.items()
        ]
    }


@router.post("/new_session")
async def create_new_session(world_id: str, player_name: str = "player", starting_scene: str = "opening"):
    """Create a new game session."""
    import uuid
    session_id = str(uuid.uuid4())
    gs = GameState(session_id=session_id, world_id=world_id, player_name=player_name, current_scene=starting_scene)
    game_sessions[session_id] = gs
    return {"session_id": session_id, "world_id": world_id, "player_name": player_name, "starting_scene": starting_scene}


@router.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """Delete a game session and its saves."""
    game_sessions.pop(session_id, None)
    save_files.pop(session_id, None)
    return {"success": True, "deleted_session": session_id}


# ---- Persona management ----
@router.post("/persona")
async def create_persona(req: PersonaCreateRequest, pm = Depends(get_persona_manager)):
    """Create a persona."""
    try:
        persona = pm.create_persona_from_data({
            "id": req.character_id,
            "name": req.name,
            "description": req.description,
            "background": req.background,
            "personality": req.personality,
            "speech_style": req.speech_style,
            "appearance": req.appearance,
        })
        pm.add_persona(persona)
        return {"success": True, "character_id": req.character_id}
    except Exception as e:
        logger.exception("create_persona failed")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/persona/{character_id}")
async def get_persona(character_id: str, pm = Depends(get_persona_manager)):
    """Get persona data."""
    persona = pm.get_persona(character_id)
    if not persona:
        raise HTTPException(status_code=404, detail="Character not found")
    return persona.to_dict()


@router.post("/persona/{character_id}/memory")
async def add_persona_memory(
    character_id: str,
    content: str,
    importance: int = 5,
    emotional_impact: int = 0,
    related_characters: List[str] = [],
    pm = Depends(get_persona_manager),
):
    """Append persona memory entry."""
    persona = pm.get_persona(character_id)
    if not persona:
        raise HTTPException(status_code=404, detail="Character not found")
    pm.update_persona_memory(
        character_id=character_id,
        event_description=content,
        importance=importance,
        emotional_impact=emotional_impact,
        related_characters=related_characters,
    )
    return {"success": True, "character_id": character_id, "memory_added": content[:50] + ("..." if len(content) > 50 else "")}


@router.get("/choices/{session_id}")
async def get_available_choices(session_id: str):
    """Return available choices (placeholder)."""
    if session_id not in game_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    choices = [
        {"id": "explore", "text": "Explore surroundings", "description": "Look around carefully"},
        {"id": "rest", "text": "Rest", "description": "Recover stamina"},
        {"id": "continue", "text": "Continue", "description": "Proceed with the current plot"},
    ]
    return {"choices": choices}


@router.get("/health")
async def health_check():
    """Basic health info for story router."""
    return {
        "status": "healthy",
        "active_sessions": len(game_sessions),
        "total_saves": sum(len(s) for s in save_files.values()),
        "timestamp": datetime.now().isoformat(),
    }


é€™æ”¯å·²ç§»é™¤ sys.path.append èˆ‡ bootstrap_cache() çš„å‰¯ä½œç”¨ï¼Œä¸¦æŠŠåŸæœ¬ pass çš„ä¾è³´æ”¹ç‚ºå¾ dependencies.py æ³¨å…¥ã€‚

é©—æ”¶ Checklist

é‡æ–°å•Ÿå‹•å¾Œæª¢æŸ¥ GET /openapi.jsonï¼šæ‡‰æœ‰ /api/v1/rag/*ã€/api/v1/safety/*ã€/api/v1/story/*ã€‚

POST /api/v1/rag/uploadï¼ˆä¸Šå‚³ txt æˆ– pdfï¼‰â†’ å›æ‡‰ doc_id / chunks_addedã€‚

POST /api/v1/rag/retrieve â†’ å› results èˆ‡ processing_time_msã€‚

POST /api/v1/rag/ask â†’ æœƒç”¨åˆ°æ³¨å…¥çš„ LLMï¼ˆstub ä¹Ÿå¯ï¼‰ã€‚

POST /api/v1/safety/check/prompt èˆ‡ /safety/upload â†’ å¯ 200ï¼›è‹¥æœªè£ PyMuPDF/Redis ä¹Ÿä¸æ‡‰æ•´é«”ç‚¸æ‰ã€‚

POST /api/v1/story/turn â†’ 200 ä¸¦è¿”å› narration/choices ç­‰ã€‚

è‹¥ä½ å¸Œæœ›ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¹«ä½ æŠŠ api/schemas.py è£¡è·Ÿ RAG/Story/Safety ç›¸é—œçš„ models ä¸€ä½µå°é½Šï¼ˆç¢ºä¿å‹åˆ¥åç¨±èˆ‡æ¬„ä½å®Œå…¨åŒ¹é…ï¼‰ï¼Œæˆ–è€…ç›´æ¥å¹«ä½ åœ¨ dependencies.py è£œä¸Šæˆ‘æ–‡ä¸­æåˆ°çš„å®‰å…¨/æ•…äº‹å…©çµ„ getterã€‚

You said:
t2i.py
Python
vlm.py
Python
vqa.py
Python
æœ€å¾Œæ˜¯é€™å¹¾å€‹
ChatGPT said:
Thought for 1m 13s

å¤ªå¥½äº†ï¼Œæœ€å¾Œé€™ 3 æ”¯æˆ‘å¹«ä½ ã€Œå°±åœ°é«”æª¢ï¼‹å¯ç›´æ¥è¦†è“‹çš„ä¿®æ­£ç‰ˆã€ã€‚åŸå‰‡ä¸€æ¨£ï¼š

ä¸åœ¨ router å…§ new å¼•æ“ã€ä¸è®€ envã€ä¸åšå‰¯ä½œç”¨ï¼›

ä¸€å¾‹é€é api/dependencies.py å–å¾—å–®ä¾‹ï¼ˆget_t2i / get_vlm / get_rag ç­‰ï¼‰ï¼›

ä¿ç•™ä½ /Claude çš„çµæ§‹èˆ‡è·¯å¾‘é¢¨æ ¼ï¼›è¨»è§£å…¨è‹±æ–‡ã€‚

ä¸‹æ–¹æ¯æ”¯éƒ½å…ˆé»å‡ºé—œéµå•é¡Œï¼ˆå¼•ç”¨ä½ æª”æ¡ˆï¼‰ï¼Œå†çµ¦ Drop-in ç‰ˆæœ¬ã€‚

1) api/routers/t2i.py

é—œéµå•é¡Œï¼ˆä½ åŸæª”ï¼‰

åœ¨æ¨¡çµ„å±¤ sys.path.append(...) èˆ‡ bootstrap_cache()ï¼Œå±¬æ–¼ import-time å‰¯ä½œç”¨ã€‚

ç›´æ¥ç¶æ­»åˆ° core.t2i.pipeline.t2i_pipeline èˆ‡ LoRAManager() çš„æ¨¡çµ„ç´šå–®ä¾‹ï¼Œç¹éæˆ‘å€‘çš„ä¾è³´æ³¨å…¥ã€‚

Drop-inï¼šä¾è³´æ³¨å…¥ç‰ˆæœ¬ï¼ˆå¯ç›´æ¥è¦†è“‹ï¼‰

# api/routers/t2i.py
"""
Text-to-Image router (DI-based).
- No import-time side effects
- Engines injected via api/dependencies.py
"""

from __future__ import annotations
from typing import Optional, Dict, Any
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel, Field
import torch

from api.schemas import T2IResponse  # keep your shared schema if you use it elsewhere
from ..dependencies import get_t2i

router = APIRouter(prefix="/t2i", tags=["text-to-image"])


# Keep Claude-style local models for this file (backward compatible)
class GenerationRequest(BaseModel):
    prompt: str = Field(..., description="Generation prompt")
    negative_prompt: str = Field("", description="Negative prompt")
    width: int = Field(768, ge=256, le=1024, description="Image width")
    height: int = Field(768, ge=256, le=1024, description="Image height")
    num_inference_steps: int = Field(25, ge=10, le=50, description="Inference steps")
    guidance_scale: float = Field(7.5, ge=1.0, le=20.0, description="Guidance scale")
    seed: Optional[int] = Field(None, description="Random seed")
    style_id: Optional[str] = Field(None, description="Style preset ID")
    scene_id: str = Field("scene_001", description="Scene identifier")
    image_type: str = Field("bg", description="Image type (bg/character/portrait)")


class GenerationResponse(BaseModel):
    success: bool
    image_path: Optional[str] = None
    metadata_path: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class ModelInfo(BaseModel):
    current_model: Optional[str]
    available_styles: Dict[str, str]
    gpu_available: bool
    memory_info: Optional[Dict[str, Any]] = None


@router.get("/models", response_model=ModelInfo)
async def get_model_info(t2i = Depends(get_t2i)):
    """Return current model and available styles."""
    try:
        memory_info = None
        if torch.cuda.is_available():
            memory_info = {
                "allocated_mb": torch.cuda.memory_allocated(0) / 1024**2,
                "reserved_mb": torch.cuda.memory_reserved(0) / 1024**2,
            }

        # Be tolerant: engines may expose different style APIs
        get_styles = getattr(t2i, "get_available_styles", None) or getattr(t2i, "styles", None)
        styles = get_styles() if callable(get_styles) else {}

        current_model = getattr(t2i, "current_model", None)
        return ModelInfo(
            current_model=current_model,
            available_styles=styles or {},
            gpu_available=torch.cuda.is_available(),
            memory_info=memory_info,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get model info: {e}")


@router.post("/generate", response_model=GenerationResponse, summary="Generate an image from text")
async def generate_image(request: GenerationRequest, t2i = Depends(get_t2i)):
    """Delegate generation to the shared T2I engine."""
    try:
        # Optional style validation if engine exposes styles
        get_styles = getattr(t2i, "get_available_styles", None) or getattr(t2i, "styles", None)
        if request.style_id and callable(get_styles):
            styles = get_styles() or {}
            if request.style_id not in styles:
                raise HTTPException(status_code=400, detail=f"Unknown style '{request.style_id}'")

        result = t2i.txt2img(
            prompt=request.prompt,
            negative=request.negative_prompt,
            steps=request.num_inference_steps,
            width=request.width,
            height=request.height,
            guidance_scale=request.guidance_scale,
            seed=request.seed,
            style_id=request.style_id,
            scene_id=request.scene_id,
            image_type=request.image_type,
        )

        # result should contain image_path / metadata_path; keep response shape
        ok = bool(result.get("image_path"))
        return GenerationResponse(
            success=ok,
            image_path=result.get("image_path"),
            metadata_path=result.get("metadata_path"),
            metadata=result.get("metadata"),
            error=None if ok else result.get("error", "Unknown error"),
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {e}")


@router.post("/load_model")
async def load_model(model_id: str = Query(..., description="Model ID to load"), t2i = Depends(get_t2i)):
    """Load a specific base model (if engine supports it)."""
    try:
        if not hasattr(t2i, "load_model"):
            raise HTTPException(status_code=501, detail="Model loading is not supported by current engine")
        success = t2i.load_model(model_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to load model")
        return {"success": True, "model": model_id, "message": "Model loaded successfully"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model loading error: {e}")


@router.post("/unload_model")
async def unload_model(t2i = Depends(get_t2i)):
    """Unload current model to free VRAM (if engine supports it)."""
    try:
        if not hasattr(t2i, "unload_model"):
            raise HTTPException(status_code=501, detail="Model unloading is not supported by current engine")
        t2i.unload_model()
        return {"success": True, "message": "Model unloaded successfully"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Model unloading error: {e}")


@router.get("/styles")
async def list_styles(t2i = Depends(get_t2i)):
    """List available style presets."""
    get_styles = getattr(t2i, "get_available_styles", None) or getattr(t2i, "styles", None)
    styles = get_styles() if callable(get_styles) else {}
    # Some engines may store presets in attribute style_presets
    presets = getattr(t2i, "style_presets", styles)
    return {"styles": styles or {}, "count": len(presets) if hasattr(presets, "__len__") else len(styles or {})}