#!/usr/bin/env python3
# scripts/benchmark_suite.py
"""
æ€§èƒ½åŸºæº–æ¸¬è©¦å¥—ä»¶
"""

import time
import statistics
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import sys
import requests

# Add backend path
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

from tests.mocks.sample_data import get_sample_data


class BenchmarkSuite:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.results = {}
        self.session = requests.Session()

    def time_function(self, func, *args, iterations=5, **kwargs):
        """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“"""
        times = []

        for _ in range(iterations):
            start = time.perf_counter()
            result = func(*args, **kwargs)
            end = time.perf_counter()
            times.append(end - start)

        return {
            "mean": statistics.mean(times),
            "median": statistics.median(times),
            "stdev": statistics.stdev(times) if len(times) > 1 else 0,
            "min": min(times),
            "max": max(times),
            "iterations": iterations,
            "raw_times": times,
        }

    def benchmark_api_endpoint(self, endpoint, method="GET", data=None, files=None):
        """åŸºæº–æ¸¬è©¦APIç«¯é»"""

        def make_request():
            url = f"{self.base_url}{endpoint}"

            if method == "GET":
                response = self.session.get(url)
            elif method == "POST":
                if files:
                    response = self.session.post(url, data=data, files=files)
                else:
                    response = self.session.post(url, json=data)

            response.raise_for_status()
            return response.json()

        return self.time_function(make_request, iterations=10)

    def benchmark_concurrent_requests(self, endpoint, num_requests=50, max_workers=10):
        """ä¸¦ç™¼è«‹æ±‚åŸºæº–æ¸¬è©¦"""

        def make_request():
            response = self.session.get(f"{self.base_url}{endpoint}")
            return response.status_code == 200

        start_time = time.perf_counter()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            results = [future.result() for future in as_completed(futures)]

        end_time = time.perf_counter()

        success_rate = sum(results) / len(results)
        total_time = end_time - start_time
        throughput = num_requests / total_time

        return {
            "total_time": total_time,
            "success_rate": success_rate,
            "throughput": throughput,
            "requests_per_second": throughput,
        }

    def benchmark_memory_usage(self):
        """è¨˜æ†¶é«”ä½¿ç”¨åŸºæº–æ¸¬è©¦"""
        process = psutil.Process()

        initial_memory = process.memory_info()

        # åŸ·è¡Œä¸€äº›æ“ä½œ
        sample_docs = get_sample_data("documents")

        for i in range(20):
            # æ¨¡æ“¬RAGæ“ä½œ
            self.benchmark_api_endpoint(
                "/api/v1/rag/add",
                method="POST",
                data={
                    "content": sample_docs[i % len(sample_docs)]["content"],
                    "metadata": sample_docs[i % len(sample_docs)]["metadata"],
                },
            )

        final_memory = process.memory_info()

        return {
            "initial_rss": initial_memory.rss / 1024 / 1024,  # MB
            "final_rss": final_memory.rss / 1024 / 1024,  # MB
            "memory_growth": (final_memory.rss - initial_memory.rss) / 1024 / 1024,
            "initial_vms": initial_memory.vms / 1024 / 1024,
            "final_vms": final_memory.vms / 1024 / 1024,
        }

    def run_full_benchmark(self):
        """åŸ·è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦"""
        print("ğŸš€ Starting Multi-Modal Lab Benchmark Suite")
        print("=" * 50)

        # å¥åº·æª¢æŸ¥
        print("ğŸ“‹ Health Check...")
        health_result = self.benchmark_api_endpoint("/api/v1/health")
        self.results["health_check"] = health_result
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {health_result['mean']:.3f}s")

        # ç‹€æ…‹ç«¯é»
        print("ğŸ“Š Status Endpoint...")
        status_result = self.benchmark_api_endpoint("/api/v1/status")
        self.results["status_check"] = status_result
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {status_result['mean']:.3f}s")

        # èŠå¤©ç«¯é»
        print("ğŸ’¬ Chat Endpoint...")
        chat_result = self.benchmark_api_endpoint(
            "/api/v1/chat",
            method="POST",
            data={"message": "æ¸¬è©¦è¨Šæ¯", "conversation_id": "bench-test"},
        )
        self.results["chat"] = chat_result
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {chat_result['mean']:.3f}s")

        # RAGæ“ä½œ
        print("ğŸ” RAG Operations...")

        # æ·»åŠ æ–‡æª”
        sample_docs = get_sample_data("documents")
        add_doc_result = self.benchmark_api_endpoint(
            "/api/v1/rag/add",
            method="POST",
            data={
                "content": sample_docs[0]["content"],
                "metadata": sample_docs[0]["metadata"],
            },
        )
        self.results["rag_add"] = add_doc_result
        print(f"   æ–‡æª”æ·»åŠ å¹³å‡æ™‚é–“: {add_doc_result['mean']:.3f}s")

        # æ–‡æª”æœå°‹
        search_result = self.benchmark_api_endpoint(
            "/api/v1/rag/search", method="POST", data={"query": "äººå·¥æ™ºæ…§", "top_k": 5}
        )
        self.results["rag_search"] = search_result
        print(f"   æœå°‹å¹³å‡æ™‚é–“: {search_result['mean']:.3f}s")

        # ä¸¦ç™¼æ¸¬è©¦
        print("âš¡ Concurrent Load Test...")
        concurrent_result = self.benchmark_concurrent_requests(
            "/api/v1/health", num_requests=100
        )
        self.results["concurrent_load"] = concurrent_result
        print(f"   ååé‡: {concurrent_result['throughput']:.1f} req/s")
        print(f"   æˆåŠŸç‡: {concurrent_result['success_rate']:.1%}")

        # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
        print("ğŸ§  Memory Usage Test...")
        memory_result = self.benchmark_memory_usage()
        self.results["memory_usage"] = memory_result
        print(f"   è¨˜æ†¶é«”å¢é•·: {memory_result['memory_growth']:.1f}MB")

        return self.results

    def generate_report(self, output_file="benchmark_report.json"):
        """ç”ŸæˆåŸºæº–æ¸¬è©¦å ±å‘Š"""
        report = {
            "timestamp": time.time(),
            "system_info": {
                "cpu_count": psutil.cpu_count(),
                "memory_total": psutil.virtual_memory().total
                / 1024
                / 1024
                / 1024,  # GB
                "python_version": sys.version,
            },
            "results": self.results,
        }

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ˆ åŸºæº–æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {output_file}")

        # ç”Ÿæˆæ‘˜è¦
        print("\nğŸ“‹ åŸºæº–æ¸¬è©¦æ‘˜è¦:")
        print("-" * 30)

        if "health_check" in self.results:
            print(f"å¥åº·æª¢æŸ¥: {self.results['health_check']['mean']:.3f}s")

        if "chat" in self.results:
            print(f"èŠå¤©éŸ¿æ‡‰: {self.results['chat']['mean']:.3f}s")

        if "rag_search" in self.results:
            print(f"RAGæœå°‹: {self.results['rag_search']['mean']:.3f}s")

        if "concurrent_load" in self.results:
            print(
                f"ä¸¦ç™¼åå: {self.results['concurrent_load']['throughput']:.1f} req/s"
            )

        if "memory_usage" in self.results:
            print(f"è¨˜æ†¶é«”å¢é•·: {self.results['memory_usage']['memory_growth']:.1f}MB")

        return report


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Multi-Modal Lab Benchmark Suite")
    parser.add_argument("--url", default="http://localhost:8000", help="API base URL")
    parser.add_argument("--output", default="benchmark_report.json", help="Output file")
    parser.add_argument(
        "--quick", action="store_true", help="Quick benchmark (fewer iterations)"
    )

    args = parser.parse_args()

    # æª¢æŸ¥APIå¯ç”¨æ€§
    try:
        response = requests.get(f"{args.url}/api/v1/health", timeout=5)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ APIä¸å¯ç”¨: {args.url}")
        print(f"   éŒ¯èª¤: {e}")
        print("   è«‹ç¢ºä¿APIæœå‹™æ­£åœ¨é‹è¡Œ")
        return 1

    # é‹è¡ŒåŸºæº–æ¸¬è©¦
    benchmark = BenchmarkSuite(args.url)

    try:
        results = benchmark.run_full_benchmark()
        report = benchmark.generate_report(args.output)

        print(f"\nğŸ‰ åŸºæº–æ¸¬è©¦å®Œæˆ!")
        return 0

    except Exception as e:
        print(f"\nâŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        return 1


if __name__ == "__main__":
    main()
