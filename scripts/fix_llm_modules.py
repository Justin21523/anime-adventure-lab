#!/usr/bin/env python3
# scripts/fix_llm_modules.py
"""
Fix and Test LLM Modules (already pass)
å®Œæ•´ä¿®å¾© LLM æ¨¡çµ„çš„å°å…¥è¡çªå’Œå¯¦ä½œå•é¡Œ
"""

import os
import sys
from pathlib import Path

# Add project root to path
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT_DIR))

# Set up environment
os.environ["AI_CACHE_ROOT"] = str(ROOT_DIR.parent / "ai_warehouse" / "cache")


def setup_test_environment():
    """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
    print("ğŸ”§ è¨­ç½®æ¸¬è©¦ç’°å¢ƒ...")

    # Create cache directories
    cache_root = Path(os.environ["AI_CACHE_ROOT"])
    cache_root.mkdir(parents=True, exist_ok=True)

    # Bootstrap shared cache
    try:
        from core.shared_cache import bootstrap_cache

        cache = bootstrap_cache()
        print(f"   âœ… å…±äº«å¿«å–åˆå§‹åŒ–: {cache.cache_root}")
    except Exception as e:
        print(f"   âŒ å¿«å–åˆå§‹åŒ–å¤±æ•—: {e}")
        return False

    return True


def test_core_imports():
    """æ¸¬è©¦æ ¸å¿ƒæ¨¡çµ„å°å…¥"""
    print("\nğŸ“¦ æ¸¬è©¦æ ¸å¿ƒæ¨¡çµ„å°å…¥...")

    try:
        # Test basic exceptions
        from core.exceptions import (
            ModelLoadError,
            ModelNotFoundError,
            CUDAOutOfMemoryError,
            SessionNotFoundError,
            ContextLengthExceededError,
            ValidationError,
        )

        print("   âœ… æ ¸å¿ƒç•°å¸¸é¡åˆ¥å°å…¥æˆåŠŸ")

        # Test config
        from core.config import get_config

        config = get_config()
        print("   âœ… é…ç½®æ¨¡çµ„å°å…¥æˆåŠŸ")

        # Test shared cache
        from core.shared_cache import get_shared_cache

        cache = get_shared_cache()
        print("   âœ… å…±äº«å¿«å–æ¨¡çµ„å°å…¥æˆåŠŸ")

        return True

    except ImportError as e:
        print(f"   âŒ æ ¸å¿ƒæ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        return False


def test_llm_imports():
    """æ¸¬è©¦ LLM æ¨¡çµ„å°å…¥"""
    print("\nğŸ¤– æ¸¬è©¦ LLM æ¨¡çµ„å°å…¥...")

    try:
        # Test base classes
        from core.llm.base import BaseLLM, ChatMessage, LLMResponse

        print("   âœ… LLM åŸºç¤é¡åˆ¥å°å…¥æˆåŠŸ")

        # Test model loader
        from core.llm.model_loader import ModelLoader, ModelLoadConfig, get_model_loader

        print("   âœ… æ¨¡å‹è¼‰å…¥å™¨å°å…¥æˆåŠŸ")

        # Test chat manager
        from core.llm.chat_manager import ChatManager, ChatSession, get_chat_manager

        print("   âœ… å°è©±ç®¡ç†å™¨å°å…¥æˆåŠŸ")

        # Test context manager
        from core.llm.context_manager import (
            ContextManager,
            ContextWindow,
            get_context_manager,
        )

        print("   âœ… ä¸Šä¸‹æ–‡ç®¡ç†å™¨å°å…¥æˆåŠŸ")

        # Test adapter
        from core.llm.adapter import (
            EnhancedTransformersLLM,
            MockLLMAdapter,
            get_llm_adapter,
        )

        print("   âœ… LLM é©é…å™¨å°å…¥æˆåŠŸ")

        # Test unified imports via __init__.py
        from core.llm import (
            get_llm_adapter,
            get_model_loader,
            get_chat_manager,
            get_context_manager,
            ChatMessage,
            LLMResponse,
            ModelLoadConfig,
        )

        print("   âœ… çµ±ä¸€æ¥å£å°å…¥æˆåŠŸ")

        return True

    except ImportError as e:
        print(f"   âŒ LLM æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_model_loader():
    """æ¸¬è©¦æ¨¡å‹è¼‰å…¥å™¨åŠŸèƒ½"""
    print("\nâš™ï¸ æ¸¬è©¦æ¨¡å‹è¼‰å…¥å™¨åŠŸèƒ½...")

    try:
        from core.llm import get_model_loader, ModelLoadConfig

        # Create loader
        loader = get_model_loader()
        print("   âœ… ModelLoader å¯¦ä¾‹å‰µå»ºæˆåŠŸ")

        # Test config creation
        config = ModelLoadConfig(
            model_name="test/model",
            device_map="cpu",
            torch_dtype="float32",
            use_quantization=False,
        )
        print("   âœ… ModelLoadConfig å‰µå»ºæˆåŠŸ")

        # Test config serialization
        config_dict = config.to_dict()
        cache_key = config.get_cache_key()
        print(f"   âœ… é…ç½®åºåˆ—åŒ–æˆåŠŸï¼Œå¿«å–éµ: {cache_key[:8]}...")

        # Test memory stats
        memory_stats = loader.get_memory_usage()
        print(f"   âœ… è¨˜æ†¶é«”çµ±è¨ˆ: {memory_stats}")

        return True

    except Exception as e:
        print(f"   âŒ æ¨¡å‹è¼‰å…¥å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_chat_manager():
    """æ¸¬è©¦å°è©±ç®¡ç†å™¨åŠŸèƒ½"""
    print("\nğŸ’¬ æ¸¬è©¦å°è©±ç®¡ç†å™¨åŠŸèƒ½...")

    try:
        from core.llm import get_chat_manager, ChatMessage
        from core.exceptions import SessionNotFoundError

        # Create manager (with persistence disabled for testing)
        manager = get_chat_manager(persist_sessions=False)
        print("   âœ… ChatManager å¯¦ä¾‹å‰µå»ºæˆåŠŸ")

        # Test session creation
        session_id = manager.create_session(
            system_prompt="You are a helpful assistant", metadata={"test": True}
        )
        print(f"   âœ… æœƒè©±å‰µå»ºæˆåŠŸ: {session_id}")

        # Test message addition
        manager.add_message(session_id, "user", "Hello, how are you?")
        manager.add_message(session_id, "assistant", "I'm doing well, thank you!")
        print("   âœ… è¨Šæ¯æ·»åŠ æˆåŠŸ")

        # Test session retrieval
        session = manager.get_session(session_id)
        print(f"   âœ… æœƒè©±æª¢ç´¢æˆåŠŸï¼Œè¨Šæ¯æ•¸é‡: {session.get_message_count()}")

        # Test invalid session handling
        try:
            manager.get_session("invalid-session-id")
            print("   âŒ æ‡‰è©²æ‹‹å‡º SessionNotFoundError")
            return False
        except SessionNotFoundError:
            print("   âœ… SessionNotFoundError æ­£ç¢ºæ‹‹å‡º")

        # Test session listing
        sessions = manager.list_sessions()
        print(f"   âœ… æœƒè©±åˆ—è¡¨: {len(sessions)} å€‹æœƒè©±")

        return True

    except Exception as e:
        print(f"   âŒ å°è©±ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_context_manager():
    """æ¸¬è©¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨åŠŸèƒ½"""
    print("\nğŸ“ æ¸¬è©¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨åŠŸèƒ½...")

    try:
        from core.llm import get_context_manager, ChatMessage

        # Create manager
        manager = get_context_manager()
        print("   âœ… ContextManager å¯¦ä¾‹å‰µå»ºæˆåŠŸ")

        # Test context window retrieval
        context_window = manager.get_context_window("microsoft/DialoGPT-medium")
        print(f"   âœ… ä¸Šä¸‹æ–‡çª—å£: {context_window.max_context_length} tokens")

        # Test token counting
        test_text = "Hello world, this is a test message for token counting."
        token_count = manager.count_tokens(test_text, "microsoft/DialoGPT-medium")
        print(f"   âœ… Token è¨ˆæ•¸: {token_count} tokens")

        # Test message preparation
        messages = [
            ChatMessage(role="system", content="You are a helpful assistant."),
            ChatMessage(role="user", content="Hello!"),
            ChatMessage(role="assistant", content="Hi there!"),
            ChatMessage(role="user", content="How are you doing today?"),
        ]

        prepared_messages, token_usage = manager.prepare_context(
            messages, "microsoft/DialoGPT-medium", max_new_tokens=100
        )

        print(
            f"   âœ… ä¸Šä¸‹æ–‡æº–å‚™: {len(prepared_messages)} è¨Šæ¯, {token_usage.prompt_tokens} tokens"
        )

        # Test context stats
        stats = manager.get_context_stats(messages, "microsoft/DialoGPT-medium")
        print(f"   âœ… ä¸Šä¸‹æ–‡çµ±è¨ˆ: {stats['utilization']:.2%} ä½¿ç”¨ç‡")

        return True

    except Exception as e:
        print(f"   âŒ ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_llm_adapter():
    """æ¸¬è©¦ LLM é©é…å™¨åŠŸèƒ½"""
    print("\nğŸ¯ æ¸¬è©¦ LLM é©é…å™¨åŠŸèƒ½...")

    try:
        from core.llm import get_llm_adapter, ChatMessage

        # Test mock adapter (always works)
        mock_adapter = get_llm_adapter(model_name="mock", use_mock=True)
        print("   âœ… Mock LLM é©é…å™¨å‰µå»ºæˆåŠŸ")

        # Test mock generation
        mock_response = mock_adapter.generate("Hello, world!")
        print(f"   âœ… Mock ç”Ÿæˆ: {mock_response[:50]}...")

        # Test mock chat
        messages = [ChatMessage(role="user", content="Hello!")]
        chat_response = mock_adapter.chat(messages)
        print(f"   âœ… Mock å°è©±: {chat_response.content[:50]}...")
        print(f"   âœ… Token ä½¿ç”¨: {chat_response.usage}")

        # Test model info
        model_info = mock_adapter.get_model_info()
        print(f"   âœ… æ¨¡å‹è³‡è¨Š: {model_info}")

        return True

    except Exception as e:
        print(f"   âŒ LLM é©é…å™¨æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_integration():
    """æ¸¬è©¦æ•´åˆåŠŸèƒ½"""
    print("\nğŸ”— æ¸¬è©¦æ•´åˆåŠŸèƒ½...")

    try:
        from core.llm import (
            get_llm_adapter,
            get_chat_manager,
            get_context_manager,
            ChatMessage,
        )

        # Create components
        adapter = get_llm_adapter(model_name="mock", use_mock=True)
        chat_manager = get_chat_manager(persist_sessions=False)
        context_manager = get_context_manager()

        # Create a conversation
        session_id = chat_manager.create_session(
            system_prompt="You are a helpful AI assistant."
        )

        # Add some conversation history
        chat_manager.add_message(session_id, "user", "What is machine learning?")
        chat_manager.add_message(
            session_id, "assistant", "Machine learning is a subset of AI..."
        )
        chat_manager.add_message(session_id, "user", "Can you give me an example?")

        # Get session messages
        session = chat_manager.get_session(session_id)
        messages = session.get_messages()

        # Prepare context
        prepared_messages, token_usage = context_manager.prepare_context(
            messages, adapter.model_name, max_new_tokens=150
        )

        # Generate response
        response = adapter.chat(prepared_messages, max_length=150)

        # Add response to session
        chat_manager.add_message(session_id, "assistant", response.content)

        print("   âœ… å®Œæ•´å°è©±æµç¨‹æ¸¬è©¦æˆåŠŸ")
        print(f"   âœ… æœ€çµ‚å›æ‡‰: {response.content[:100]}...")
        print(f"   âœ… Token ä½¿ç”¨: {response.usage}")

        return True

    except Exception as e:
        print(f"   âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹ LLM æ¨¡çµ„ä¿®å¾©èˆ‡æ¸¬è©¦\n")

    tests = [
        ("ç’°å¢ƒè¨­ç½®", setup_test_environment),
        ("æ ¸å¿ƒæ¨¡çµ„å°å…¥", test_core_imports),
        ("LLM æ¨¡çµ„å°å…¥", test_llm_imports),
        ("æ¨¡å‹è¼‰å…¥å™¨", test_model_loader),
        ("å°è©±ç®¡ç†å™¨", test_chat_manager),
        ("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", test_context_manager),
        ("LLM é©é…å™¨", test_llm_adapter),
        ("æ•´åˆæ¸¬è©¦", test_integration),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ {test_name} ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            results.append((test_name, False))

    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦:")
    print("=" * 60)

    passed = 0
    failed = 0

    for test_name, result in results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name:20} {status}")

        if result:
            passed += 1
        else:
            failed += 1

    print("=" * 60)
    print(f"ç¸½è¨ˆ: {passed} é€šé, {failed} å¤±æ•—")

    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼LLM æ¨¡çµ„ä¿®å¾©æˆåŠŸï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥ä¿®å¾©")
        return False


def cleanup_and_exit(success: bool):
    """æ¸…ç†ä¸¦é€€å‡º"""
    print("\nğŸ§¹ æ¸…ç†æ¸¬è©¦ç’°å¢ƒ...")

    try:
        # Reset global instances
        from core.llm import shutdown_llm_modules

        shutdown_llm_modules()
        print("   âœ… LLM æ¨¡çµ„æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"   âš ï¸ æ¸…ç†éç¨‹ä¸­å‡ºç¾è­¦å‘Š: {e}")

    print("\nâœ¨ æ¸¬è©¦å®Œæˆ")
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    try:
        success = run_all_tests()
        cleanup_and_exit(success)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        cleanup_and_exit(False)
    except Exception as e:
        print(f"\n\nğŸ’¥ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
        cleanup_and_exit(False)
