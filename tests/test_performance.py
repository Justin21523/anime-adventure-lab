# tests/test_performance.py
"""
æ€§èƒ½å’Œè² è¼‰æ¸¬è©¦
"""

import pytest
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import psutil
import gc


class TestPerformance:
    """æ€§èƒ½æ¸¬è©¦"""

    @pytest.mark.slow
    def test_concurrent_requests(self, client):
        """æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚è™•ç†"""

        def make_request():
            response = client.get("/api/v1/health")
            return response.status_code == 200

        # ä¸¦ç™¼åŸ·è¡Œ50å€‹è«‹æ±‚
        with ThreadPoolExecutor(max_workers=10) as executor:
            start_time = time.time()
            futures = [executor.submit(make_request) for _ in range(50)]
            results = [f.result() for f in futures]
            end_time = time.time()

        # é©—è­‰çµæœ
        assert all(results), "Some requests failed"
        duration = end_time - start_time
        assert duration < 30, f"Requests took too long: {duration:.2f}s"

        print(f"âœ… 50 concurrent requests completed in {duration:.2f}s")

    @pytest.mark.slow
    def test_memory_usage(self, client, mock_models, sample_documents):
        """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨"""
        import psutil
        import gc

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # åŸ·è¡Œå¤šå€‹æ“ä½œ
        for i in range(10):
            # RAGæ“ä½œ
            payload = {
                "content": sample_documents[0]["content"] * 10,  # æ“´å¤§å…§å®¹
                "metadata": sample_documents[0]["metadata"],
            }
            response = client.post("/api/v1/rag/add", json=payload)
            assert response.status_code == 200

            # æœå°‹æ“ä½œ
            search_payload = {"query": "æ¸¬è©¦æŸ¥è©¢", "top_k": 5}
            response = client.post("/api/v1/rag/search", json=search_payload)
            assert response.status_code == 200

        # æª¢æŸ¥è¨˜æ†¶é«”å¢é•·
        gc.collect()  # å¼·åˆ¶åƒåœ¾å›æ”¶
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_growth = final_memory - initial_memory

        print(
            f"ğŸ“Š Memory usage: {initial_memory:.1f}MB -> {final_memory:.1f}MB (+{memory_growth:.1f}MB)"
        )

        # è¨˜æ†¶é«”å¢é•·ä¸æ‡‰éå¤§ï¼ˆæ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´é–¾å€¼ï¼‰
        assert memory_growth < 100, f"Memory growth too high: {memory_growth:.1f}MB"

    def test_response_times(self, client, mock_models, sample_image_data):
        """æ¸¬è©¦å„ç«¯é»éŸ¿æ‡‰æ™‚é–“"""
        endpoints = [
            ("GET", "/api/v1/health", None, 1.0),  # å¥åº·æª¢æŸ¥æ‡‰è©²å¾ˆå¿«
            (
                "POST",
                "/api/v1/chat",
                {"message": "æ¸¬è©¦", "conversation_id": "perf-test"},
                3.0,
            ),
            ("POST", "/api/v1/rag/search", {"query": "æ¸¬è©¦", "top_k": 3}, 2.0),
        ]

        results = {}

        for method, endpoint, payload, threshold in endpoints:
            start_time = time.time()

            if method == "GET":
                response = client.get(endpoint)
            elif method == "POST":
                response = client.post(endpoint, json=payload)

            duration = time.time() - start_time
            results[endpoint] = duration

            assert response.status_code == 200, f"Endpoint {endpoint} failed"
            assert (
                duration < threshold
            ), f"Endpoint {endpoint} too slow: {duration:.2f}s > {threshold}s"

            print(f"âš¡ {endpoint}: {duration:.3f}s (< {threshold}s)")

        return results


class TestStressTest:
    """å£“åŠ›æ¸¬è©¦"""

    @pytest.mark.slow
    def test_rag_bulk_operations(self, client, sample_documents):
        """RAGæ‰¹é‡æ“ä½œå£“åŠ›æ¸¬è©¦"""

        # æ‰¹é‡æ·»åŠ æ–‡æª”
        doc_ids = []
        start_time = time.time()

        for i in range(100):  # æ·»åŠ 100å€‹æ–‡æª”
            payload = {
                "content": f"{sample_documents[i % len(sample_documents)]['content']} - æ–‡æª”{i}",
                "metadata": {"title": f"æ¸¬è©¦æ–‡æª”{i}", "batch_id": "stress_test"},
            }
            response = client.post("/api/v1/rag/add", json=payload)
            assert response.status_code == 200
            doc_ids.append(response.json()["document_id"])

        add_duration = time.time() - start_time
        print(f"ğŸ“ Added 100 documents in {add_duration:.2f}s")

        # æ‰¹é‡æœå°‹
        queries = ["äººå·¥æ™ºæ…§", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "é›»è…¦è¦–è¦º", "è‡ªç„¶èªè¨€"]
        search_start = time.time()

        for query in queries * 20:  # æ¯å€‹æŸ¥è©¢åŸ·è¡Œ20æ¬¡
            search_payload = {"query": query, "top_k": 5}
            response = client.post("/api/v1/rag/search", json=search_payload)
            assert response.status_code == 200

        search_duration = time.time() - search_start
        print(f"ğŸ” Completed 100 searches in {search_duration:.2f}s")

        # æ€§èƒ½æ–·è¨€
        assert add_duration < 60, f"Document addition too slow: {add_duration:.2f}s"
        assert (
            search_duration < 30
        ), f"Search operations too slow: {search_duration:.2f}s"
