# SagaForge éƒ¨ç½²æŒ‡å—

## å¿«é€Ÿé–‹å§‹ (Quick Start)

### 1. ç³»çµ±éœ€æ±‚

**æœ€ä½é…ç½®**
- CPU: 4 cores, 8GB RAM
- GPU: NVIDIA GTX 1060 6GB+ (æˆ– CPU-only æ¨¡å¼)
- ç¡¬ç¢Ÿ: 50GB å¯ç”¨ç©ºé–“
- ä½œæ¥­ç³»çµ±: Ubuntu 20.04+, Windows 10+, macOS 12+

**æ¨è–¦é…ç½®**
- CPU: 8+ cores, 16GB+ RAM
- GPU: NVIDIA RTX 3070 8GB+ æˆ– RTX 4060 Ti 16GB+
- ç¡¬ç¢Ÿ: 100GB+ SSD
- Docker èˆ‡ Docker Compose å·²å®‰è£

### 2. ç’°å¢ƒæº–å‚™

```bash
# 1. å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/your-org/saga-forge.git
cd saga-forge

# 2. å»ºç«‹å…±ç”¨æ¨¡å‹å€‰å„²
mkdir -p ../ai_warehouse/cache
export AI_CACHE_ROOT="$(pwd)/../ai_warehouse/cache"

# 3. è¤‡è£½ç’°å¢ƒé…ç½®
cp .env.example .env
# ç·¨è¼¯ .env æª”æ¡ˆï¼Œè¨­å®šä½ çš„é…ç½®
```

**é—œéµç’°å¢ƒè®Šæ•¸ (.env)**
```bash
# å¿…å¡«
AI_CACHE_ROOT=/path/to/ai_warehouse/cache
POSTGRES_PASSWORD=your_secure_password
MINIO_PASSWORD=your_minio_password

# å¯é¸
CUDA_VISIBLE_DEVICES=0
API_CORS_ORIGINS=http://localhost:7860
HUGGINGFACE_TOKEN=your_hf_token  # ç”¨æ–¼ç§æœ‰æ¨¡å‹
```

### 3. ä¸€éµéƒ¨ç½²

**ç”Ÿç”¢ç’°å¢ƒ (æ¨è–¦)**
```bash
# ä½¿ç”¨ Docker Compose éƒ¨ç½²æ‰€æœ‰æœå‹™
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æ‰€æœ‰æœå‹™å•Ÿå‹• (ç´„ 2-5 åˆ†é˜)
docker-compose -f docker-compose.prod.yml logs -f

# æª¢æŸ¥å¥åº·ç‹€æ…‹
curl http://localhost:8000/monitoring/health
```

**é–‹ç™¼ç’°å¢ƒ**
```bash
# å®‰è£ Python ä¾è³´
conda create -n adventure-lab python=3.10 -y
conda activate adventure-lab
pip install -r requirements.txt

# å•Ÿå‹•åŸºç¤æœå‹™
docker-compose up postgres redis -d

# å•Ÿå‹• API ä¼ºæœå™¨
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload

# å¦é–‹çµ‚ç«¯å•Ÿå‹• worker
celery -A workers.celery_app worker --loglevel=INFO

# å•Ÿå‹• WebUI
python frontend/gradio/app.py
```

### 4. é©—è­‰éƒ¨ç½²

```bash
# API å¥åº·æª¢æŸ¥
curl http://localhost:8000/healthz

# Web ä»‹é¢
open http://localhost:7860

# ç›£æ§é¢æ¿
open http://localhost:5555  # Celery Flower

# ç³»çµ±æŒ‡æ¨™
curl http://localhost:8000/monitoring/metrics
```

---

## å®Œæ•´é…ç½®æŒ‡å—

### è³‡æ–™åº«è¨­å®š

**PostgreSQL with pgvector**
```sql
-- åˆå§‹åŒ–è…³æœ¬ (scripts/init_db.sql)
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;  -- ç”¨æ–¼å…¨æ–‡æœå°‹

-- å»ºç«‹å‘é‡ç´¢å¼•
CREATE INDEX CONCURRENTLY embeddings_vector_idx
ON vectors USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**Redis é…ç½®å„ªåŒ–**
```redis
# redis.conf å»ºè­°è¨­å®š
maxmemory 4gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

### GPU èˆ‡è¨˜æ†¶é«”å„ªåŒ–

**NVIDIA Docker è¨­å®š**
```bash
# å®‰è£ NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

**ä½ VRAM å„ªåŒ– (configs/app.yaml)**
```yaml
performance:
  low_vram_mode: true
  enable_xformers: true
  enable_attention_slicing: true
  enable_cpu_offload: true
  max_batch_size: 1

models:
  default_precision: "fp16"  # æˆ– "int8" ç”¨æ–¼ 8-bit æ¨ç†
  use_safetensors: true
  cache_models: true
```

### å®‰å…¨å¼·åŒ–

**API å®‰å…¨**
```python
# åŠ å…¥èªè­‰ä¸­ä»‹è»Ÿé«”
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    # å¯¦ä½œ JWT é©—è­‰é‚è¼¯
    if not verify_jwt(token.credentials):
        raise HTTPException(401, "Invalid token")
    return token

# ä¿è­·æ•æ„Ÿç«¯é»
@router.post("/admin/models")
async def admin_endpoint(token: str = Depends(verify_token)):
    pass
```

**è³‡æ–™åŠ å¯†**
```yaml
# åŠ å¯†æ•æ„Ÿè³‡æ–™
postgres:
  environment:
    POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
  volumes:
    - postgres_data:/var/lib/postgresql/data:Z  # SELinux æ¨™ç±¤
```

**ç¶²è·¯éš”é›¢**
```yaml
# å…§éƒ¨ç¶²è·¯é…ç½®
networks:
  backend:
    internal: true  # åƒ…é™å…§éƒ¨é€šè¨Š
  frontend:
    # å°å¤–ç¶²è·¯
services:
  postgres:
    networks: [backend]
  api:
    networks: [backend, frontend]
```

---

## ç¶­é‹æ‰‹å†Š

### æ—¥å¸¸ç¶­è­·

**æ¯æ—¥æª¢æŸ¥æ¸…å–®**
- [ ] æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹: `curl localhost:8000/monitoring/health`
- [ ] æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ: `docker-compose logs --tail=100 api worker`
- [ ] ç›£æ§è³‡æºä½¿ç”¨: `curl localhost:8000/monitoring/metrics`
- [ ] æª¢æŸ¥ä»»å‹™éšŠåˆ—: `celery -A workers.celery_app inspect active`

**æ¯é€±ç¶­è­·**
- [ ] æ¸…ç†èˆŠæ—¥èªŒæª”æ¡ˆ
- [ ] æª¢æŸ¥ç£ç¢Ÿç©ºé–“ä½¿ç”¨ç‡
- [ ] æ›´æ–°ç³»çµ±å¥—ä»¶: `apt update && apt upgrade`
- [ ] å‚™ä»½è³‡æ–™åº«å’Œé‡è¦æª”æ¡ˆ

**æ¯æœˆç¶­è­·**
- [ ] æ›´æ–° Docker æ˜ åƒ: `docker-compose pull && docker-compose up -d`
- [ ] æª¢æŸ¥ä¸¦æ›´æ–° AI æ¨¡å‹ç‰ˆæœ¬
- [ ] æ•ˆèƒ½åŸºæº–æ¸¬è©¦å’Œèª¿å„ª
- [ ] å®‰å…¨æ€§æƒæå’Œæ›´æ–°

### ç·Šæ€¥æ‡‰è®Š

**æœå‹™åœæ©Ÿè™•ç†**
```bash
# 1. å¿«é€Ÿè¨ºæ–·
docker-compose ps
docker-compose logs api worker

# 2. é‡å•Ÿæœå‹™
docker-compose restart api worker

# 3. å¦‚æœå•é¡ŒæŒçºŒï¼Œå›æ»¾åˆ°ç©©å®šç‰ˆæœ¬
git checkout v1.0.0
docker-compose down && docker-compose up -d
```

**è³‡æ–™å¾©åŸ**
```bash
# å¾å‚™ä»½æ¢å¾©è³‡æ–™åº«
gunzip < backup_20241201.sql.gz | docker-compose exec -T postgres psql -U saga sagaforge

# æ¢å¾©æ¨¡å‹æª”æ¡ˆ
rsync -av /backup/ai_warehouse/ /path/to/ai_warehouse/
```

---

## API æ–‡æª”

### æ ¸å¿ƒç«¯é»

**å¥åº·æª¢æŸ¥**
```http
GET /healthz
Response: {"status": "healthy", "version": "1.0.0", "uptime": 3600}
```

**LLM å°è©±**
```http
POST /llm/turn
Content-Type: application/json

{
    "message": "å‘Šè¨´æˆ‘é—œæ–¼ Alice çš„æ•…äº‹",
    "world_id": "neo_taipei",
    "character_id": "alice",
    "use_rag": true,
    "max_tokens": 500,
    "temperature": 0.7
}

Response:
{
    "narration": "åœ¨éœ“è™¹ç‡ˆé–ƒçˆçš„æ–°å°åŒ—è¡—é ­ï¼ŒAlice Chen...",
    "choices": [
        {"id": "choice_1", "text": "ç¹¼çºŒæ¢ç´¢åŸå¸‚"},
        {"id": "choice_2", "text": "é€²å…¥å’–å•¡å»³"}
    ],
    "citations": ["doc_123@section_1:chunk_2"],
    "game_state": {...}
}
```

**åœ–åƒç”Ÿæˆ**
```http
POST /t2i/generate
Content-Type: application/json

{
    "prompt": "anime girl, cyberpunk city, neon lights",
    "negative_prompt": "blurry, low quality",
    "width": 1024,
    "height": 1024,
    "steps": 30,
    "cfg_scale": 7.5,
    "seed": 42,
    "style_preset": "anime_style"
}

Response:
{
    "image_path": "/warehouse/outputs/20241201/img_001.png",
    "metadata": {
        "prompt": "...",
        "seed": 42,
        "model": "sdxl-base-1.0",
        "generation_time": 15.2
    }
}
```

**RAG æª¢ç´¢**
```http
POST /rag/retrieve
Content-Type: application/json

{
    "world_id": "neo_taipei",
    "query": "Alice çš„èƒŒæ™¯æ•…äº‹",
    "top_k": 5,
    "rerank": true
}

Response:
{
    "chunks": [
        {
            "id": "doc_123@section_1:chunk_2",
            "text": "Alice Chen æ˜¯ä¸€ä½å¹´è¼•çš„ç¨‹å¼è¨­è¨ˆå¸«...",
            "score": 0.89,
            "metadata": {"source": "characters.yaml"}
        }
    ],
    "query_time_ms": 45
}
```

### æ‰¹æ¬¡ä½œæ¥­

**æäº¤æ‰¹æ¬¡ä»»å‹™**
```http
POST /batch/submit
Content-Type: application/json

{
    "world_id": "neo_taipei",
    "job_type": "image_generation",
    "config": {
        "prompts": ["prompt1", "prompt2", "prompt3"],
        "style_preset": "anime_style",
        "generation_params": {
            "width": 512,
            "height": 512,
            "steps": 25
        }
    }
}

Response:
{
    "batch_id": "batch_123456",
    "status": "submitted",
    "total_tasks": 3,
    "estimated_time": 180
}
```

**æŸ¥è©¢æ‰¹æ¬¡ç‹€æ…‹**
```http
GET /batch/{batch_id}/status

Response:
{
    "batch_id": "batch_123456",
    "status": "running",  // pending, running, completed, failed, cancelled
    "progress": {
        "total": 3,
        "completed": 1,
        "failed": 0,
        "remaining": 2
    },
    "results": [
        {
            "task_id": "task_001",
            "status": "completed",
            "output_path": "/warehouse/outputs/batch_123456/img_001.png"
        }
    ]
}
```

### LoRA è¨“ç·´

**æäº¤è¨“ç·´ä»»å‹™**
```http
POST /finetune/lora
Content-Type: application/json

{
    "base_model": "stabilityai/stable-diffusion-xl-base-1.0",
    "dataset_config": {
        "character_name": "alice",
        "data_path": "/warehouse/datasets/alice_images",
        "caption_file": "captions.txt"
    },
    "training_config": {
        "rank": 16,
        "learning_rate": 1e-4,
        "max_steps": 2000,
        "batch_size": 1,
        "resolution": 1024
    },
    "output_name": "alice_v1"
}

Response:
{
    "job_id": "lora_train_789",
    "status": "submitted",
    "estimated_time": 7200,
    "config_path": "/warehouse/configs/lora_train_789.yaml"
}
```

---

## ç¯„ä¾‹èˆ‡æ•™å­¸

### åŸºæœ¬ä½¿ç”¨æµç¨‹

**1. ä¸Šå‚³ä¸–ç•Œè§€è³‡æ–™**
```python
import requests

# ä¸Šå‚³ worldpack
with open('my_world.zip', 'rb') as f:
    files = {'file': f}
    data = {'world_id': 'my_world', 'license': 'CC-BY-SA-4.0'}
    response = requests.post('http://localhost:8000/rag/upload',
                           files=files, data=data)
print(f"ä¸Šå‚³çµæœ: {response.json()}")
```

**2. é–‹å§‹å°è©±**
```python
# èˆ‡ AI è§’è‰²å°è©±
conversation = {
    "message": "ä½ å¥½ï¼Œæˆ‘æƒ³æ¢ç´¢é€™å€‹ä¸–ç•Œ",
    "world_id": "my_world",
    "character_id": "protagonist",
    "use_rag": True
}

response = requests.post('http://localhost:8000/llm/turn', json=conversation)
result = response.json()
print(f"æ—ç™½: {result['narration']}")
print(f"é¸æ“‡: {result['choices']}")
```

**3. ç”Ÿæˆå ´æ™¯åœ–ç‰‡**
```python
# æ ¹æ“šæ•…äº‹æƒ…ç¯€ç”Ÿæˆåœ–ç‰‡
image_request = {
    "prompt": "medieval castle, sunset, fantasy landscape",
    "width": 1024,
    "height": 768,
    "steps": 30,
    "style_preset": "fantasy_art"
}

response = requests.post('http://localhost:8000/t2i/generate', json=image_request)
result = response.json()
print(f"åœ–ç‰‡å·²ç”Ÿæˆ: {result['image_path']}")
```

### é€²éšè‡ªå‹•åŒ–

**æ‰¹æ¬¡ç”Ÿæˆè§’è‰²ç«‹ç¹ª**
```python
import json

# æº–å‚™è§’è‰²è³‡æ–™
characters = [
    {"name": "Alice", "desc": "short black hair, blue eyes, casual clothes"},
    {"name": "Bob", "desc": "tall, brown hair, formal suit"},
    {"name": "Carol", "desc": "long red hair, green dress, friendly smile"}
]

# æ‰¹æ¬¡ç”Ÿæˆ
prompts = []
for char in characters:
    prompt = f"anime style portrait, {char['desc']}, high quality"
    prompts.append(prompt)

batch_job = {
    "world_id": "my_world",
    "job_type": "image_generation",
    "config": {
        "prompts": prompts,
        "style_preset": "anime_portrait",
        "generation_params": {
            "width": 512,
            "height": 768,
            "steps": 25,
            "cfg_scale": 7.0
        }
    }
}

response = requests.post('http://localhost:8000/batch/submit', json=batch_job)
batch_id = response.json()['batch_id']
print(f"æ‰¹æ¬¡ä»»å‹™ ID: {batch_id}")

# ç›£æ§é€²åº¦
import time
while True:
    status_response = requests.get(f'http://localhost:8000/batch/{batch_id}/status')
    status = status_response.json()

    print(f"é€²åº¦: {status['progress']['completed']}/{status['progress']['total']}")

    if status['status'] in ['completed', 'failed']:
        break
    time.sleep(10)
```

### Webhook æ•´åˆ

**è¨­å®šå®Œæˆé€šçŸ¥**
```python
# åœ¨æ‰¹æ¬¡ä»»å‹™å®Œæˆæ™‚ç™¼é€é€šçŸ¥
webhook_config = {
    "webhook_url": "https://your-app.com/api/batch_complete",
    "events": ["batch_completed", "batch_failed"],
    "secret": "your_webhook_secret"
}

# ç³»çµ±æœƒåœ¨ä»»å‹™å®Œæˆæ™‚ç™¼é€ POST è«‹æ±‚åˆ°ä½ çš„ webhook
# POST https://your-app.com/api/batch_complete
# {
#   "event": "batch_completed",
#   "batch_id": "batch_123456",
#   "timestamp": "2024-12-01T10:30:00Z",
#   "results": [...]
# }
```

---

## ç¤¾ç¾¤èˆ‡æ”¯æ´

### å•é¡Œå›å ±

**Bug å›å ±æ¨¡æ¿**
```markdown
**ç’°å¢ƒè³‡è¨Š:**
- OS: Ubuntu 22.04
- Docker ç‰ˆæœ¬: 24.0.5
- GPU: NVIDIA RTX 4070
- SagaForge ç‰ˆæœ¬: v1.0.0

**é‡ç¾æ­¥é©Ÿ:**
1. åŸ·è¡Œ `docker-compose up -d`
2. è¨ªå• http://localhost:8000/healthz
3. éŒ¯èª¤ç™¼ç”Ÿ...

**é æœŸè¡Œç‚º:**
æ‡‰è©²è¿”å› 200 OK

**å¯¦éš›è¡Œç‚º:**
è¿”å› 500 éŒ¯èª¤

**æ—¥èªŒ:**
```bash
docker-compose logs api
```

**åŠŸèƒ½è«‹æ±‚æ¨¡æ¿**
```markdown
**åŠŸèƒ½æè¿°:**
å¸Œæœ›æ·»åŠ æ”¯æ´ Stable Video Diffusion çš„å½±ç‰‡ç”ŸæˆåŠŸèƒ½

**ä½¿ç”¨å ´æ™¯:**
ç‚ºäº’å‹•æ•…äº‹ç”ŸæˆçŸ­å½±ç‰‡ç‰‡æ®µ

**å¯èƒ½çš„å¯¦ä½œæ–¹å¼:**
- æ–°å¢ `/t2v/generate` ç«¯é»
- æ•´åˆ SVD æ¨¡å‹ç®¡é“
- æ”¯æ´é—œéµå¹€æ§åˆ¶
```

### è²¢ç»æŒ‡å—

**é–‹ç™¼æµç¨‹**
1. Fork å°ˆæ¡ˆä¸¦å»ºç«‹åŠŸèƒ½åˆ†æ”¯
2. éµå¾ªç¨‹å¼ç¢¼é¢¨æ ¼æŒ‡å— (Black + Ruff)
3. æ·»åŠ å°æ‡‰çš„æ¸¬è©¦æ¡ˆä¾‹
4. æ›´æ–°ç›¸é—œæ–‡æª”
5. æäº¤ Pull Request

**ç¨‹å¼ç¢¼å¯©æŸ¥æ¨™æº–**
- [ ] åŠŸèƒ½æ­£ç¢ºä¸”å®Œæ•´
- [ ] æ¸¬è©¦è¦†è“‹ç‡ > 80%
- [ ] æ–‡æª”å®Œæ•´ä¸”æ¸…æ™°
- [ ] éµå¾ªå°ˆæ¡ˆæ¶æ§‹åŸå‰‡
- [ ] ç„¡å®‰å…¨æ¼æ´

### ç‰ˆæœ¬ç™¼å¸ƒ

**ç™¼å¸ƒé€±æœŸ**
- **è£œä¸ç‰ˆæœ¬** (v1.0.x): æ¯æœˆï¼Œä¿®å¾© bug å’Œå°æ”¹é€²
- **æ¬¡è¦ç‰ˆæœ¬** (v1.x.0): æ¯å­£ï¼Œæ–°åŠŸèƒ½å’Œ API æ“´å……
- **ä¸»è¦ç‰ˆæœ¬** (vX.0.0): æ¯å¹´ï¼Œé‡å¤§æ¶æ§‹è®Šæ›´

**å‡ç´šæŒ‡å—**
```bash
# å‚™ä»½ç•¶å‰ç‰ˆæœ¬
docker-compose down
cp -r ai_warehouse ai_warehouse_backup

# æ‹‰å–æ–°ç‰ˆæœ¬
git pull origin main
docker-compose pull

# åŸ·è¡Œè³‡æ–™åº«é·ç§» (å¦‚éœ€è¦)
python scripts/migrate_database.py

# é‡æ–°å•Ÿå‹•æœå‹™
docker-compose up -d

# é©—è­‰å‡ç´š
curl http://localhost:8000/healthz
```

**ç›¸å®¹æ€§æ‰¿è«¾**
- API ç«¯é»åœ¨ä¸»è¦ç‰ˆæœ¬å…§ä¿æŒå‘å¾Œç›¸å®¹
- è³‡æ–™åº«é·ç§»è…³æœ¬ç¢ºä¿è³‡æ–™å®‰å…¨å‡ç´š
- é…ç½®æª”æ¡ˆæ ¼å¼è®Šæ›´æœƒæä¾›è½‰æ›å·¥å…·

---

## é™„éŒ„

### é è¨­é…ç½®æª”æ¡ˆ

**configs/app.yaml (å®Œæ•´ç¯„ä¾‹)**
```yaml
# SagaForge ä¸»è¦é…ç½®æª”æ¡ˆ
app:
  name: "SagaForge"
  version: "1.0.0"
  debug: false
  cors_origins:
    - "http://localhost:7860"
    - "http://localhost:3000"

models:
  llm:
    default: "Qwen/Qwen2.5-7B-Instruct"
    alternatives: ["microsoft/DialoGPT-medium"]
    max_tokens: 2048
    temperature: 0.7

  embedding:
    default: "BAAI/bge-m3"
    dimension: 1024
    normalize: true

  t2i:
    default: "stabilityai/stable-diffusion-xl-base-1.0"
    scheduler: "DPMSolverMultistepScheduler"
    safety_checker: true

  vlm:
    default: "Salesforce/blip2-opt-2.7b"
    max_image_size: 1024

performance:
  low_vram_mode: false
  enable_xformers: true
  enable_attention_slicing: false
  max_batch_size: 4
  cache_models: true

  redis:
    max_connections: 20
    ttl_default: 3600

  celery:
    worker_concurrency: 2
    task_timeout: 1800

security:
  enable_auth: false
  api_rate_limit: "100/minute"
  upload_max_size: "100MB"
  allowed_file_types: [".zip", ".txt", ".md", ".pdf", ".png", ".jpg"]

  content_filter:
    enable_nsfw_filter: true
    enable_face_detection: true
    watermark_generated_images: true

storage:
  backend: "filesystem"  # æˆ– "minio"
  retention_days: 90
  cleanup_schedule: "0 2 * * *"  # æ¯æ—¥å‡Œæ™¨ 2 é»æ¸…ç†
```

### å¸¸ç”¨å‘½ä»¤åƒè€ƒ

```bash
# === Docker ç›¸é—œ ===
# æŸ¥çœ‹æœå‹™ç‹€æ…‹
docker-compose ps

# æŸ¥çœ‹å³æ™‚æ—¥èªŒ
docker-compose logs -f api worker

# é‡å•Ÿå–®ä¸€æœå‹™
docker-compose restart api

# æ¸…ç†æœªä½¿ç”¨çš„æ˜ åƒ
docker system prune -f

# === è³‡æ–™åº«æ“ä½œ ===
# é€²å…¥ PostgreSQL æ§åˆ¶å°
docker-compose exec postgres psql -U saga sagaforge

# å‚™ä»½è³‡æ–™åº«
docker-compose exec postgres pg_dump -U saga sagaforge > backup.sql

# æ¢å¾©è³‡æ–™åº«
cat backup.sql | docker-compose exec -T postgres psql -U saga sagaforge

# === Celery ä»»å‹™ç®¡ç† ===
# æŸ¥çœ‹æ´»èºä»»å‹™
celery -A workers.celery_app inspect active

# æ¸…ç©ºä»»å‹™éšŠåˆ—
celery -A workers.celery_app purge

# ç›£æ§ä»»å‹™åŸ·è¡Œ
celery -A workers.celery_app events

# === æ¨¡å‹ç®¡ç† ===
# ä¸‹è¼‰é è¨­æ¨¡å‹
python scripts/download_models.py --all

# æ¸…ç†æ¨¡å‹å¿«å–
python scripts/clean_cache.py --models

# æª¢æŸ¥æ¨¡å‹å®Œæ•´æ€§
python scripts/verify_models.py

# === æ—¥èªŒèˆ‡ç›£æ§ ===
# æŸ¥çœ‹ API éŒ¯èª¤æ—¥èªŒ
grep "ERROR" logs/api.log | tail -20

# ç›£æ§ GPU ä½¿ç”¨ç‡
watch -n 1 nvidia-smi

# æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨ç‡
df -h /path/to/ai_warehouse
```

---

**ğŸ¯ æ­å–œï¼** ä½ ç¾åœ¨å·²ç¶“æ“æœ‰ä¸€å€‹å®Œæ•´é‹è¡Œçš„ SagaForge ç³»çµ±ã€‚é–‹å§‹å‰µå»ºä½ çš„äº’å‹•æ•…äº‹ä¸–ç•Œå§ï¼

å¦‚éœ€æ›´å¤šå¹«åŠ©ï¼Œè«‹æŸ¥çœ‹ï¼š
- ğŸ“– [API æ–‡æª”](http://localhost:8000/docs)
- ğŸ’¬ [ç¤¾ç¾¤è¨è«–](https://github.com/your-org/saga-forge/discussions)
- ğŸ› [å•é¡Œå›å ±](https://github.com/your-org/saga-forge/issues)
- ğŸ“º [å½±ç‰‡æ•™å­¸](https://youtube.com/playlist?list=your-tutorials)èˆ‡ç¶²è·¯é…ç½®

**Nginx åå‘ä»£ç†**
```nginx
# nginx.conf
upstream saga_api {
    server api:8000;
}

upstream saga_webui {
    server webui:7860;
}

server {
    listen 80;
    server_name your-domain.com;

    # API è·¯ç”±
    location /api/ {
        proxy_pass http://saga_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # WebUI è·¯ç”±
    location / {
        proxy_pass http://saga_webui;
        proxy_set_header Host $host;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

**SSL/TLS è¨­å®š**
```bash
# ä½¿ç”¨ Let's Encrypt
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d your-domain.com
```

---

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

**1. GPU è¨˜æ†¶é«”ä¸è¶³ (CUDA OOM)**
```bash
# æª¢æŸ¥ GPU ä½¿ç”¨ç‹€æ³
nvidia-smi

# è§£æ±ºæ–¹æ¡ˆ
# 1. é™ä½æ‰¹æ¬¡å¤§å°
# 2. å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–é¸é …
# 3. ä½¿ç”¨ CPU fallback
export CUDA_VISIBLE_DEVICES=""  # å¼·åˆ¶ä½¿ç”¨ CPU
```

**2. æ¨¡å‹ä¸‹è¼‰å¤±æ•—**
```bash
# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹
python scripts/download_models.py --model bge-m3 --cache /path/to/cache

# æˆ–è¨­å®š HuggingFace é¡åƒ
export HF_ENDPOINT=https://hf-mirror.com
```

**3. è³‡æ–™åº«é€£ç·šå•é¡Œ**
```bash
# æª¢æŸ¥ PostgreSQL ç‹€æ…‹
docker-compose logs postgres

# é‡ç½®è³‡æ–™åº«
docker-compose down -v
docker-compose up postgres -d
python scripts/init_database.py
```

**4. Celery ä»»å‹™å¡ä½**
```bash
# æŸ¥çœ‹ worker ç‹€æ…‹
celery -A workers.celery_app inspect active

# æ¸…ç©ºä»»å‹™éšŠåˆ—
celery -A workers.celery_app purge

# é‡å•Ÿ worker
docker-compose restart worker
```

### æ•ˆèƒ½èª¿æ ¡

**API æœ€ä½³åŒ–**
```python
# configs/app.yaml
api:
  workers: 4  # æ ¹æ“š CPU æ ¸å¿ƒæ•¸èª¿æ•´
  max_requests: 1000
  max_requests_jitter: 50
  timeout: 300

cache:
  embedding_ttl: 3600  # 1å°æ™‚
  model_cache_size: 5
  redis_max_connections: 20
```

**è³‡æ–™åº«æœ€ä½³åŒ–**
```sql
-- èª¿æ•´ PostgreSQL è¨­å®š
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
SELECT pg_reload_conf();
```

### ç›£æ§èˆ‡æ—¥èªŒ

**æ—¥èªŒæ”¶é›†**
```yaml
# docker-compose.yml å¢åŠ æ—¥èªŒé…ç½®
services:
  api:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

**Prometheus ç›£æ§ (é¸é…)**
```yaml
# docker-compose.monitoring.yml
services:
  prometheus:
    image: prom/prometheus
    ports: ["9090:9090"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports: ["3000:3000"]
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
```

---

## æ“´å±•èˆ‡å®¢è£½åŒ–

### æ–°å¢è‡ªè¨‚æ¨¡å‹

**1. è¨»å†Šæ–°çš„ LLM å¾Œç«¯**
```python
# core/llm/custom_llm.py
from core.llm.adapter import LLMAdapter

class CustomLLMAdapter(LLMAdapter):
    def __init__(self, model_id: str):
        self.model_id = model_id
        # åˆå§‹åŒ–ä½ çš„è‡ªè¨‚æ¨¡å‹

    async def generate(self, messages: List[dict], **kwargs):
        # å¯¦ä½œç”Ÿæˆé‚è¼¯
        pass
```

**2. æ–°å¢ LoRA é¢¨æ ¼é è¨­**
```yaml
# configs/presets/my_style.yaml
style_id: "my_custom_style"
base_model: "stabilityai/stable-diffusion-xl-base-1.0"
lora_path: "../ai_warehouse/models/lora/my_style"
lora_scale: 0.8
trigger_words: ["my_style", "custom_art"]
negative_prompt: "low quality, blurry"
```

### é–‹ç™¼å·¥ä½œæµç¨‹

**è¨­å®šé–‹ç™¼ç’°å¢ƒ**
```bash
# å®‰è£é–‹ç™¼ä¾è³´
pip install -r requirements-dev.txt

# è¨­å®š pre-commit
pre-commit install

# åŸ·è¡Œæ¸¬è©¦
pytest tests/ -v

# ç¨‹å¼ç¢¼æ ¼å¼åŒ–
black . && ruff --fix .
```

**æ–°åŠŸèƒ½é–‹ç™¼**
```bash
# å»ºç«‹åŠŸèƒ½åˆ†æ”¯
git checkout -b feature/my-new-feature

# æäº¤éµå¾ª Conventional Commits
git commit -m "feat(api): add new custom endpoint"

# åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
python -m pytest tests/test_e2e_complete.py::TestE2EWorkflow -v
```

---

## ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦å‹™

### å‚™ä»½ç­–ç•¥

**è³‡æ–™åº«å‚™ä»½**
```bash
# æ¯æ—¥è‡ªå‹•å‚™ä»½è…³æœ¬
#!/bin/bash
docker-compose exec postgres pg_dump -U saga sagaforge | gzip > backup_$(date +%Y%m%d).sql.gz

# ä¿ç•™æœ€è¿‘ 30 å¤©çš„å‚™ä»½
find ./backups -name "backup_*.sql.gz" -mtime +30 -delete
```

**æ¨¡å‹èˆ‡å¿«å–å‚™ä»½**
```bash
# åŒæ­¥æ¨¡å‹å€‰å„²åˆ°å‚™ä»½ä½ç½®
rsync -av --progress /path/to/ai_warehouse/ /backup/ai_warehouse/

# æˆ–ä½¿ç”¨ MinIO çš„è·¨å€åŸŸè¤‡è£½
mc mirror local/models s3/backup-bucket/models
```

### é«˜å¯ç”¨æ€§éƒ¨ç½²

**è² è¼‰å¹³è¡¡å™¨è¨­å®š**
```yaml
# docker-compose.ha.yml
services:
  api:
    deploy:
      replicas: 3
    depends_on:
      - postgres
      - redis

  worker:
    deploy:
      replicas: 2
```

**è³‡æ–™åº«é›†ç¾¤ (é€²éš)**
```bash
# PostgreSQL ä¸»å¾è¤‡è£½è¨­å®š
# 1. è¨­å®šä¸»ç¯€é»
# 2. é…ç½®å¾ç¯€é»
# 3. è¨­å®šè‡ªå‹•æ•…éšœè½‰ç§»
```

### å®‰å…¨